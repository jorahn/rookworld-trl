üö® DEBUGGING GRPO TRAINING - STEP BY STEP
======================================================================
üîç MANUAL GRPO DEBUG - SINGLE BATCH ANALYSIS
======================================================================
üìã Configuration (TRL-matched):
  Batch size: 4
  Generations per prompt: 4
  Max new tokens: 256
  Beta (KL penalty): 0.1 (low for more learning)
  Steps: 50 (sequential GRPO updates)
  Overfit mode: ON (reuse the same batch each step)
  Seed: 42
  Learning rate: 1e-06
  Sampling (task-conditional): P(temp=0.5, top_p=0.9) | A(temp=0.95, top_p=0.95)
  AdamW: Œ≤1=0.9, Œ≤2=0.999, Œµ=1e-08, decay=0.0

==================== PHASE 1: SETUP ====================
üì• Loading base model...
‚úÖ Loaded reference model (frozen) and training model
üèÜ Initializing reward function...
‚úì Auto-detected Stockfish at: /usr/games/stockfish
‚úì Stockfish initialized at /usr/games/stockfish (depth=10, time=0.1s)
üìä Loading mixed batch...
Loading 20 samples from RookWorld dataset...
‚úì Loaded 20 mixed task samples
üßæ Using dataset prompts [0:4) of 20 (wrap-around: no)
‚úÖ Setup complete. Batch composition:
  P: tasks: 3/4
  A: tasks: 1/4

==================== PHASE 2: INITIAL MODEL PERFORMANCE ====================
  Prompt 1: avg reward = 0.135
  Prompt 2: avg reward = 0.387
  Prompt 3: avg reward = 0.630
  Prompt 4: avg reward = 0.554
üìä INITIAL TRAINING MODEL Performance:
  Average reward: 0.4266
  Positive ratio: 100.0%

==================== PHASE 3: MANUAL GRPO STEP ====================
ü§ñ Generating completions for GRPO training...

üéØ Prompt 1: P: 2k4r/ppp1q1p1/nbb1N3/4PP2/3p2P1/PB5P/1PPK3R/3RQ3 b - - 3 ...
    Gen 1: Train_LP= -440.0, Ref_LP=  -45.0
           Text:                                M: d4d3 a6c5 e7d7 c6f3 c6d7  ...
           Normalized: M: d4d3 a6c5 e7d7 c6f3 c6d7 E: -3.65 -3.77 -4.03 -4.03 -3.94...
    Gen 2: Train_LP= -320.0, Ref_LP=  -35.2
           Text:                                M: c6b5 d4d3 e7d7 a6c5 c6d7  ...
           Normalized: M: c6b5 d4d3 e7d7 a6c5 c6d7 E: -4.31 -4.05 -4.4 -3.96 -4.07 ...
    Gen 3: Train_LP= -596.0, Ref_LP=  -36.0
           Text:                                M: a6c5 h8h6 d4d3 c6d7 c6b5  ...
           Normalized: M: a6c5 h8h6 d4d3 c6d7 c6b5 E: -3.37 -3.96 -3.5 -3.84 -3.87 ...
    Gen 4: Train_LP= -376.0, Ref_LP=  -43.5
           Text:                                M: a6c5 e7d7 d4d3 e7e8 h8e8  ...
           Normalized: M: a6c5 e7d7 d4d3 e7e8 h8e8 E: -3.82 -3.83 -3.87 -4.02 -3.91...

üéØ Prompt 2: P: 2N5/2k5/3p1p2/3P1BbP/4Pp2/1n6/4K3/8 w - - 9 49...
    Gen 1: Train_LP= -688.0, Ref_LP=  -26.4
           Text:                                             M: c8a7 e2f2 e2f...
           Normalized: M: c8a7 e2f2 e2f3 c8e7 e2d3 E: 0.1 0.12 0.2 0.21 0.09 B: c8e...
    Gen 2: Train_LP= -338.0, Ref_LP=  -47.5
           Text:                                             M: e2f1 e2f2 c8e...
           Normalized: M: e2f1 e2f2 c8e7 c8a7 e2f3 E: 0.23 0.18 0.25 0.27 0.28 B: e...
    Gen 3: Train_LP= -760.0, Ref_LP=  -47.2
           Text:                                             M: e2f3 c8e7 e2d...
           Normalized: M: e2f3 c8e7 e2d3 e2f2 c8a7 E: 0.45 0.48 0.42 0.45 0.45 B: c...
    Gen 4: Train_LP= -470.0, Ref_LP=  -50.0
           Text:                                             M: e2f3 c8e7 e2f...
           Normalized: M: e2f3 c8e7 e2f1 e2f2 e2e1 E: 0.38 0.48 0.29 0.28 0.26 B: c...

üéØ Prompt 3: A: 8/2k2b2/R1p4r/3p1p2/P7/2P2B1P/5PPK/4R3 w - - 1 31+a6a5+f7...
    Gen 1: Train_LP= -292.0, Ref_LP=   -0.0
           Text: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
           Normalized: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
    Gen 2: Train_LP= -228.0, Ref_LP=   -0.0
           Text: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
           Normalized: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
    Gen 3: Train_LP= -255.0, Ref_LP=   -0.0
           Text: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
           Normalized: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
    Gen 4: Train_LP= -228.0, Ref_LP=   -0.0
           Text: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
           Normalized: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
    ‚ö†Ô∏è  All 4 completions are IDENTICAL!

üéØ Prompt 4: P: rnbqkbnr/pp1ppppp/2p5/8/4P3/2N5/PPPP1PPP/R1BQKBNR b KQkq ...
    Gen 1: Train_LP= -664.0, Ref_LP=  -35.8
           Text:                             M: d7d5 d7d6 c6c5 e7e6 e7e5     ...
           Normalized: M: d7d5 d7d6 c6c5 e7e6 e7e5 E: -0.48 -0.59 -0.76 -0.74 -0.46...
    Gen 2: Train_LP= -576.0, Ref_LP=  -23.2
           Text:                             M: d7d6 d7d5 e7e5 g7g6 e7e6     ...
           Normalized: M: d7d6 d7d5 e7e5 g7g6 e7e6 E: -0.63 -0.46 -0.63 -0.63 -0.8 ...
    Gen 3: Train_LP= -556.0, Ref_LP=  -36.0
           Text:                             M: d7d5 d7d6 e7e5 g7g6 d8c7     ...
           Normalized: M: d7d5 d7d6 e7e5 g7g6 d8c7 E: -0.49 -0.71 -0.64 -0.65 -0.67...
    Gen 4: Train_LP= -448.0, Ref_LP=  -35.2
           Text:                             M: d7d6 d7d5 e7e5 g7g6 e7e6     ...
           Normalized: M: d7d6 d7d5 e7e5 g7g6 e7e6 E: -0.53 -0.39 -0.62 -0.69 -0.68...

==================== PHASE 4: REWARD CALCULATION ====================

üéØ Scoring Prompt 1 completions:
  Gen 1: Reward= 0.255 | M: d4d3 a6c5 e7d7 c6f3 c6d7 E: -3.65 -3....
  Gen 2: Reward= 0.210 | M: c6b5 d4d3 e7d7 a6c5 c6d7 E: -4.31 -4....
  Gen 3: Reward= 0.129 | M: a6c5 h8h6 d4d3 c6d7 c6b5 E: -3.37 -3....
  Gen 4: Reward= 0.205 | M: a6c5 e7d7 d4d3 e7e8 h8e8 E: -3.82 -3....
  üìä Average reward: 0.200

üéØ Scoring Prompt 2 completions:
  Gen 1: Reward= 0.539 | M: c8a7 e2f2 e2f3 c8e7 e2d3 E: 0.1 0.12 ...
  Gen 2: Reward= 0.059 | M: e2f1 e2f2 c8e7 c8a7 e2f3 E: 0.23 0.18...
  Gen 3: Reward= 0.549 | M: e2f3 c8e7 e2d3 e2f2 c8a7 E: 0.45 0.48...
  Gen 4: Reward= 0.415 | M: e2f3 c8e7 e2f1 e2f2 e2e1 E: 0.38 0.48...
  üìä Average reward: 0.391

üéØ Scoring Prompt 3 completions:
  Gen 1: Reward= 0.630 | 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b...
  Gen 2: Reward= 0.630 | 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b...
  Gen 3: Reward= 0.630 | 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b...
  Gen 4: Reward= 0.630 | 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b...
  üìä Average reward: 0.630

üéØ Scoring Prompt 4 completions:
  Gen 1: Reward= 0.647 | M: d7d5 d7d6 c6c5 e7e6 e7e5 E: -0.48 -0....
  Gen 2: Reward= 0.373 | M: d7d6 d7d5 e7e5 g7g6 e7e6 E: -0.63 -0....
  Gen 3: Reward= 0.541 | M: d7d5 d7d6 e7e5 g7g6 d8c7 E: -0.49 -0....
  Gen 4: Reward= 0.717 | M: d7d6 d7d5 e7e5 g7g6 e7e6 E: -0.53 -0....
  üìä Average reward: 0.569

==================== PHASE 5: TRL EXACT ADVANTAGE CALCULATION ====================
üî¢ TRL Advantage Calculation (exact formula):
  Total rewards shape: torch.Size([16])
  Rewards: [0.25457143 0.2102     0.1294     0.2054     0.53937143 0.05937143
 0.54857143 0.415      0.63       0.63       0.63       0.63
 0.64657143 0.37257143 0.54057143 0.71697143]
  Grouped rewards shape: torch.Size([4, 4])
  Group means: [0.19989286 0.39057857 0.63       0.56917143]
  Expanded means: [0.19989286 0.19989286 0.19989286 0.19989286 0.39057857 0.39057857
 0.39057857 0.39057857 0.63       0.63       0.63       0.63
 0.56917143 0.56917143 0.56917143 0.56917143]
  Raw advantages: [ 0.05467857  0.01030714 -0.07049286  0.00550714  0.14879286 -0.33120714
  0.15799286  0.02442143  0.          0.          0.          0.
  0.0774     -0.1966     -0.0286      0.1478    ]
  Group std devs: [0.05194726 0.22905286 0.         0.14978331]
  Normalized advantages: [ 1.05257845  0.19841551 -1.35700807  0.10601411  0.64960053 -1.44598564
  0.68976593  0.10661918  0.          0.          0.          0.
  0.51674649 -1.31256279 -0.1909425   0.9867588 ]

üìä TRL Advantage Summary:
  Prompt 1: ['+1.053', '+0.198', '-1.357', '+0.106']
    Stats: mean=+0.000 (expect ‚âà0), std=0.866 (expect >0)
    ‚úÖ Good range (2.410) - clear learning signal
  Prompt 2: ['+0.650', '-1.446', '+0.690', '+0.107']
    Stats: mean=-0.000 (expect ‚âà0), std=0.866 (expect >0)
    ‚úÖ Good range (2.136) - clear learning signal
  Prompt 3: ['+0.000', '+0.000', '+0.000', '+0.000']
    Stats: mean=+0.000 (expect ‚âà0), std=0.000 (expect >0)
    ‚ö†Ô∏è  Very small range (0.0000) - little learning signal!
    ‚ö†Ô∏è  Very low std - insufficient reward diversity!
  Prompt 4: ['+0.517', '-1.313', '-0.191', '+0.987']
    Stats: mean=+0.000 (expect ‚âà0), std=0.866 (expect >0)
    ‚úÖ Good range (2.299) - clear learning signal

==================== PHASE 6-8: CORRECTED GRPO LOSS CALCULATION ====================
üîÑ Computing GRPO loss with proper token-level KL...

üéØ Prompt 1 - Corrected GRPO Loss:
  Gen 1: A=+1.053, logp/len=-6.000, kl=-4.125
         PG=6.312, KL_penalty=-0.412, total=5.906
  Gen 2: A=+0.198, logp/len=-7.188, kl=-5.375
         PG=1.430, KL_penalty=-0.539, total=0.891
  Gen 3: A=-1.357, logp/len=-7.281, kl=-5.219
         PG=-9.875, KL_penalty=-0.523, total=-10.375
  Gen 4: A=+0.106, logp/len=-7.094, kl=-5.406
         PG=0.754, KL_penalty=-0.539, total=0.215
  üìä Prompt averages: PG=-0.344, KL=-0.504

üéØ Prompt 2 - Corrected GRPO Loss:
  Gen 1: A=+0.650, logp/len=-7.469, kl=-5.344
         PG=4.844, KL_penalty=-0.535, total=4.312
  Gen 2: A=-1.446, logp/len=-6.312, kl=-4.312
         PG=-9.125, KL_penalty=-0.432, total=-9.562
  Gen 3: A=+0.690, logp/len=-6.938, kl=-5.094
         PG=4.781, KL_penalty=-0.508, total=4.281
  Gen 4: A=+0.107, logp/len=-6.656, kl=-4.625
         PG=0.711, KL_penalty=-0.463, total=0.248
  üìä Prompt averages: PG=0.303, KL=-0.484

üéØ Prompt 3 - Corrected GRPO Loss:
  Gen 1: A=+0.000, logp/len=-4.719, kl=-4.344
         PG=0.000, KL_penalty=-0.434, total=-0.434
  Gen 2: A=+0.000, logp/len=-5.188, kl=-4.844
         PG=0.000, KL_penalty=-0.484, total=-0.484
  Gen 3: A=+0.000, logp/len=-5.156, kl=-4.781
         PG=0.000, KL_penalty=-0.479, total=-0.479
  Gen 4: A=+0.000, logp/len=-5.250, kl=-4.844
         PG=0.000, KL_penalty=-0.484, total=-0.484
  üìä Prompt averages: PG=0.000, KL=-0.471

üéØ Prompt 4 - Corrected GRPO Loss:
  Gen 1: A=+0.517, logp/len=-5.812, kl=-4.156
         PG=3.000, KL_penalty=-0.416, total=2.578
  Gen 2: A=-1.313, logp/len=-6.156, kl=-4.594
         PG=-8.062, KL_penalty=-0.459, total=-8.500
  Gen 3: A=-0.191, logp/len=-6.250, kl=-4.750
         PG=-1.195, KL_penalty=-0.475, total=-1.672
  Gen 4: A=+0.987, logp/len=-6.969, kl=-5.375
         PG=6.875, KL_penalty=-0.539, total=6.344
  üìä Prompt averages: PG=0.156, KL=-0.473

üî¢ CORRECTED Loss Breakdown:
  Policy Gradient Loss:    0.029
  KL Loss (with gradients):   -0.484
  Total Loss:          =  -0.455

  KL penalty ratio: 94.5%

üîÑ Performing corrected gradient update...
  Total gradient norm: 20.18
  ‚úÖ Gradient update applied with proper KL regularization

==================== PHASE 9: POST-UPDATE PERFORMANCE ====================
  Prompt 1: avg reward = 0.148
  Prompt 2: avg reward = 0.387
  Prompt 3: avg reward = 0.630
  Prompt 4: avg reward = 0.399
üìä POST-UPDATE TRAINING MODEL (step 1) Performance:
  Average reward: 0.3911
  Positive ratio: 100.0%

==================== PHASE 10: ANALYSIS ====================
üîç Corrected GRPO Step Impact:
  Initial performance: 0.4266
  Post-update performance: 0.3911
  Performance change: -0.0354

üìä Comparison with Training Logs:
  Corrected manual result: 0.3911
  Training log average: -0.2070
  Difference: 0.5981

üîç KL Regularization Analysis:
  KL magnitude: 0.484
  PG magnitude: 0.029
  ‚ö†Ô∏è  KL dominates - consider lowering beta

======================================================================
üß≠ SEQUENTIAL STEP 2/50
======================================================================
üßæ Overfit mode: reusing initial batch (no dataset slice advance)

ü§ñ Generating completions for GRPO training (step 2)...

üéØ Prompt 1: P: 2k4r/ppp1q1p1/nbb1N3/4PP2/3p2P1/PB5P/1PPK3R/3RQ3 b - - 3 ...
    Gen 1: Train_LP= -624.0, Ref_LP=  -33.5
           Text:                                M: a6c5 c8b8 d4d3 h8e8 c6f3  ...
           Normalized: M: a6c5 c8b8 d4d3 h8e8 c6f3 E: -3.33 -3.74 -3.6 -3.86 -3.56 ...
    Gen 2: Train_LP= -516.0, Ref_LP=  -44.5
           Text:                                M: d4d3 b6a5 e7h4 a6c5 e7d7  ...
           Normalized: M: d4d3 b6a5 e7h4 a6c5 e7d7 E: -3.63 -3.85 -4.08 -3.71 -3.88...
    Gen 3: Train_LP= -264.0, Ref_LP=  -35.0
           Text:                                M: c6f3 a6c5 d4d3 e7d7 e7e8  ...
           Normalized: M: c6f3 a6c5 d4d3 e7d7 e7e8 E: -3.94 -3.55 -3.58 -3.75 -3.7 ...
    Gen 4: Train_LP= -544.0, Ref_LP=  -45.5
           Text:                                M: b6a5 a6c5 e7h4 h8e8 d4d3  ...
           Normalized: M: b6a5 a6c5 e7h4 h8e8 d4d3 E: -3.61 -3.49 -4.22 -4.09 -3.85...

üéØ Prompt 2: P: 2N5/2k5/3p1p2/3P1BbP/4Pp2/1n6/4K3/8 w - - 9 49...
    Gen 1: Train_LP= -350.0, Ref_LP=  -33.5
           Text:                                             M: e2f2 e2f1 e2f...
           Normalized: M: e2f2 e2f1 e2f3 e2e1 c8e7 E: 0.09 0.09 0.08 0.08 0.08 B: e...
    Gen 2: Train_LP= -532.0, Ref_LP=  -34.8
           Text:                                             M: e2f3 e2f1 c8e...
           Normalized: M: e2f3 e2f1 c8e7 c8a7 e2f2 E: 0.42 0.42 0.42 0.43 0.43 B: e...
    Gen 3: Train_LP= -434.0, Ref_LP=  -28.2
           Text:                                             M: f5g4 c8e7 e2d...
           Normalized: M: f5g4 c8e7 e2d3 f5e6 e2f2 E: 0.22 0.28 0.27 0.23 0.2 B: c8...
    Gen 4: Train_LP= -688.0, Ref_LP=  -35.8
           Text:                                             M: e2d3 e2f3 e2f...
           Normalized: M: e2d3 e2f3 e2f2 c8e7 c8a7 E: 0.37 0.39 0.37 0.37 0.38 B: c...

üéØ Prompt 3: A: 8/2k2b2/R1p4r/3p1p2/P7/2P2B1P/5PPK/4R3 w - - 1 31+a6a5+f7...
    Gen 1: Train_LP= -270.0, Ref_LP=   -0.0
           Text: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
           Normalized: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
    Gen 2: Train_LP= -290.0, Ref_LP=   -0.0
           Text: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
           Normalized: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
    Gen 3: Train_LP= -270.0, Ref_LP=   -0.0
           Text: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
           Normalized: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
    Gen 4: Train_LP= -304.0, Ref_LP=   -0.0
           Text: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
           Normalized: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...

üéØ Prompt 4: P: rnbqkbnr/pp1ppppp/2p5/8/4P3/2N5/PPPP1PPP/R1BQKBNR b KQkq ...
    Gen 1: Train_LP= -620.0, Ref_LP=  -23.2
           Text:                             M: d7d6 d7d5 e7e5 g7g6 e7e6     ...
           Normalized: M: d7d6 d7d5 e7e5 g7g6 e7e6 E: -0.62 -0.4 -0.64 -0.65 -0.81 ...
    Gen 2: Train_LP= -422.0, Ref_LP=  -24.1
           Text:                             M: d7d6 e7e5 d7d5 g7g6 e7e6     ...
           Normalized: M: d7d6 e7e5 d7d5 g7g6 e7e6 E: -0.63 -0.67 -0.4 -0.64 -0.67 ...
    Gen 3: Train_LP= -612.0, Ref_LP=  -33.8
           Text:                             M: d7d6 d7d5 e7e5 e7e6 g7g6     ...
           Normalized: M: d7d6 d7d5 e7e5 e7e6 g7g6 E: -0.59 -0.54 -0.53 -0.74 -0.73...
    Gen 4: Train_LP= -520.0, Ref_LP=  -36.2
           Text:                             M: d7d5 d7d6 g7g6 e7e5 c6c5     ...
           Normalized: M: d7d5 d7d6 g7g6 e7e5 c6c5 E: -0.52 -0.66 -0.85 -0.46 -0.84...

==================== PHASE 4: REWARD CALCULATION (step 2) ====================

üéØ Scoring Prompt 1 completions:
  Gen 1: Reward= 0.327 | M: a6c5 c8b8 d4d3 h8e8 c6f3 E: -3.33 -3....
  Gen 2: Reward= 0.193 | M: d4d3 b6a5 e7h4 a6c5 e7d7 E: -3.63 -3....
  Gen 3: Reward= 0.511 | M: c6f3 a6c5 d4d3 e7d7 e7e8 E: -3.94 -3....
  Gen 4: Reward= 0.242 | M: b6a5 a6c5 e7h4 h8e8 d4d3 E: -3.61 -3....
  üìä Average reward: 0.318

üéØ Scoring Prompt 2 completions:
  Gen 1: Reward=-0.026 | M: e2f2 e2f1 e2f3 e2e1 c8e7 E: 0.09 0.09...
  Gen 2: Reward= 0.369 | M: e2f3 e2f1 c8e7 c8a7 e2f2 E: 0.42 0.42...
  Gen 3: Reward= 0.431 | M: f5g4 c8e7 e2d3 f5e6 e2f2 E: 0.22 0.28...
  Gen 4: Reward= 0.269 | M: e2d3 e2f3 e2f2 c8e7 c8a7 E: 0.37 0.39...
  üìä Average reward: 0.260

üéØ Scoring Prompt 3 completions:
  Gen 1: Reward= 0.630 | 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b...
  Gen 2: Reward= 0.630 | 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b...
  Gen 3: Reward= 0.630 | 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b...
  Gen 4: Reward= 0.630 | 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b...
  üìä Average reward: 0.630

üéØ Scoring Prompt 4 completions:
  Gen 1: Reward= 0.372 | M: d7d6 d7d5 e7e5 g7g6 e7e6 E: -0.62 -0....
  Gen 2: Reward= 0.364 | M: d7d6 e7e5 d7d5 g7g6 e7e6 E: -0.63 -0....
  Gen 3: Reward= 0.366 | M: d7d6 d7d5 e7e5 e7e6 g7g6 E: -0.59 -0....
  Gen 4: Reward= 0.808 | M: d7d5 d7d6 g7g6 e7e5 c6c5 E: -0.52 -0....
  üìä Average reward: 0.478

==================== PHASE 5: TRL EXACT ADVANTAGE CALCULATION (step 2) ====================
üî¢ TRL Advantage Calculation (exact formula):
  Total rewards shape: torch.Size([16])
  Rewards: [ 0.3268      0.19337143  0.51097143  0.24217143 -0.0262      0.36857143
  0.431       0.26857143  0.63        0.63        0.63        0.63
  0.37217143  0.36377143  0.36617143  0.8084    ]

==================== PHASE 6-8: CORRECTED GRPO LOSS CALCULATION (step 2) ====================
üîÑ Computing GRPO loss with proper token-level KL...

üéØ Prompt 1 - Corrected GRPO Loss:
  Gen 1: A=+0.061, logp/len=-7.844, kl=-6.094
         PG=0.475, KL_penalty=-0.609, total=-0.135
  Gen 2: A=-0.894, logp/len=-6.312, kl=-4.500
         PG=-5.656, KL_penalty=-0.449, total=-6.094
  Gen 3: A=+1.378, logp/len=-8.000, kl=-6.125
         PG=11.000, KL_penalty=-0.613, total=10.375
  Gen 4: A=-0.545, logp/len=-6.688, kl=-5.031
         PG=-3.641, KL_penalty=-0.504, total=-4.156
  üìä Prompt averages: PG=0.543, KL=-0.543

üéØ Prompt 2 - Corrected GRPO Loss:
  Gen 1: A=-1.416, logp/len=-9.562, kl=-7.844
         PG=-13.562, KL_penalty=-0.785, total=-14.375
  Gen 2: A=+0.534, logp/len=-6.812, kl=-4.875
         PG=3.641, KL_penalty=-0.488, total=3.156
  Gen 3: A=+0.842, logp/len=-6.750, kl=-4.625
         PG=5.688, KL_penalty=-0.463, total=5.219
  Gen 4: A=+0.040, logp/len=-7.188, kl=-5.406
         PG=0.287, KL_penalty=-0.539, total=-0.252
  üìä Prompt averages: PG=-0.992, KL=-0.570

üéØ Prompt 3 - Corrected GRPO Loss:
  Gen 1: A=+0.000, logp/len=-5.875, kl=-5.531
         PG=0.000, KL_penalty=-0.555, total=-0.555
  Gen 2: A=+0.000, logp/len=-5.156, kl=-4.781
         PG=0.000, KL_penalty=-0.479, total=-0.479
  Gen 3: A=+0.000, logp/len=-5.156, kl=-4.812
         PG=0.000, KL_penalty=-0.480, total=-0.480
  Gen 4: A=+0.000, logp/len=-5.625, kl=-5.250
         PG=0.000, KL_penalty=-0.523, total=-0.523
  üìä Prompt averages: PG=0.000, KL=-0.508

üéØ Prompt 4 - Corrected GRPO Loss:
  Gen 1: A=-0.478, logp/len=-5.719, kl=-4.125
         PG=-2.734, KL_penalty=-0.412, total=-3.141
  Gen 2: A=-0.516, logp/len=-5.562, kl=-4.031
         PG=-2.875, KL_penalty=-0.402, total=-3.281
  Gen 3: A=-0.505, logp/len=-7.688, kl=-6.125
         PG=-3.891, KL_penalty=-0.613, total=-4.500
  Gen 4: A=+1.500, logp/len=-6.875, kl=-5.062
         PG=10.312, KL_penalty=-0.508, total=9.812
  üìä Prompt averages: PG=0.203, KL=-0.482

üî¢ CORRECTED Loss Breakdown:
  Policy Gradient Loss:   -0.062
  KL Loss (with gradients):   -0.523
  Total Loss:          =  -0.586
  KL penalty ratio: 89.5%

üîÑ Performing corrected gradient update (step 2)...
  Total gradient norm: 21.08
  ‚úÖ Gradient update applied with proper KL regularization

==================== PHASE 9: POST-UPDATE PERFORMANCE (step 2) ====================
  Prompt 1: avg reward = 0.154
  Prompt 2: avg reward = 0.387
  Prompt 3: avg reward = 0.630
  Prompt 4: avg reward = 0.559
üìä POST-UPDATE (step 2) Performance:
  Average reward: 0.4325
  Positive ratio: 100.0%

==================== PHASE 10: ANALYSIS (step 2) ====================
üîç GRPO Step Impact (step 2):
  Post-update performance: 0.4325
  Step performance change: +0.0413

======================================================================
üß≠ SEQUENTIAL STEP 3/50
======================================================================
üßæ Overfit mode: reusing initial batch (no dataset slice advance)

ü§ñ Generating completions for GRPO training (step 3)...

üéØ Prompt 1: P: 2k4r/ppp1q1p1/nbb1N3/4PP2/3p2P1/PB5P/1PPK3R/3RQ3 b - - 3 ...
    Gen 1: Train_LP= -392.0, Ref_LP=  -33.5
           Text:                                M: a6c5 d4d3 h8e8 e7d7 e7e8  ...
           Normalized: M: a6c5 d4d3 h8e8 e7d7 e7e8 E: -3.4 -3.82 -3.77 -3.82 -3.91 ...
    Gen 2: Train_LP= -408.0, Ref_LP=  -45.5
           Text:                                M: c6f3 a6c5 e7d7 e7e8 c8b8  ...
           Normalized: M: c6f3 a6c5 e7d7 e7e8 c8b8 E: -3.56 -3.25 -3.37 -3.59 -3.42...
    Gen 3: Train_LP= -380.0, Ref_LP=  -44.0
           Text:                                M: a6c5 d4d3 b6a5 e7d7 c8b8  ...
           Normalized: M: a6c5 d4d3 b6a5 e7d7 c8b8 E: -2.96 -3.17 -3.24 -3.28 -3.45...
    Gen 4: Train_LP= -556.0, Ref_LP=  -43.8
           Text:                                M: a6c5 c6d7 e7d7 c8b8 d4d3  ...
           Normalized: M: a6c5 c6d7 e7d7 c8b8 d4d3 E: -3.54 -3.76 -3.86 -3.76 -3.63...

üéØ Prompt 2: P: 2N5/2k5/3p1p2/3P1BbP/4Pp2/1n6/4K3/8 w - - 9 49...
    Gen 1: Train_LP= -512.0, Ref_LP=  -14.6
           Text:                                             M: c8e7 e2f3 e2f...
           Normalized: M: c8e7 e2f3 e2f1 e2f2 c8a7 E: 0.0 0.0 0.0 0.0 0.0 B: e2f2...
    Gen 2: Train_LP= -440.0, Ref_LP=  -15.6
           Text:                                             M: e2f3 e2d3 c8e...
           Normalized: M: e2f3 e2d3 c8e7 e2f2 e2f1 E: 0.0 0.0 0.0 0.0 0.0 B: e2f3...
    Gen 3: Train_LP= -460.0, Ref_LP=  -78.0
           Text:                                             M: e2f1 e2f3 e2f...
           Normalized: M: e2f1 e2f3 e2f2 c8a7 c8e7 E: 0.32 0.32 0.33 0.33 0.34 B: c...
    Gen 4: Train_LP= -748.0, Ref_LP=  -17.0
           Text:                                             M: e2f1 e2f3 f5g...
           Normalized: M: e2f1 e2f3 f5g6 c8e7 f5e6 E: 0.0 0.0 0.0 0.0 0.0 B: c8e7...

üéØ Prompt 3: A: 8/2k2b2/R1p4r/3p1p2/P7/2P2B1P/5PPK/4R3 w - - 1 31+a6a5+f7...
    Gen 1: Train_LP= -241.0, Ref_LP=   -0.0
           Text: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
           Normalized: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
    Gen 2: Train_LP= -308.0, Ref_LP=   -0.0
           Text: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
           Normalized: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
    Gen 3: Train_LP= -312.0, Ref_LP=   -0.0
           Text: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
           Normalized: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
    Gen 4: Train_LP= -312.0, Ref_LP=   -0.0
           Text: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
           Normalized: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...

üéØ Prompt 4: P: rnbqkbnr/pp1ppppp/2p5/8/4P3/2N5/PPPP1PPP/R1BQKBNR b KQkq ...
    Gen 1: Train_LP= -512.0, Ref_LP=  -34.8
           Text:                             M: e7e5 g7g6 d7d6 d7d5 c6c5     ...
           Normalized: M: e7e5 g7g6 d7d6 d7d5 c6c5 E: -0.61 -0.73 -0.62 -0.46 -0.74...
    Gen 2: Train_LP= -560.0, Ref_LP=  -34.8
           Text:                             M: d7d6 d7d5 e7e5 g7g6 d8c7     ...
           Normalized: M: d7d6 d7d5 e7e5 g7g6 d8c7 E: -0.57 -0.42 -0.62 -0.62 -0.76...
    Gen 3: Train_LP= -644.0, Ref_LP=  -24.8
           Text:                             M: d7d6 d7d5 g7g6 e7e6 e7e5     ...
           Normalized: M: d7d6 d7d5 g7g6 e7e6 e7e5 E: -0.61 -0.37 -0.59 -0.9 -0.59 ...
    Gen 4: Train_LP= -548.0, Ref_LP=  -34.8
           Text:                             M: d7d5 e7e5 e7e6 d7d6 g7g6     ...
           Normalized: M: d7d5 e7e5 e7e6 d7d6 g7g6 E: -0.41 -0.56 -0.71 -0.65 -0.73...

==================== PHASE 4: REWARD CALCULATION (step 3) ====================

üéØ Scoring Prompt 1 completions:
  Gen 1: Reward= 0.179 | M: a6c5 d4d3 h8e8 e7d7 e7e8 E: -3.4 -3.8...
  Gen 2: Reward= 0.219 | M: c6f3 a6c5 e7d7 e7e8 c8b8 E: -3.56 -3....
  Gen 3: Reward= 0.400 | M: a6c5 d4d3 b6a5 e7d7 c8b8 E: -2.96 -3....
  Gen 4: Reward= 0.251 | M: a6c5 c6d7 e7d7 c8b8 d4d3 E: -3.54 -3....
  üìä Average reward: 0.262

üéØ Scoring Prompt 2 completions:
  Gen 1: Reward= 0.359 | M: c8e7 e2f3 e2f1 e2f2 c8a7 E: 0.0 0.0 0...
  Gen 2: Reward=-0.075 | M: e2f3 e2d3 c8e7 e2f2 e2f1 E: 0.0 0.0 0...
  Gen 3: Reward= 0.469 | M: e2f1 e2f3 e2f2 c8a7 c8e7 E: 0.32 0.32...
  Gen 4: Reward= 0.333 | M: e2f1 e2f3 f5g6 c8e7 f5e6 E: 0.0 0.0 0...
  üìä Average reward: 0.271

üéØ Scoring Prompt 3 completions:
  Gen 1: Reward= 0.630 | 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b...
  Gen 2: Reward= 0.630 | 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b...
  Gen 3: Reward= 0.630 | 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b...
  Gen 4: Reward= 0.630 | 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b...
  üìä Average reward: 0.630

üéØ Scoring Prompt 4 completions:
  Gen 1: Reward= 0.716 | M: e7e5 g7g6 d7d6 d7d5 c6c5 E: -0.61 -0....
  Gen 2: Reward= 0.367 | M: d7d6 d7d5 e7e5 g7g6 d8c7 E: -0.57 -0....
  Gen 3: Reward= 0.366 | M: d7d6 d7d5 g7g6 e7e6 e7e5 E: -0.61 -0....
  Gen 4: Reward= 0.751 | M: d7d5 e7e5 e7e6 d7d6 g7g6 E: -0.41 -0....
  üìä Average reward: 0.550

==================== PHASE 5: TRL EXACT ADVANTAGE CALCULATION (step 3) ====================
üî¢ TRL Advantage Calculation (exact formula):
  Total rewards shape: torch.Size([16])
  Rewards: [ 0.1794      0.21857143  0.4         0.25057143  0.35857143 -0.075
  0.46857143  0.33333333  0.63        0.63        0.63        0.63
  0.716       0.36737143  0.36617143  0.75137143]

==================== PHASE 6-8: CORRECTED GRPO LOSS CALCULATION (step 3) ====================
üîÑ Computing GRPO loss with proper token-level KL...

üéØ Prompt 1 - Corrected GRPO Loss:
  Gen 1: A=-0.858, logp/len=-6.625, kl=-4.656
         PG=-5.688, KL_penalty=-0.465, total=-6.156
  Gen 2: A=-0.452, logp/len=-7.188, kl=-5.406
         PG=-3.250, KL_penalty=-0.539, total=-3.781
  Gen 3: A=+1.430, logp/len=-7.000, kl=-5.312
         PG=10.000, KL_penalty=-0.531, total=9.500
  Gen 4: A=-0.120, logp/len=-5.406, kl=-3.672
         PG=-0.648, KL_penalty=-0.367, total=-1.016
  üìä Prompt averages: PG=0.104, KL=-0.475

üéØ Prompt 2 - Corrected GRPO Loss:
  Gen 1: A=+0.366, logp/len=-5.250, kl=-3.703
         PG=1.922, KL_penalty=-0.371, total=1.547
  Gen 2: A=-1.454, logp/len=-6.000, kl=-4.188
         PG=-8.750, KL_penalty=-0.418, total=-9.188
  Gen 3: A=+0.828, logp/len=-6.625, kl=-4.844
         PG=5.469, KL_penalty=-0.484, total=5.000
  Gen 4: A=+0.260, logp/len=-7.594, kl=-6.000
         PG=1.977, KL_penalty=-0.602, total=1.375
  üìä Prompt averages: PG=0.158, KL=-0.469

üéØ Prompt 3 - Corrected GRPO Loss:
  Gen 1: A=+0.000, logp/len=-5.594, kl=-5.219
         PG=0.000, KL_penalty=-0.523, total=-0.523
  Gen 2: A=+0.000, logp/len=-5.375, kl=-5.000
         PG=0.000, KL_penalty=-0.500, total=-0.500
  Gen 3: A=+0.000, logp/len=-4.938, kl=-4.562
         PG=0.000, KL_penalty=-0.457, total=-0.457
  Gen 4: A=+0.000, logp/len=-6.594, kl=-6.250
         PG=0.000, KL_penalty=-0.625, total=-0.625
  üìä Prompt averages: PG=0.000, KL=-0.527

üéØ Prompt 4 - Corrected GRPO Loss:
  Gen 1: A=+0.781, logp/len=-5.906, kl=-4.156
         PG=4.625, KL_penalty=-0.416, total=4.219
  Gen 2: A=-0.861, logp/len=-6.969, kl=-5.562
         PG=-6.000, KL_penalty=-0.555, total=-6.562
  Gen 3: A=-0.867, logp/len=-7.031, kl=-5.406
         PG=-6.094, KL_penalty=-0.539, total=-6.625
  Gen 4: A=+0.947, logp/len=-6.438, kl=-4.812
         PG=6.094, KL_penalty=-0.480, total=5.625
  üìä Prompt averages: PG=-0.344, KL=-0.496

üî¢ CORRECTED Loss Breakdown:
  Policy Gradient Loss:   -0.021
  KL Loss (with gradients):   -0.492
  Total Loss:          =  -0.512
  KL penalty ratio: 96.0%

üîÑ Performing corrected gradient update (step 3)...
  Total gradient norm: 19.25
  ‚úÖ Gradient update applied with proper KL regularization

==================== PHASE 9: POST-UPDATE PERFORMANCE (step 3) ====================
  Prompt 1: avg reward = 0.156
  Prompt 2: avg reward = 0.387
  Prompt 3: avg reward = 0.630
  Prompt 4: avg reward = 0.561
üìä POST-UPDATE (step 3) Performance:
  Average reward: 0.4334
  Positive ratio: 100.0%

==================== PHASE 10: ANALYSIS (step 3) ====================
üîç GRPO Step Impact (step 3):
  Post-update performance: 0.4334
  Step performance change: +0.0009

======================================================================
üß≠ SEQUENTIAL STEP 4/50
======================================================================
üßæ Overfit mode: reusing initial batch (no dataset slice advance)

ü§ñ Generating completions for GRPO training (step 4)...

üéØ Prompt 1: P: 2k4r/ppp1q1p1/nbb1N3/4PP2/3p2P1/PB5P/1PPK3R/3RQ3 b - - 3 ...
    Gen 1: Train_LP= -444.0, Ref_LP=  -32.8
           Text:                                M: d4d3 c8b8 a6c5 e7d7 b6a5  ...
           Normalized: M: d4d3 c8b8 a6c5 e7d7 b6a5 E: -3.46 -3.81 -3.31 -3.91 -3.7 ...
    Gen 2: Train_LP= -540.0, Ref_LP=  -35.8
           Text:                                M: c6f3 c6d7 d4d3 h8h4 a6c5  ...
           Normalized: M: c6f3 c6d7 d4d3 h8h4 a6c5 E: -4.11 -4.1 -3.96 -4.12 -3.79 ...
    Gen 3: Train_LP= -450.0, Ref_LP=  -35.2
           Text:                                M: h8h3 c8b8 a6c5 b6a5 d4d3  ...
           Normalized: M: h8h3 c8b8 a6c5 b6a5 d4d3 E: -3.4 -3.47 -3.11 -3.52 -3.18 ...
    Gen 4: Train_LP= -458.0, Ref_LP=  -47.5
           Text:                                M: h8h3 e7d7 a6c5 c8b8 e7e8  ...
           Normalized: M: h8h3 e7d7 a6c5 c8b8 e7e8 E: -4.03 -4.15 -3.83 -4.25 -4.07...

üéØ Prompt 2: P: 2N5/2k5/3p1p2/3P1BbP/4Pp2/1n6/4K3/8 w - - 9 49...
    Gen 1: Train_LP= -536.0, Ref_LP=  -56.5
           Text:                                             M: e2f3 c8e7 e2f...
           Normalized: M: e2f3 c8e7 e2f2 c8a7 e2d3 E: 0.22 0.21 0.22 0.2 0.2 B: e2f...
    Gen 2: Train_LP= -804.0, Ref_LP=  -78.0
           Text:                                             M: f5g4 e2d3 e2f...
           Normalized: M: f5g4 e2d3 e2f3 c8a7 c8e7 E: 0.12 0.13 0.13 0.13 0.14 B: c...
    Gen 3: Train_LP= -346.0, Ref_LP=  -15.5
           Text:                                             M: e2f3 e2d3 c8e...
           Normalized: M: e2f3 e2d3 c8e7 e2f2 e2f1 E: 0.0 0.0 0.0 0.0 0.0 B: e2f2...
    Gen 4: Train_LP= -528.0, Ref_LP=  -77.0
           Text:                                             M: e2f2 c8a7 e2d...
           Normalized: M: e2f2 c8a7 e2d3 c8e7 e2f3 E: 0.02 0.02 0.02 0.05 0.03 B: c...

üéØ Prompt 3: A: 8/2k2b2/R1p4r/3p1p2/P7/2P2B1P/5PPK/4R3 w - - 1 31+a6a5+f7...
    Gen 1: Train_LP= -290.0, Ref_LP=   -0.0
           Text: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
           Normalized: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
    Gen 2: Train_LP= -316.0, Ref_LP=   -0.0
           Text: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
           Normalized: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
    Gen 3: Train_LP= -286.0, Ref_LP=   -0.0
           Text: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
           Normalized: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
    Gen 4: Train_LP= -272.0, Ref_LP=   -0.0
           Text: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
           Normalized: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...

üéØ Prompt 4: P: rnbqkbnr/pp1ppppp/2p5/8/4P3/2N5/PPPP1PPP/R1BQKBNR b KQkq ...
    Gen 1: Train_LP= -428.0, Ref_LP=  -35.2
           Text:                             M: e7e5 d7d5 d7d6 g7g6 e7e6     ...
           Normalized: M: e7e5 d7d5 d7d6 g7g6 e7e6 E: -0.45 -0.42 -0.68 -0.72 -0.72...
    Gen 2: Train_LP= -572.0, Ref_LP=  -34.2
           Text:                             M: d7d5 e7e5 d7d6 e7e6 g7g6     ...
           Normalized: M: d7d5 e7e5 d7d6 e7e6 g7g6 E: -0.43 -0.59 -0.66 -0.79 -0.68...
    Gen 3: Train_LP= -660.0, Ref_LP=  -35.0
           Text:                             M: e7e6 e7e5 d7d5 d7d6 g7g6     ...
           Normalized: M: e7e6 e7e5 d7d5 d7d6 g7g6 E: -0.74 -0.46 -0.34 -0.61 -0.65...
    Gen 4: Train_LP= -676.0, Ref_LP=  -24.1
           Text:                             M: e7e5 d7d6 d7d5 e7e6 g7g6     ...
           Normalized: M: e7e5 d7d6 d7d5 e7e6 g7g6 E: -0.5 -0.51 -0.46 -0.89 -0.78 ...

==================== PHASE 4: REWARD CALCULATION (step 4) ====================

üéØ Scoring Prompt 1 completions:
  Gen 1: Reward= 0.278 | M: d4d3 c8b8 a6c5 e7d7 b6a5 E: -3.46 -3....
  Gen 2: Reward= 0.253 | M: c6f3 c6d7 d4d3 h8h4 a6c5 E: -4.11 -4....
  Gen 3: Reward= 0.700 | M: h8h3 c8b8 a6c5 b6a5 d4d3 E: -3.4 -3.4...
  Gen 4: Reward= 0.275 | M: h8h3 e7d7 a6c5 c8b8 e7e8 E: -4.03 -4....
  üìä Average reward: 0.376

üéØ Scoring Prompt 2 completions:
  Gen 1: Reward= 0.109 | M: e2f3 c8e7 e2f2 c8a7 e2d3 E: 0.22 0.21...
  Gen 2: Reward= 0.415 | M: f5g4 e2d3 e2f3 c8a7 c8e7 E: 0.12 0.13...
  Gen 3: Reward= 0.275 | M: e2f3 e2d3 c8e7 e2f2 e2f1 E: 0.0 0.0 0...
  Gen 4: Reward= 0.571 | M: e2f2 c8a7 e2d3 c8e7 e2f3 E: 0.02 0.02...
  üìä Average reward: 0.343

üéØ Scoring Prompt 3 completions:
  Gen 1: Reward= 0.630 | 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b...
  Gen 2: Reward= 0.630 | 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b...
  Gen 3: Reward= 0.630 | 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b...
  Gen 4: Reward= 0.630 | 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b...
  üìä Average reward: 0.630

üéØ Scoring Prompt 4 completions:
  Gen 1: Reward= 0.717 | M: e7e5 d7d5 d7d6 g7g6 e7e6 E: -0.45 -0....
  Gen 2: Reward= 0.402 | M: d7d5 e7e5 d7d6 e7e6 g7g6 E: -0.43 -0....
  Gen 3: Reward= 0.364 | M: e7e6 e7e5 d7d5 d7d6 g7g6 E: -0.74 -0....
  Gen 4: Reward= 0.365 | M: e7e5 d7d6 d7d5 e7e6 g7g6 E: -0.5 -0.5...
  üìä Average reward: 0.462

==================== PHASE 5: TRL EXACT ADVANTAGE CALCULATION (step 4) ====================
üî¢ TRL Advantage Calculation (exact formula):
  Total rewards shape: torch.Size([16])
  Rewards: [0.2776     0.25297143 0.7        0.275      0.10937143 0.415
 0.275      0.57097143 0.63       0.63       0.63       0.63
 0.71657143 0.40177143 0.36417143 0.36497143]

==================== PHASE 6-8: CORRECTED GRPO LOSS CALCULATION (step 4) ====================
üîÑ Computing GRPO loss with proper token-level KL...

üéØ Prompt 1 - Corrected GRPO Loss:
  Gen 1: A=-0.457, logp/len=-6.812, kl=-5.094
         PG=-3.109, KL_penalty=-0.508, total=-3.625
  Gen 2: A=-0.571, logp/len=-7.469, kl=-5.719
         PG=-4.281, KL_penalty=-0.570, total=-4.844
  Gen 3: A=+1.498, logp/len=-7.312, kl=-5.500
         PG=10.938, KL_penalty=-0.551, total=10.375
  Gen 4: A=-0.469, logp/len=-8.062, kl=-6.156
         PG=-3.781, KL_penalty=-0.617, total=-4.406
  üìä Prompt averages: PG=-0.055, KL=-0.562

üéØ Prompt 2 - Corrected GRPO Loss:
  Gen 1: A=-1.184, logp/len=-7.500, kl=-5.531
         PG=-8.875, KL_penalty=-0.555, total=-9.438
  Gen 2: A=+0.368, logp/len=-6.000, kl=-4.156
         PG=2.203, KL_penalty=-0.416, total=1.789
  Gen 3: A=-0.343, logp/len=-7.969, kl=-6.188
         PG=-2.734, KL_penalty=-0.617, total=-3.344
  Gen 4: A=+1.160, logp/len=-6.719, kl=-4.781
         PG=7.781, KL_penalty=-0.479, total=7.312
  üìä Prompt averages: PG=-0.414, KL=-0.516

üéØ Prompt 3 - Corrected GRPO Loss:
  Gen 1: A=+0.000, logp/len=-5.406, kl=-5.031
         PG=0.000, KL_penalty=-0.504, total=-0.504
  Gen 2: A=+0.000, logp/len=-6.000, kl=-5.656
         PG=0.000, KL_penalty=-0.566, total=-0.566
  Gen 3: A=+0.000, logp/len=-6.031, kl=-5.656
         PG=0.000, KL_penalty=-0.566, total=-0.566
  Gen 4: A=+0.000, logp/len=-6.094, kl=-5.719
         PG=0.000, KL_penalty=-0.570, total=-0.570
  üìä Prompt averages: PG=0.000, KL=-0.555

üéØ Prompt 4 - Corrected GRPO Loss:
  Gen 1: A=+1.492, logp/len=-5.906, kl=-4.344
         PG=8.812, KL_penalty=-0.434, total=8.375
  Gen 2: A=-0.352, logp/len=-5.688, kl=-4.062
         PG=-2.000, KL_penalty=-0.406, total=-2.406
  Gen 3: A=-0.572, logp/len=-7.375, kl=-5.719
         PG=-4.219, KL_penalty=-0.570, total=-4.781
  Gen 4: A=-0.568, logp/len=-7.562, kl=-5.906
         PG=-4.281, KL_penalty=-0.590, total=-4.875
  üìä Prompt averages: PG=-0.422, KL=-0.500

üî¢ CORRECTED Loss Breakdown:
  Policy Gradient Loss:   -0.223
  KL Loss (with gradients):   -0.531
  Total Loss:          =  -0.754
  KL penalty ratio: 70.5%

üîÑ Performing corrected gradient update (step 4)...
  Total gradient norm: 19.60
  ‚úÖ Gradient update applied with proper KL regularization

==================== PHASE 9: POST-UPDATE PERFORMANCE (step 4) ====================
  Prompt 1: avg reward = 0.158
  Prompt 2: avg reward = 0.387
  Prompt 3: avg reward = 0.630
  Prompt 4: avg reward = 0.559
üìä POST-UPDATE (step 4) Performance:
  Average reward: 0.4335
  Positive ratio: 100.0%

==================== PHASE 10: ANALYSIS (step 4) ====================
üîç GRPO Step Impact (step 4):
  Post-update performance: 0.4335
  Step performance change: +0.0001

======================================================================
üß≠ SEQUENTIAL STEP 5/50
======================================================================
üßæ Overfit mode: reusing initial batch (no dataset slice advance)

ü§ñ Generating completions for GRPO training (step 5)...

üéØ Prompt 1: P: 2k4r/ppp1q1p1/nbb1N3/4PP2/3p2P1/PB5P/1PPK3R/3RQ3 b - - 3 ...
    Gen 1: Train_LP= -404.0, Ref_LP=  -44.0
           Text:                                M: a6c5 c8b8 c6d7 e7d7 d4d3  ...
           Normalized: M: a6c5 c8b8 c6d7 e7d7 d4d3 E: -3.76 -3.84 -3.95 -3.9 -3.81 ...
    Gen 2: Train_LP= -348.0, Ref_LP=  -56.8
           Text:                                M: e7d7 a6c5 h8h4 c6d7 d4d3  ...
           Normalized: M: e7d7 a6c5 h8h4 c6d7 d4d3 E: -3.96 -3.51 -4.05 -3.85 -3.71...
    Gen 3: Train_LP= -636.0, Ref_LP=  -34.5
           Text:                                M: d4d3 a6c5 e7d7 c6b5 h8e8  ...
           Normalized: M: d4d3 a6c5 e7d7 c6b5 h8e8 E: -4.01 -4.2 -4.07 -4.21 -4.3 B...
    Gen 4: Train_LP= -568.0, Ref_LP=  -57.2
           Text:                                M: d4d3 c6f3 a6c5 c6d7 h8h3  ...
           Normalized: M: d4d3 c6f3 a6c5 c6d7 h8h3 E: -3.89 -4.22 -3.93 -4.19 -3.98...

üéØ Prompt 2: P: 2N5/2k5/3p1p2/3P1BbP/4Pp2/1n6/4K3/8 w - - 9 49...
    Gen 1: Train_LP= -322.0, Ref_LP=  -81.5
           Text:                                             M: e2d3 c8e7 e2f...
           Normalized: M: e2d3 c8e7 e2f3 e2f2 e2f1 E: 0.33 0.22 0.26 0.24 0.22 B: e...
    Gen 2: Train_LP= -700.0, Ref_LP=  -14.0
           Text:                                             M: e2f1 c8e7 e2f...
           Normalized: M: e2f1 c8e7 e2f3 e2f2 c8a7 E: 0.0 0.0 0.0 0.0 0.0 B: c8a7...
    Gen 3: Train_LP= -672.0, Ref_LP=  -69.0
           Text:                                             M: f5h3 e2f3 e2f...
           Normalized: M: f5h3 e2f3 e2f2 c8e7 f5g4 E: 0.12 0.1 0.12 0.14 0.14 B: f5...
    Gen 4: Train_LP= -812.0, Ref_LP=  -81.0
           Text:                                             M: f5g6 c8a7 c8e...
           Normalized: M: f5g6 c8a7 c8e7 e2d3 e2f2 E: 0.09 0.14 0.16 0.19 0.09 B: e...

üéØ Prompt 3: A: 8/2k2b2/R1p4r/3p1p2/P7/2P2B1P/5PPK/4R3 w - - 1 31+a6a5+f7...
    Gen 1: Train_LP= -250.0, Ref_LP=   -0.0
           Text: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
           Normalized: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
    Gen 2: Train_LP= -268.0, Ref_LP=   -0.0
           Text: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
           Normalized: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
    Gen 3: Train_LP= -306.0, Ref_LP=   -0.0
           Text: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
           Normalized: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
    Gen 4: Train_LP= -282.0, Ref_LP=   -0.0
           Text: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
           Normalized: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...

üéØ Prompt 4: P: rnbqkbnr/pp1ppppp/2p5/8/4P3/2N5/PPPP1PPP/R1BQKBNR b KQkq ...
    Gen 1: Train_LP= -524.0, Ref_LP=  -45.5
           Text:                             M: e7e6 d7d6 d7d5 e7e5 g7g6     ...
           Normalized: M: e7e6 d7d6 d7d5 e7e5 g7g6 E: -0.73 -0.63 -0.31 -0.63 -0.71...
    Gen 2: Train_LP= -652.0, Ref_LP=  -33.5
           Text:                             M: e7e5 d7d6 d7d5 e7e6 g7g6     ...
           Normalized: M: e7e5 d7d6 d7d5 e7e6 g7g6 E: -0.54 -0.65 -0.48 -0.78 -0.7 ...
    Gen 3: Train_LP= -680.0, Ref_LP=  -25.2
           Text:                             M: e7e5 d7d6 d7d5 g7g6 c6c5     ...
           Normalized: M: e7e5 d7d6 d7d5 g7g6 c6c5 E: -0.61 -0.73 -0.6 -0.8 -0.78 B...
    Gen 4: Train_LP= -446.0, Ref_LP=  -47.0
           Text:                             M: d7d6 d7d5 e7e5 g7g6 c6c5     ...
           Normalized: M: d7d6 d7d5 e7e5 g7g6 c6c5 E: -0.64 -0.39 -0.58 -0.77 -0.72...

==================== PHASE 4: REWARD CALCULATION (step 5) ====================

üéØ Scoring Prompt 1 completions:
  Gen 1: Reward= 0.279 | M: a6c5 c8b8 c6d7 e7d7 d4d3 E: -3.76 -3....
  Gen 2: Reward= 0.178 | M: e7d7 a6c5 h8h4 c6d7 d4d3 E: -3.96 -3....
  Gen 3: Reward= 0.235 | M: d4d3 a6c5 e7d7 c6b5 h8e8 E: -4.01 -4....
  Gen 4: Reward= 0.429 | M: d4d3 c6f3 a6c5 c6d7 h8h3 E: -3.89 -4....
  üìä Average reward: 0.280

üéØ Scoring Prompt 2 completions:
  Gen 1: Reward=-0.035 | M: e2d3 c8e7 e2f3 e2f2 e2f1 E: 0.33 0.22...
  Gen 2: Reward= 0.229 | M: e2f1 c8e7 e2f3 e2f2 c8a7 E: 0.0 0.0 0...
  Gen 3: Reward=-0.075 | M: f5h3 e2f3 e2f2 c8e7 f5g4 E: 0.12 0.1 ...
  Gen 4: Reward= 0.107 | M: f5g6 c8a7 c8e7 e2d3 e2f2 E: 0.09 0.14...
  üìä Average reward: 0.056

üéØ Scoring Prompt 3 completions:
  Gen 1: Reward= 0.630 | 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b...
  Gen 2: Reward= 0.630 | 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b...
  Gen 3: Reward= 0.630 | 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b...
  Gen 4: Reward= 0.630 | 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b...
  üìä Average reward: 0.630

üéØ Scoring Prompt 4 completions:
  Gen 1: Reward= 0.367 | M: e7e6 d7d6 d7d5 e7e5 g7g6 E: -0.73 -0....
  Gen 2: Reward= 0.369 | M: e7e5 d7d6 d7d5 e7e6 g7g6 E: -0.54 -0....
  Gen 3: Reward= 0.783 | M: e7e5 d7d6 d7d5 g7g6 c6c5 E: -0.61 -0....
  Gen 4: Reward= 0.792 | M: d7d6 d7d5 e7e5 g7g6 c6c5 E: -0.64 -0....
  üìä Average reward: 0.578

==================== PHASE 5: TRL EXACT ADVANTAGE CALCULATION (step 5) ====================
üî¢ TRL Advantage Calculation (exact formula):
  Total rewards shape: torch.Size([16])
  Rewards: [ 0.27857143  0.1778      0.2354      0.42857143 -0.035       0.22857143
 -0.075       0.10697143  0.63        0.63        0.63        0.63
  0.36737143  0.36937143  0.7828      0.7916    ]

==================== PHASE 6-8: CORRECTED GRPO LOSS CALCULATION (step 5) ====================
üîÑ Computing GRPO loss with proper token-level KL...

üéØ Prompt 1 - Corrected GRPO Loss:
  Gen 1: A=-0.014, logp/len=-6.188, kl=-4.406
         PG=-0.087, KL_penalty=-0.441, total=-0.527
  Gen 2: A=-0.954, logp/len=-7.250, kl=-5.406
         PG=-6.906, KL_penalty=-0.539, total=-7.438
  Gen 3: A=-0.417, logp/len=-7.594, kl=-5.875
         PG=-3.156, KL_penalty=-0.586, total=-3.750
  Gen 4: A=+1.384, logp/len=-7.438, kl=-5.500
         PG=10.312, KL_penalty=-0.551, total=9.750
  üìä Prompt averages: PG=0.047, KL=-0.527

üéØ Prompt 2 - Corrected GRPO Loss:
  Gen 1: A=-0.658, logp/len=-5.781, kl=-3.688
         PG=-3.812, KL_penalty=-0.369, total=-4.188
  Gen 2: A=+1.240, logp/len=-6.000, kl=-4.438
         PG=7.438, KL_penalty=-0.443, total=7.000
  Gen 3: A=-0.946, logp/len=-6.750, kl=-4.938
         PG=-6.375, KL_penalty=-0.494, total=-6.875
  Gen 4: A=+0.364, logp/len=-9.188, kl=-7.000
         PG=3.344, KL_penalty=-0.699, total=2.641
  üìä Prompt averages: PG=0.148, KL=-0.500

üéØ Prompt 3 - Corrected GRPO Loss:
  Gen 1: A=+0.000, logp/len=-5.469, kl=-5.125
         PG=0.000, KL_penalty=-0.512, total=-0.512
  Gen 2: A=+0.000, logp/len=-5.469, kl=-5.094
         PG=0.000, KL_penalty=-0.508, total=-0.508
  Gen 3: A=+0.000, logp/len=-5.625, kl=-5.250
         PG=0.000, KL_penalty=-0.523, total=-0.523
  Gen 4: A=+0.000, logp/len=-5.594, kl=-5.250
         PG=0.000, KL_penalty=-0.523, total=-0.523
  üìä Prompt averages: PG=0.000, KL=-0.516

üéØ Prompt 4 - Corrected GRPO Loss:
  Gen 1: A=-0.870, logp/len=-8.750, kl=-7.125
         PG=-7.625, KL_penalty=-0.711, total=-8.312
  Gen 2: A=-0.862, logp/len=-5.594, kl=-3.984
         PG=-4.812, KL_penalty=-0.398, total=-5.219
  Gen 3: A=+0.848, logp/len=-7.906, kl=-6.250
         PG=6.688, KL_penalty=-0.625, total=6.062
  Gen 4: A=+0.884, logp/len=-6.719, kl=-5.125
         PG=5.938, KL_penalty=-0.512, total=5.438
  üìä Prompt averages: PG=0.047, KL=-0.562

üî¢ CORRECTED Loss Breakdown:
  Policy Gradient Loss:    0.061
  KL Loss (with gradients):   -0.527
  Total Loss:          =  -0.467
  KL penalty ratio: 90.0%

üîÑ Performing corrected gradient update (step 5)...
  Total gradient norm: 27.91
  ‚úÖ Gradient update applied with proper KL regularization

==================== PHASE 9: POST-UPDATE PERFORMANCE (step 5) ====================
  Prompt 1: avg reward = 0.148
  Prompt 2: avg reward = 0.387
  Prompt 3: avg reward = 0.630
  Prompt 4: avg reward = 0.505
üìä POST-UPDATE (step 5) Performance:
  Average reward: 0.4176
  Positive ratio: 100.0%

==================== PHASE 10: ANALYSIS (step 5) ====================
üîç GRPO Step Impact (step 5):
  Post-update performance: 0.4176
  Step performance change: -0.0160

======================================================================
üß≠ SEQUENTIAL STEP 6/50
======================================================================
üßæ Overfit mode: reusing initial batch (no dataset slice advance)

ü§ñ Generating completions for GRPO training (step 6)...

üéØ Prompt 1: P: 2k4r/ppp1q1p1/nbb1N3/4PP2/3p2P1/PB5P/1PPK3R/3RQ3 b - - 3 ...
    Gen 1: Train_LP= -536.0, Ref_LP=  -33.5
           Text:                                M: c8b8 h8h6 a6c5 e7d7 h8e8  ...
           Normalized: M: c8b8 h8h6 a6c5 e7d7 h8e8 E: -3.82 -3.82 -3.66 -3.9 -3.82 ...
    Gen 2: Train_LP= -510.0, Ref_LP=  -36.5
           Text:                                M: c6d7 d4d3 e7h4 h8h4 a6c5  ...
           Normalized: M: c6d7 d4d3 e7h4 h8h4 a6c5 E: -3.68 -3.67 -3.8 -3.68 -3.49 ...
    Gen 3: Train_LP= -388.0, Ref_LP=  -45.5
           Text:                                M: a6c5 h8h6 c8b8 c6b5 d4d3  ...
           Normalized: M: a6c5 h8h6 c8b8 c6b5 d4d3 E: -3.39 -3.69 -3.44 -3.45 -3.29...
    Gen 4: Train_LP= -544.0, Ref_LP=  -45.2
           Text:                                M: d4d3 c6f3 e7e8 c8b8 a6c5  ...
           Normalized: M: d4d3 c6f3 e7e8 c8b8 a6c5 E: -3.71 -3.87 -3.77 -3.85 -3.49...

üéØ Prompt 2: P: 2N5/2k5/3p1p2/3P1BbP/4Pp2/1n6/4K3/8 w - - 9 49...
    Gen 1: Train_LP= -676.0, Ref_LP=  -80.5
           Text:                                             M: f5g6 e2f3 c8e...
           Normalized: M: f5g6 e2f3 c8e7 e2f2 c8a7 E: 0.32 0.32 0.26 0.32 0.32 B: e...
    Gen 2: Train_LP= -684.0, Ref_LP=  -16.4
           Text:                                             M: e2f3 e2d3 e2f...
           Normalized: M: e2f3 e2d3 e2f2 e2d1 c8e7 E: 0.0 0.0 0.0 0.0 0.0 B: e2f3...
    Gen 3: Train_LP= -536.0, Ref_LP=  -16.5
           Text:                                             M: e2f3 f5h3 e2f...
           Normalized: M: e2f3 f5h3 e2f2 c8a7 c8e7 E: 0.0 0.0 0.0 0.0 0.0 B: c8e7...
    Gen 4: Train_LP= -740.0, Ref_LP=  -80.0
           Text:                                             M: f5g4 c8e7 e2f...
           Normalized: M: f5g4 c8e7 e2f2 e2f3 f5h3 E: 0.08 0.05 0.06 0.09 0.05 B: e...

üéØ Prompt 3: A: 8/2k2b2/R1p4r/3p1p2/P7/2P2B1P/5PPK/4R3 w - - 1 31+a6a5+f7...
    Gen 1: Train_LP= -328.0, Ref_LP=   -0.0
           Text: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
           Normalized: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
    Gen 2: Train_LP= -264.0, Ref_LP=   -0.0
           Text: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
           Normalized: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
    Gen 3: Train_LP= -253.0, Ref_LP=   -0.0
           Text: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
           Normalized: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
    Gen 4: Train_LP= -332.0, Ref_LP=   -0.0
           Text: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
           Normalized: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...

üéØ Prompt 4: P: rnbqkbnr/pp1ppppp/2p5/8/4P3/2N5/PPPP1PPP/R1BQKBNR b KQkq ...
    Gen 1: Train_LP= -736.0, Ref_LP=  -23.4
           Text:                             M: e7e5 d7d5 d7d6 g7g6 e7e6     ...
           Normalized: M: e7e5 d7d5 d7d6 g7g6 e7e6 E: -0.58 -0.38 -0.7 -0.74 -0.74 ...
    Gen 2: Train_LP= -388.0, Ref_LP=  -24.4
           Text:                             M: d7d5 d7d6 g7g6 e7e5 c6c5     ...
           Normalized: M: d7d5 d7d6 g7g6 e7e5 c6c5 E: -0.34 -0.72 -0.79 -0.54 -0.7 ...
    Gen 3: Train_LP= -556.0, Ref_LP=  -34.5
           Text:                             M: d7d6 g7g6 d7d5 e7e5 e7e6     ...
           Normalized: M: d7d6 g7g6 d7d5 e7e5 e7e6 E: -0.67 -0.65 -0.53 -0.65 -0.65...
    Gen 4: Train_LP= -612.0, Ref_LP=  -25.1
           Text:                             M: d7d6 e7e5 d7d5 g7g6 d8c7     ...
           Normalized: M: d7d6 e7e5 d7d5 g7g6 d8c7 E: -0.72 -0.7 -0.45 -0.73 -0.86 ...

==================== PHASE 4: REWARD CALCULATION (step 6) ====================

üéØ Scoring Prompt 1 completions:
  Gen 1: Reward= 0.182 | M: c8b8 h8h6 a6c5 e7d7 h8e8 E: -3.82 -3....
  Gen 2: Reward= 0.125 | M: c6d7 d4d3 e7h4 h8h4 a6c5 E: -3.68 -3....
  Gen 3: Reward= 0.213 | M: a6c5 h8h6 c8b8 c6b5 d4d3 E: -3.39 -3....
  Gen 4: Reward= 0.589 | M: d4d3 c6f3 e7e8 c8b8 a6c5 E: -3.71 -3....
  üìä Average reward: 0.277

üéØ Scoring Prompt 2 completions:
  Gen 1: Reward= 0.019 | M: f5g6 e2f3 c8e7 e2f2 c8a7 E: 0.32 0.32...
  Gen 2: Reward= 0.019 | M: e2f3 e2d3 e2f2 e2d1 c8e7 E: 0.0 0.0 0...
  Gen 3: Reward= 0.429 | M: e2f3 f5h3 e2f2 c8a7 c8e7 E: 0.0 0.0 0...
  Gen 4: Reward=-0.075 | M: f5g4 c8e7 e2f2 e2f3 f5h3 E: 0.08 0.05...
  üìä Average reward: 0.098

üéØ Scoring Prompt 3 completions:
  Gen 1: Reward= 0.630 | 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b...
  Gen 2: Reward= 0.630 | 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b...
  Gen 3: Reward= 0.630 | 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b...
  Gen 4: Reward= 0.630 | 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b...
  üìä Average reward: 0.630

üéØ Scoring Prompt 4 completions:
  Gen 1: Reward= 0.371 | M: e7e5 d7d5 d7d6 g7g6 e7e6 E: -0.58 -0....
  Gen 2: Reward= 0.813 | M: d7d5 d7d6 g7g6 e7e5 c6c5 E: -0.34 -0....
  Gen 3: Reward= 0.532 | M: d7d6 g7g6 d7d5 e7e5 e7e6 E: -0.67 -0....
  Gen 4: Reward= 0.365 | M: d7d6 e7e5 d7d5 g7g6 d8c7 E: -0.72 -0....
  üìä Average reward: 0.520

==================== PHASE 5: TRL EXACT ADVANTAGE CALCULATION (step 6) ====================
üî¢ TRL Advantage Calculation (exact formula):
  Total rewards shape: torch.Size([16])
  Rewards: [ 0.1818      0.125       0.21297143  0.5892      0.01857143  0.01897143
  0.42857143 -0.075       0.63        0.63        0.63        0.63
  0.37057143  0.8128      0.53217143  0.36457143]

==================== PHASE 6-8: CORRECTED GRPO LOSS CALCULATION (step 6) ====================
üîÑ Computing GRPO loss with proper token-level KL...

üéØ Prompt 1 - Corrected GRPO Loss:
  Gen 1: A=-0.452, logp/len=-5.719, kl=-4.000
         PG=-2.578, KL_penalty=-0.400, total=-2.984
  Gen 2: A=-0.721, logp/len=-6.844, kl=-5.094
         PG=-4.938, KL_penalty=-0.508, total=-5.438
  Gen 3: A=-0.304, logp/len=-5.875, kl=-4.125
         PG=-1.789, KL_penalty=-0.412, total=-2.203
  Gen 4: A=+1.478, logp/len=-6.688, kl=-4.906
         PG=9.875, KL_penalty=-0.490, total=9.375
  üìä Prompt averages: PG=0.141, KL=-0.453

üéØ Prompt 2 - Corrected GRPO Loss:
  Gen 1: A=-0.352, logp/len=-7.469, kl=-5.594
         PG=-2.625, KL_penalty=-0.559, total=-3.188
  Gen 2: A=-0.350, logp/len=-7.375, kl=-5.812
         PG=-2.578, KL_penalty=-0.582, total=-3.156
  Gen 3: A=+1.471, logp/len=-7.438, kl=-5.781
         PG=10.938, KL_penalty=-0.578, total=10.375
  Gen 4: A=-0.768, logp/len=-7.156, kl=-5.219
         PG=-5.500, KL_penalty=-0.523, total=-6.031
  üìä Prompt averages: PG=0.062, KL=-0.562

üéØ Prompt 3 - Corrected GRPO Loss:
  Gen 1: A=+0.000, logp/len=-5.250, kl=-4.906
         PG=0.000, KL_penalty=-0.490, total=-0.490
  Gen 2: A=+0.000, logp/len=-6.594, kl=-6.250
         PG=0.000, KL_penalty=-0.625, total=-0.625
  Gen 3: A=+0.000, logp/len=-5.438, kl=-5.094
         PG=0.000, KL_penalty=-0.508, total=-0.508
  Gen 4: A=+0.000, logp/len=-5.094, kl=-4.719
         PG=0.000, KL_penalty=-0.473, total=-0.473
  üìä Prompt averages: PG=0.000, KL=-0.523

üéØ Prompt 4 - Corrected GRPO Loss:
  Gen 1: A=-0.712, logp/len=-7.469, kl=-5.938
         PG=-5.312, KL_penalty=-0.594, total=-5.906
  Gen 2: A=+1.394, logp/len=-8.250, kl=-6.469
         PG=11.500, KL_penalty=-0.648, total=10.875
  Gen 3: A=+0.058, logp/len=-9.438, kl=-7.750
         PG=0.547, KL_penalty=-0.773, total=-0.227
  Gen 4: A=-0.740, logp/len=-9.500, kl=-8.000
         PG=-7.031, KL_penalty=-0.801, total=-7.844
  üìä Prompt averages: PG=-0.070, KL=-0.703

üî¢ CORRECTED Loss Breakdown:
  Policy Gradient Loss:    0.033
  KL Loss (with gradients):   -0.562
  Total Loss:          =  -0.531
  KL penalty ratio: 95.0%

üîÑ Performing corrected gradient update (step 6)...
  Total gradient norm: 33.38
  ‚úÖ Gradient update applied with proper KL regularization

==================== PHASE 9: POST-UPDATE PERFORMANCE (step 6) ====================
  Prompt 1: avg reward = 0.160
  Prompt 2: avg reward = 0.387
  Prompt 3: avg reward = 0.630
  Prompt 4: avg reward = 0.560
üìä POST-UPDATE (step 6) Performance:
  Average reward: 0.4342
  Positive ratio: 100.0%

==================== PHASE 10: ANALYSIS (step 6) ====================
üîç GRPO Step Impact (step 6):
  Post-update performance: 0.4342
  Step performance change: +0.0166

======================================================================
üß≠ SEQUENTIAL STEP 7/50
======================================================================
üßæ Overfit mode: reusing initial batch (no dataset slice advance)

ü§ñ Generating completions for GRPO training (step 7)...

üéØ Prompt 1: P: 2k4r/ppp1q1p1/nbb1N3/4PP2/3p2P1/PB5P/1PPK3R/3RQ3 b - - 3 ...
    Gen 1: Train_LP= -560.0, Ref_LP=  -47.0
           Text:                                M: h8h3 a6c5 e7h4 h8e8 c8b8  ...
           Normalized: M: h8h3 a6c5 e7h4 h8e8 c8b8 E: -4.27 -4.22 -4.26 -4.26 -4.15...
    Gen 2: Train_LP= -418.0, Ref_LP=  -45.2
           Text:                                M: a6c5 d4d3 e7d7 h8h3 h8e8  ...
           Normalized: M: a6c5 d4d3 e7d7 h8h3 h8e8 E: -4.17 -4.18 -4.33 -4.17 -4.16...
    Gen 3: Train_LP= -584.0, Ref_LP=  -33.8
           Text:                                M: h8e8 a6c5 c8b8 c6d7 b6a5  ...
           Normalized: M: h8e8 a6c5 c8b8 c6d7 b6a5 E: -3.76 -3.46 -3.65 -3.77 -3.6 ...
    Gen 4: Train_LP= -628.0, Ref_LP=  -44.2
           Text:                                M: d4d3 c6d7 a6c5 c8b8 e7d7  ...
           Normalized: M: d4d3 c6d7 a6c5 c8b8 e7d7 E: -3.63 -3.85 -3.68 -3.93 -3.84...

üéØ Prompt 2: P: 2N5/2k5/3p1p2/3P1BbP/4Pp2/1n6/4K3/8 w - - 9 49...
    Gen 1: Train_LP= -428.0, Ref_LP=  -13.2
           Text:                                             M: e2f1 e2f2 e2f...
           Normalized: M: e2f1 e2f2 e2f3 c8e7 c8a7 E: 0.0 0.0 0.0 0.0 0.0 B: e2f3...
    Gen 2: Train_LP= -466.0, Ref_LP=  -17.2
           Text:                                             M: e2f3 e2f2 e2e...
           Normalized: M: e2f3 e2f2 e2e1 c8a7 e2d1 E: 0.0 0.0 0.0 0.0 0.0 B: e2f3...
    Gen 3: Train_LP= -696.0, Ref_LP=  -48.8
           Text:                                             M: e2d3 e2f3 f5g...
           Normalized: M: e2d3 e2f3 f5g4 c8e7 e2f2 E: 0.38 0.32 0.3 0.3 0.3 B: e2d3...
    Gen 4: Train_LP= -536.0, Ref_LP=  -82.0
           Text:                                             M: f5g4 c8e7 f5h...
           Normalized: M: f5g4 c8e7 f5h3 e2f3 f5e6 E: 0.26 0.25 0.28 0.28 0.27 B: f...

üéØ Prompt 3: A: 8/2k2b2/R1p4r/3p1p2/P7/2P2B1P/5PPK/4R3 w - - 1 31+a6a5+f7...
    Gen 1: Train_LP= -234.0, Ref_LP=   -0.0
           Text: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
           Normalized: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
    Gen 2: Train_LP= -304.0, Ref_LP=   -0.0
           Text: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
           Normalized: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
    Gen 3: Train_LP= -322.0, Ref_LP=   -0.0
           Text: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
           Normalized: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
    Gen 4: Train_LP= -264.0, Ref_LP=   -0.0
           Text: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
           Normalized: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...

üéØ Prompt 4: P: rnbqkbnr/pp1ppppp/2p5/8/4P3/2N5/PPPP1PPP/R1BQKBNR b KQkq ...
    Gen 1: Train_LP= -672.0, Ref_LP=  -23.4
           Text:                             M: e7e5 d7d5 g7g6 d7d6 e7e6     ...
           Normalized: M: e7e5 d7d5 g7g6 d7d6 e7e6 E: -0.54 -0.42 -0.61 -0.62 -0.65...
    Gen 2: Train_LP= -592.0, Ref_LP=  -25.8
           Text:                             M: d7d6 e7e5 d7d5 g7g6 g8f6     ...
           Normalized: M: d7d6 e7e5 d7d5 g7g6 g8f6 E: -0.55 -0.52 -0.43 -0.76 -0.77...
    Gen 3: Train_LP= -584.0, Ref_LP=  -24.5
           Text:                             M: d7d6 e7e5 d7d5 g7g6 d8c7     ...
           Normalized: M: d7d6 e7e5 d7d5 g7g6 d8c7 E: -0.71 -0.62 -0.43 -0.84 -0.87...
    Gen 4: Train_LP= -768.0, Ref_LP=  -23.8
           Text:                             M: d7d5 d7d6 g7g6 e7e5 e7e6     ...
           Normalized: M: d7d5 d7d6 g7g6 e7e5 e7e6 E: -0.38 -0.74 -0.65 -0.59 -0.67...

==================== PHASE 4: REWARD CALCULATION (step 7) ====================

üéØ Scoring Prompt 1 completions:
  Gen 1: Reward= 0.145 | M: h8h3 a6c5 e7h4 h8e8 c8b8 E: -4.27 -4....
  Gen 2: Reward= 0.136 | M: a6c5 d4d3 e7d7 h8h3 h8e8 E: -4.17 -4....
  Gen 3: Reward= 0.241 | M: h8e8 a6c5 c8b8 c6d7 b6a5 E: -3.76 -3....
  Gen 4: Reward= 0.426 | M: d4d3 c6d7 a6c5 c8b8 e7d7 E: -3.63 -3....
  üìä Average reward: 0.237

üéØ Scoring Prompt 2 completions:
  Gen 1: Reward=-0.001 | M: e2f1 e2f2 e2f3 c8e7 c8a7 E: 0.0 0.0 0...
  Gen 2: Reward= 0.121 | M: e2f3 e2f2 e2e1 c8a7 e2d1 E: 0.0 0.0 0...
  Gen 3: Reward=-0.035 | M: e2d3 e2f3 f5g4 c8e7 e2f2 E: 0.38 0.32...
  Gen 4: Reward=-0.117 | M: f5g4 c8e7 f5h3 e2f3 f5e6 E: 0.26 0.25...
  üìä Average reward: -0.008

üéØ Scoring Prompt 3 completions:
  Gen 1: Reward= 0.630 | 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b...
  Gen 2: Reward= 0.630 | 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b...
  Gen 3: Reward= 0.630 | 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b...
  Gen 4: Reward= 0.630 | 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b...
  üìä Average reward: 0.630

üéØ Scoring Prompt 4 completions:
  Gen 1: Reward= 0.367 | M: e7e5 d7d5 g7g6 d7d6 e7e6 E: -0.54 -0....
  Gen 2: Reward= 0.494 | M: d7d6 e7e5 d7d5 g7g6 g8f6 E: -0.55 -0....
  Gen 3: Reward= 0.360 | M: d7d6 e7e5 d7d5 g7g6 d8c7 E: -0.71 -0....
  Gen 4: Reward= 0.546 | M: d7d5 d7d6 g7g6 e7e5 e7e6 E: -0.38 -0....
  üìä Average reward: 0.442

==================== PHASE 5: TRL EXACT ADVANTAGE CALCULATION (step 7) ====================
üî¢ TRL Advantage Calculation (exact formula):
  Total rewards shape: torch.Size([16])
  Rewards: [ 0.145       0.1362      0.24057143  0.42617143 -0.00142857  0.12097143
 -0.035      -0.11666667  0.63        0.63        0.63        0.63
  0.36657143  0.494       0.36017143  0.54617143]

==================== PHASE 6-8: CORRECTED GRPO LOSS CALCULATION (step 7) ====================
üîÑ Computing GRPO loss with proper token-level KL...

üéØ Prompt 1 - Corrected GRPO Loss:
  Gen 1: A=-0.683, logp/len=-5.562, kl=-3.875
         PG=-3.797, KL_penalty=-0.387, total=-4.188
  Gen 2: A=-0.748, logp/len=-6.875, kl=-5.188
         PG=-5.156, KL_penalty=-0.520, total=-5.688
  Gen 3: A=+0.027, logp/len=-5.906, kl=-4.094
         PG=0.157, KL_penalty=-0.410, total=-0.254
  Gen 4: A=+1.405, logp/len=-7.531, kl=-5.656
         PG=10.562, KL_penalty=-0.566, total=10.000
  üìä Prompt averages: PG=0.453, KL=-0.469

üéØ Prompt 2 - Corrected GRPO Loss:
  Gen 1: A=+0.067, logp/len=-6.656, kl=-5.188
         PG=0.445, KL_penalty=-0.520, total=-0.074
  Gen 2: A=+1.307, logp/len=-7.344, kl=-5.625
         PG=9.625, KL_penalty=-0.562, total=9.062
  Gen 3: A=-0.273, logp/len=-8.062, kl=-6.031
         PG=-2.203, KL_penalty=-0.602, total=-2.812
  Gen 4: A=-1.101, logp/len=-8.062, kl=-6.156
         PG=-8.875, KL_penalty=-0.617, total=-9.500
  üìä Prompt averages: PG=-0.250, KL=-0.574

üéØ Prompt 3 - Corrected GRPO Loss:
  Gen 1: A=+0.000, logp/len=-4.906, kl=-4.531
         PG=0.000, KL_penalty=-0.453, total=-0.453
  Gen 2: A=+0.000, logp/len=-5.031, kl=-4.656
         PG=0.000, KL_penalty=-0.465, total=-0.465
  Gen 3: A=+0.000, logp/len=-6.469, kl=-6.125
         PG=0.000, KL_penalty=-0.613, total=-0.613
  Gen 4: A=+0.000, logp/len=-6.250, kl=-5.875
         PG=0.000, KL_penalty=-0.586, total=-0.586
  üìä Prompt averages: PG=0.000, KL=-0.531

üéØ Prompt 4 - Corrected GRPO Loss:
  Gen 1: A=-0.808, logp/len=-7.562, kl=-5.969
         PG=-6.125, KL_penalty=-0.598, total=-6.719
  Gen 2: A=+0.562, logp/len=-6.312, kl=-4.906
         PG=3.547, KL_penalty=-0.490, total=3.062
  Gen 3: A=-0.877, logp/len=-7.281, kl=-5.812
         PG=-6.375, KL_penalty=-0.582, total=-6.969
  Gen 4: A=+1.123, logp/len=-6.969, kl=-5.219
         PG=7.812, KL_penalty=-0.523, total=7.281
  üìä Prompt averages: PG=-0.281, KL=-0.547

üî¢ CORRECTED Loss Breakdown:
  Policy Gradient Loss:   -0.020
  KL Loss (with gradients):   -0.531
  Total Loss:          =  -0.551
  KL penalty ratio: 96.5%

üîÑ Performing corrected gradient update (step 7)...
  Total gradient norm: 25.21
  ‚úÖ Gradient update applied with proper KL regularization

==================== PHASE 9: POST-UPDATE PERFORMANCE (step 7) ====================
  Prompt 1: avg reward = 0.154
  Prompt 2: avg reward = 0.212
  Prompt 3: avg reward = 0.630
  Prompt 4: avg reward = 0.504
üìä POST-UPDATE (step 7) Performance:
  Average reward: 0.3749
  Positive ratio: 87.5%

==================== PHASE 10: ANALYSIS (step 7) ====================
üîç GRPO Step Impact (step 7):
  Post-update performance: 0.3749
  Step performance change: -0.0593

======================================================================
üß≠ SEQUENTIAL STEP 8/50
======================================================================
üßæ Overfit mode: reusing initial batch (no dataset slice advance)

ü§ñ Generating completions for GRPO training (step 8)...

üéØ Prompt 1: P: 2k4r/ppp1q1p1/nbb1N3/4PP2/3p2P1/PB5P/1PPK3R/3RQ3 b - - 3 ...
    Gen 1: Train_LP= -426.0, Ref_LP=  -47.0
           Text:                                M: h8h3 a6c5 c8b8 e7e8 e7d7  ...
           Normalized: M: h8h3 a6c5 c8b8 e7e8 e7d7 E: -3.99 -3.5 -3.71 -3.93 -3.65 ...
    Gen 2: Train_LP= -416.0, Ref_LP=  -45.0
           Text:                                M: b6a5 c6d7 a6c5 e7d7 d4d3  ...
           Normalized: M: b6a5 c6d7 a6c5 e7d7 d4d3 E: -4.1 -3.95 -3.62 -3.82 -3.69 ...
    Gen 3: Train_LP= -428.0, Ref_LP=  -55.0
           Text:                                M: e7e8 a6c5 h8e8 c8b8 d4d3  ...
           Normalized: M: e7e8 a6c5 h8e8 c8b8 d4d3 E: -3.82 -3.51 -3.89 -3.88 -3.85...
    Gen 4: Train_LP= -604.0, Ref_LP=  -32.5
           Text:                                M: a6c5 d4d3 e7d7 c8b8 h8e8  ...
           Normalized: M: a6c5 d4d3 e7d7 c8b8 h8e8 E: -3.7 -3.6 -3.97 -3.77 -4.06 B...

üéØ Prompt 2: P: 2N5/2k5/3p1p2/3P1BbP/4Pp2/1n6/4K3/8 w - - 9 49...
    Gen 1: Train_LP= -432.0, Ref_LP=  -57.8
           Text:                                             M: e2f2 c8a7 e2f...
           Normalized: M: e2f2 c8a7 e2f1 c8e7 e2f3 E: 0.22 0.16 0.13 0.15 0.18 B: e...
    Gen 2: Train_LP= -460.0, Ref_LP=  -26.0
           Text:                                             M: c8e7 e2f3 e2f...
           Normalized: M: c8e7 e2f3 e2f2 f5g6 c8a7 E: 0.3 0.2 0.2 0.21 0.21 B: c8e7...
    Gen 3: Train_LP= -374.0, Ref_LP=  -46.0
           Text:                                             M: e2f2 c8e7 e2f...
           Normalized: M: e2f2 c8e7 e2f3 c8a7 e2f1 E: 0.36 0.37 0.4 0.38 0.38 B: e2...
    Gen 4: Train_LP= -560.0, Ref_LP=  -56.0
           Text:                                             M: f5g6 c8a7 e2f...
           Normalized: M: f5g6 c8a7 e2f2 e2f3 c8e7 E: 0.15 0.16 0.16 0.15 0.15 B: c...

üéØ Prompt 3: A: 8/2k2b2/R1p4r/3p1p2/P7/2P2B1P/5PPK/4R3 w - - 1 31+a6a5+f7...
    Gen 1: Train_LP= -280.0, Ref_LP=   -0.0
           Text: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
           Normalized: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
    Gen 2: Train_LP= -256.0, Ref_LP=   -0.0
           Text: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
           Normalized: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
    Gen 3: Train_LP= -342.0, Ref_LP=   -0.0
           Text: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
           Normalized: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
    Gen 4: Train_LP= -272.0, Ref_LP=   -0.0
           Text: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
           Normalized: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...

üéØ Prompt 4: P: rnbqkbnr/pp1ppppp/2p5/8/4P3/2N5/PPPP1PPP/R1BQKBNR b KQkq ...
    Gen 1: Train_LP= -516.0, Ref_LP=  -23.5
           Text:                             M: d7d5 g7g6 d7d6 e7e5 e7e6     ...
           Normalized: M: d7d5 g7g6 d7d6 e7e5 e7e6 E: -0.53 -0.6 -0.57 -0.57 -0.9 B...
    Gen 2: Train_LP= -448.0, Ref_LP=  -35.8
           Text:                             M: d7d6 e7e5 g7g6 d7d5 e7e6     ...
           Normalized: M: d7d6 e7e5 g7g6 d7d5 e7e6 E: -0.58 -0.5 -0.59 -0.35 -0.77 ...
    Gen 3: Train_LP= -536.0, Ref_LP=  -46.8
           Text:                             M: d7d6 e7e6 d7d5 g7g6 e7e5     ...
           Normalized: M: d7d6 e7e6 d7d5 g7g6 e7e5 E: -0.79 -0.84 -0.35 -0.83 -0.49...
    Gen 4: Train_LP= -454.0, Ref_LP=  -24.6
           Text:                             M: e7e6 d7d6 d7d5 e7e5 g7g6     ...
           Normalized: M: e7e6 d7d6 d7d5 e7e5 g7g6 E: -0.77 -0.69 -0.5 -0.6 -0.68 B...

==================== PHASE 4: REWARD CALCULATION (step 8) ====================

üéØ Scoring Prompt 1 completions:
  Gen 1: Reward= 0.145 | M: h8h3 a6c5 c8b8 e7e8 e7d7 E: -3.99 -3....
  Gen 2: Reward= 0.689 | M: b6a5 c6d7 a6c5 e7d7 d4d3 E: -4.1 -3.9...
  Gen 3: Reward= 0.266 | M: e7e8 a6c5 h8e8 c8b8 d4d3 E: -3.82 -3....
  Gen 4: Reward= 0.262 | M: a6c5 d4d3 e7d7 c8b8 h8e8 E: -3.7 -3.6...
  üìä Average reward: 0.341

üéØ Scoring Prompt 2 completions:
  Gen 1: Reward= 0.369 | M: e2f2 c8a7 e2f1 c8e7 e2f3 E: 0.22 0.16...
  Gen 2: Reward= 0.499 | M: c8e7 e2f3 e2f2 f5g6 c8a7 E: 0.3 0.2 0...
  Gen 3: Reward= 0.019 | M: e2f2 c8e7 e2f3 c8a7 e2f1 E: 0.36 0.37...
  Gen 4: Reward= 0.269 | M: f5g6 c8a7 e2f2 e2f3 c8e7 E: 0.15 0.16...
  üìä Average reward: 0.289

üéØ Scoring Prompt 3 completions:
  Gen 1: Reward= 0.630 | 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b...
  Gen 2: Reward= 0.630 | 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b...
  Gen 3: Reward= 0.630 | 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b...
  Gen 4: Reward= 0.630 | 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b...
  üìä Average reward: 0.630

üéØ Scoring Prompt 4 completions:
  Gen 1: Reward= 0.409 | M: d7d5 g7g6 d7d6 e7e5 e7e6 E: -0.53 -0....
  Gen 2: Reward= 0.371 | M: d7d6 e7e5 g7g6 d7d5 e7e6 E: -0.58 -0....
  Gen 3: Reward= 0.351 | M: d7d6 e7e6 d7d5 g7g6 e7e5 E: -0.79 -0....
  Gen 4: Reward= 0.367 | M: e7e6 d7d6 d7d5 e7e5 g7g6 E: -0.77 -0....
  üìä Average reward: 0.374

==================== PHASE 5: TRL EXACT ADVANTAGE CALCULATION (step 8) ====================
üî¢ TRL Advantage Calculation (exact formula):
  Total rewards shape: torch.Size([16])
  Rewards: [0.145      0.68937143 0.26577143 0.26217143 0.36857143 0.49857143
 0.01857143 0.26857143 0.63       0.63       0.63       0.63
 0.40937143 0.37057143 0.35057143 0.36657143]

==================== PHASE 6-8: CORRECTED GRPO LOSS CALCULATION (step 8) ====================
üîÑ Computing GRPO loss with proper token-level KL...

üéØ Prompt 1 - Corrected GRPO Loss:
  Gen 1: A=-0.818, logp/len=-7.812, kl=-5.781
         PG=-6.375, KL_penalty=-0.578, total=-6.938
  Gen 2: A=+1.458, logp/len=-7.688, kl=-5.906
         PG=11.188, KL_penalty=-0.590, total=10.625
  Gen 3: A=-0.313, logp/len=-6.531, kl=-4.875
         PG=-2.047, KL_penalty=-0.488, total=-2.531
  Gen 4: A=-0.328, logp/len=-7.312, kl=-5.719
         PG=-2.391, KL_penalty=-0.570, total=-2.969
  üìä Prompt averages: PG=0.094, KL=-0.555

üéØ Prompt 2 - Corrected GRPO Loss:
  Gen 1: A=+0.394, logp/len=-6.281, kl=-4.312
         PG=2.469, KL_penalty=-0.432, total=2.031
  Gen 2: A=+1.034, logp/len=-7.469, kl=-5.656
         PG=7.719, KL_penalty=-0.566, total=7.156
  Gen 3: A=-1.329, logp/len=-9.625, kl=-7.750
         PG=-12.812, KL_penalty=-0.773, total=-13.562
  Gen 4: A=-0.098, logp/len=-5.500, kl=-3.641
         PG=-0.543, KL_penalty=-0.363, total=-0.906
  üìä Prompt averages: PG=-0.793, KL=-0.535

üéØ Prompt 3 - Corrected GRPO Loss:
  Gen 1: A=+0.000, logp/len=-5.375, kl=-5.000
         PG=0.000, KL_penalty=-0.500, total=-0.500
  Gen 2: A=+0.000, logp/len=-6.375, kl=-6.000
         PG=0.000, KL_penalty=-0.602, total=-0.602
  Gen 3: A=+0.000, logp/len=-5.094, kl=-4.719
         PG=0.000, KL_penalty=-0.473, total=-0.473
  Gen 4: A=+0.000, logp/len=-6.000, kl=-5.625
         PG=0.000, KL_penalty=-0.562, total=-0.562
  üìä Prompt averages: PG=0.000, KL=-0.535

üéØ Prompt 4 - Corrected GRPO Loss:
  Gen 1: A=+1.407, logp/len=-6.281, kl=-4.531
         PG=8.812, KL_penalty=-0.453, total=8.375
  Gen 2: A=-0.148, logp/len=-8.250, kl=-6.594
         PG=-1.227, KL_penalty=-0.660, total=-1.891
  Gen 3: A=-0.950, logp/len=-8.875, kl=-7.344
         PG=-8.438, KL_penalty=-0.734, total=-9.188
  Gen 4: A=-0.309, logp/len=-6.906, kl=-5.281
         PG=-2.125, KL_penalty=-0.527, total=-2.656
  üìä Prompt averages: PG=-0.742, KL=-0.594

üî¢ CORRECTED Loss Breakdown:
  Policy Gradient Loss:   -0.359
  KL Loss (with gradients):   -0.555
  Total Loss:          =  -0.914
  KL penalty ratio: 60.5%

üîÑ Performing corrected gradient update (step 8)...
  Total gradient norm: 25.22
  ‚úÖ Gradient update applied with proper KL regularization

==================== PHASE 9: POST-UPDATE PERFORMANCE (step 8) ====================
  Prompt 1: avg reward = 0.153
  Prompt 2: avg reward = 0.387
  Prompt 3: avg reward = 0.630
  Prompt 4: avg reward = 0.560
üìä POST-UPDATE (step 8) Performance:
  Average reward: 0.4323
  Positive ratio: 100.0%

==================== PHASE 10: ANALYSIS (step 8) ====================
üîç GRPO Step Impact (step 8):
  Post-update performance: 0.4323
  Step performance change: +0.0575

======================================================================
üß≠ SEQUENTIAL STEP 9/50
======================================================================
üßæ Overfit mode: reusing initial batch (no dataset slice advance)

ü§ñ Generating completions for GRPO training (step 9)...

üéØ Prompt 1: P: 2k4r/ppp1q1p1/nbb1N3/4PP2/3p2P1/PB5P/1PPK3R/3RQ3 b - - 3 ...
    Gen 1: Train_LP= -668.0, Ref_LP=  -44.8
           Text:                                M: a6c5 c8b8 h8e8 c6d7 e7d7  ...
           Normalized: M: a6c5 c8b8 h8e8 c6d7 e7d7 E: -3.32 -3.53 -3.78 -3.82 -3.73...
    Gen 2: Train_LP= -656.0, Ref_LP=  -46.8
           Text:                                M: h8h3 a6c5 c6d7 b6a5 d4d3  ...
           Normalized: M: h8h3 a6c5 c6d7 b6a5 d4d3 E: -3.48 -3.44 -3.44 -3.48 -3.43...
    Gen 3: Train_LP= -544.0, Ref_LP=  -36.5
           Text:                                M: h8h6 a6c5 e7h4 c6f3 c8b8  ...
           Normalized: M: h8h6 a6c5 e7h4 c6f3 c8b8 E: -4.07 -3.73 -4.22 -3.8 -3.79 ...
    Gen 4: Train_LP= -556.0, Ref_LP=  -45.0
           Text:                                M: c8b8 c6f3 e7e8 a6c5 e7d7  ...
           Normalized: M: c8b8 c6f3 e7e8 a6c5 e7d7 E: -4.22 -4.11 -4.21 -3.99 -4.07...

üéØ Prompt 2: P: 2N5/2k5/3p1p2/3P1BbP/4Pp2/1n6/4K3/8 w - - 9 49...
    Gen 1: Train_LP= -616.0, Ref_LP=  -80.5
           Text:                                             M: c8a7 e2d3 e2f...
           Normalized: M: c8a7 e2d3 e2f3 e2f2 c8e7 E: 0.36 0.45 0.44 0.44 0.45 B: c...
    Gen 2: Train_LP= -536.0, Ref_LP=  -14.9
           Text:                                             M: c8e7 e2d3 e2f...
           Normalized: M: c8e7 e2d3 e2f1 e2f2 e2f3 E: 0.0 0.0 0.0 0.0 0.0 B: e2f2...
    Gen 3: Train_LP= -394.0, Ref_LP=  -79.0
           Text:                                             M: e2f2 e2d3 e2f...
           Normalized: M: e2f2 e2d3 e2f3 c8e7 c8a7 E: 0.11 0.16 0.13 0.12 0.13 B: e...
    Gen 4: Train_LP= -688.0, Ref_LP=  -69.5
           Text:                                             M: c8a7 e2f2 c8e...
           Normalized: M: c8a7 e2f2 c8e7 e2f3 e2f1 E: 0.41 0.48 0.48 0.5 0.42 B: e2...

üéØ Prompt 3: A: 8/2k2b2/R1p4r/3p1p2/P7/2P2B1P/5PPK/4R3 w - - 1 31+a6a5+f7...
    Gen 1: Train_LP= -272.0, Ref_LP=   -0.0
           Text: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
           Normalized: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
    Gen 2: Train_LP= -260.0, Ref_LP=   -0.0
           Text: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
           Normalized: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
    Gen 3: Train_LP= -276.0, Ref_LP=   -0.0
           Text: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
           Normalized: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
    Gen 4: Train_LP= -402.0, Ref_LP=   -0.0
           Text: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
           Normalized: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...

üéØ Prompt 4: P: rnbqkbnr/pp1ppppp/2p5/8/4P3/2N5/PPPP1PPP/R1BQKBNR b KQkq ...
    Gen 1: Train_LP= -776.0, Ref_LP=  -36.0
           Text:                             M: e7e6 e7e5 d7d5 d7d6 c6c5     ...
           Normalized: M: e7e6 e7e5 d7d5 d7d6 c6c5 E: -0.72 -0.51 -0.45 -0.76 -0.85...
    Gen 2: Train_LP= -322.0, Ref_LP=  -24.4
           Text:                             M: d7d5 g7g6 d7d6 e7e5 e7e6     ...
           Normalized: M: d7d5 g7g6 d7d6 e7e5 e7e6 E: -0.59 -0.72 -0.68 -0.71 -0.7 ...
    Gen 3: Train_LP= -548.0, Ref_LP=  -24.1
           Text:                             M: d7d6 e7e5 d7d5 e7e6 g7g6     ...
           Normalized: M: d7d6 e7e5 d7d5 e7e6 g7g6 E: -0.59 -0.61 -0.57 -0.85 -0.8 ...
    Gen 4: Train_LP= -620.0, Ref_LP=  -37.0
           Text:                             M: e7e5 d7d5 g7g6 d7d6 a7a6     ...
           Normalized: M: e7e5 d7d5 g7g6 d7d6 a7a6 E: -0.46 -0.37 -0.56 -0.71 -0.65...

==================== PHASE 4: REWARD CALCULATION (step 9) ====================

üéØ Scoring Prompt 1 completions:
  Gen 1: Reward= 0.156 | M: a6c5 c8b8 h8e8 c6d7 e7d7 E: -3.32 -3....
  Gen 2: Reward= 0.649 | M: h8h3 a6c5 c6d7 b6a5 d4d3 E: -3.48 -3....
  Gen 3: Reward= 0.319 | M: h8h6 a6c5 e7h4 c6f3 c8b8 E: -4.07 -3....
  Gen 4: Reward= 0.431 | M: c8b8 c6f3 e7e8 a6c5 e7d7 E: -4.22 -4....
  üìä Average reward: 0.388

üéØ Scoring Prompt 2 completions:
  Gen 1: Reward= 0.549 | M: c8a7 e2d3 e2f3 e2f2 c8e7 E: 0.36 0.45...
  Gen 2: Reward= 0.305 | M: c8e7 e2d3 e2f1 e2f2 e2f3 E: 0.0 0.0 0...
  Gen 3: Reward= 0.099 | M: e2f2 e2d3 e2f3 c8e7 c8a7 E: 0.11 0.16...
  Gen 4: Reward= 0.039 | M: c8a7 e2f2 c8e7 e2f3 e2f1 E: 0.41 0.48...
  üìä Average reward: 0.248

üéØ Scoring Prompt 3 completions:
  Gen 1: Reward= 0.630 | 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b...
  Gen 2: Reward= 0.630 | 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b...
  Gen 3: Reward= 0.630 | 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b...
  Gen 4: Reward= 0.630 | 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b...
  üìä Average reward: 0.630

üéØ Scoring Prompt 4 completions:
  Gen 1: Reward= 0.613 | M: e7e6 e7e5 d7d5 d7d6 c6c5 E: -0.72 -0....
  Gen 2: Reward= 0.756 | M: d7d5 g7g6 d7d6 e7e5 e7e6 E: -0.59 -0....
  Gen 3: Reward= 0.367 | M: d7d6 e7e5 d7d5 e7e6 g7g6 E: -0.59 -0....
  Gen 4: Reward= 0.504 | M: e7e5 d7d5 g7g6 d7d6 a7a6 E: -0.46 -0....
  üìä Average reward: 0.560

==================== PHASE 5: TRL EXACT ADVANTAGE CALCULATION (step 9) ====================
üî¢ TRL Advantage Calculation (exact formula):
  Total rewards shape: torch.Size([16])
  Rewards: [0.1558     0.64857143 0.31897143 0.43057143 0.54857143 0.305
 0.09857143 0.03857143 0.63       0.63       0.63       0.63
 0.61257143 0.75577143 0.36737143 0.50377143]

==================== PHASE 6-8: CORRECTED GRPO LOSS CALCULATION (step 9) ====================
üîÑ Computing GRPO loss with proper token-level KL...

üéØ Prompt 1 - Corrected GRPO Loss:
  Gen 1: A=-1.125, logp/len=-6.719, kl=-4.750
         PG=-7.562, KL_penalty=-0.475, total=-8.062
  Gen 2: A=+1.257, logp/len=-7.406, kl=-5.469
         PG=9.312, KL_penalty=-0.547, total=8.750
  Gen 3: A=-0.336, logp/len=-6.281, kl=-4.500
         PG=-2.109, KL_penalty=-0.449, total=-2.562
  Gen 4: A=+0.203, logp/len=-7.531, kl=-5.625
         PG=1.531, KL_penalty=-0.562, total=0.969
  üìä Prompt averages: PG=0.293, KL=-0.508

üéØ Prompt 2 - Corrected GRPO Loss:
  Gen 1: A=+1.304, logp/len=-6.719, kl=-4.875
         PG=8.750, KL_penalty=-0.488, total=8.250
  Gen 2: A=+0.248, logp/len=-6.312, kl=-4.562
         PG=1.570, KL_penalty=-0.457, total=1.109
  Gen 3: A=-0.646, logp/len=-7.688, kl=-5.938
         PG=-4.969, KL_penalty=-0.594, total=-5.562
  Gen 4: A=-0.906, logp/len=-6.219, kl=-4.094
         PG=-5.625, KL_penalty=-0.410, total=-6.031
  üìä Prompt averages: PG=-0.070, KL=-0.488

üéØ Prompt 3 - Corrected GRPO Loss:
  Gen 1: A=+0.000, logp/len=-6.406, kl=-6.062
         PG=0.000, KL_penalty=-0.605, total=-0.605
  Gen 2: A=+0.000, logp/len=-5.438, kl=-5.062
         PG=0.000, KL_penalty=-0.508, total=-0.508
  Gen 3: A=+0.000, logp/len=-6.094, kl=-5.688
         PG=0.000, KL_penalty=-0.570, total=-0.570
  Gen 4: A=+0.000, logp/len=-5.438, kl=-5.094
         PG=0.000, KL_penalty=-0.508, total=-0.508
  üìä Prompt averages: PG=0.000, KL=-0.547

üéØ Prompt 4 - Corrected GRPO Loss:
  Gen 1: A=+0.320, logp/len=-8.188, kl=-6.406
         PG=2.625, KL_penalty=-0.641, total=1.984
  Gen 2: A=+1.190, logp/len=-7.469, kl=-5.719
         PG=8.875, KL_penalty=-0.570, total=8.312
  Gen 3: A=-1.169, logp/len=-5.406, kl=-3.812
         PG=-6.312, KL_penalty=-0.381, total=-6.688
  Gen 4: A=-0.341, logp/len=-7.250, kl=-5.594
         PG=-2.469, KL_penalty=-0.559, total=-3.031
  üìä Prompt averages: PG=0.680, KL=-0.539

üî¢ CORRECTED Loss Breakdown:
  Policy Gradient Loss:    0.226
  KL Loss (with gradients):   -0.523
  Total Loss:          =  -0.297
  KL penalty ratio: 70.0%

üîÑ Performing corrected gradient update (step 9)...
  Total gradient norm: 25.73
  ‚úÖ Gradient update applied with proper KL regularization

==================== PHASE 9: POST-UPDATE PERFORMANCE (step 9) ====================
  Prompt 1: avg reward = 0.150
  Prompt 2: avg reward = 0.387
  Prompt 3: avg reward = 0.630
  Prompt 4: avg reward = 0.502
üìä POST-UPDATE (step 9) Performance:
  Average reward: 0.4174
  Positive ratio: 100.0%

==================== PHASE 10: ANALYSIS (step 9) ====================
üîç GRPO Step Impact (step 9):
  Post-update performance: 0.4174
  Step performance change: -0.0150

======================================================================
üß≠ SEQUENTIAL STEP 10/50
======================================================================
üßæ Overfit mode: reusing initial batch (no dataset slice advance)

ü§ñ Generating completions for GRPO training (step 10)...

üéØ Prompt 1: P: 2k4r/ppp1q1p1/nbb1N3/4PP2/3p2P1/PB5P/1PPK3R/3RQ3 b - - 3 ...
    Gen 1: Train_LP= -380.0, Ref_LP=  -56.8
           Text:                                M: b6a5 a6c5 c8b8 h8h3 e7h4  ...
           Normalized: M: b6a5 a6c5 c8b8 h8h3 e7h4 E: -3.72 -3.45 -3.75 -3.75 -3.66...
    Gen 2: Train_LP= -390.0, Ref_LP=  -57.8
           Text:                                M: a6c5 h8h4 c8b8 c6f3 h8h3  ...
           Normalized: M: a6c5 h8h4 c8b8 c6f3 h8h3 E: -3.34 -3.68 -3.59 -3.58 -3.54...
    Gen 3: Train_LP= -476.0, Ref_LP=  -36.2
           Text:                                M: e7d7 c6d7 c8b8 e7h4 d4d3  ...
           Normalized: M: e7d7 c6d7 c8b8 e7h4 d4d3 E: -4.16 -4.3 -4.29 -4.24 -3.9 B...
    Gen 4: Train_LP= -640.0, Ref_LP=  -33.8
           Text:                                M: c6d7 a6c5 d4d3 e7d7 e7e8  ...
           Normalized: M: c6d7 a6c5 d4d3 e7d7 e7e8 E: -3.89 -3.5 -3.87 -3.91 -3.9 B...

üéØ Prompt 2: P: 2N5/2k5/3p1p2/3P1BbP/4Pp2/1n6/4K3/8 w - - 9 49...
    Gen 1: Train_LP= -624.0, Ref_LP=  -66.5
           Text:                                             M: e2f2 e2f3 c8e...
           Normalized: M: e2f2 e2f3 c8e7 e2d3 c8a7 E: 0.02 0.02 0.0 0.01 0.01 B: e2...
    Gen 2: Train_LP= -592.0, Ref_LP=  -14.9
           Text:                                             M: e2d1 c8e7 e2f...
           Normalized: M: e2d1 c8e7 e2f3 e2f2 c8a7 E: 0.0 0.0 0.0 0.0 0.0 B: c8e7...
    Gen 3: Train_LP= -378.0, Ref_LP=  -44.0
           Text:                                             M: c8a7 e2f2 e2f...
           Normalized: M: c8a7 e2f2 e2f1 e2f3 c8e7 E: 0.07 0.09 0.0 0.0 0.0 B: e2f2...
    Gen 4: Train_LP= -724.0, Ref_LP=  -78.0
           Text:                                             M: e2f2 c8e7 e2f...
           Normalized: M: e2f2 c8e7 e2f3 e2f1 c8a7 E: 0.13 0.17 0.12 0.12 0.15 B: c...

üéØ Prompt 3: A: 8/2k2b2/R1p4r/3p1p2/P7/2P2B1P/5PPK/4R3 w - - 1 31+a6a5+f7...
    Gen 1: Train_LP= -270.0, Ref_LP=   -0.0
           Text: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
           Normalized: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
    Gen 2: Train_LP= -370.0, Ref_LP=   -0.0
           Text: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
           Normalized: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
    Gen 3: Train_LP= -250.0, Ref_LP=   -0.0
           Text: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
           Normalized: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
    Gen 4: Train_LP= -318.0, Ref_LP=   -0.0
           Text: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
           Normalized: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...

üéØ Prompt 4: P: rnbqkbnr/pp1ppppp/2p5/8/4P3/2N5/PPPP1PPP/R1BQKBNR b KQkq ...
    Gen 1: Train_LP= -688.0, Ref_LP=  -35.0
           Text:                             M: d7d5 e7e5 d7d6 e7e6 g7g6     ...
           Normalized: M: d7d5 e7e5 d7d6 e7e6 g7g6 E: -0.53 -0.46 -0.66 -0.68 -0.71...
    Gen 2: Train_LP= -612.0, Ref_LP=  -34.0
           Text:                             M: d7d6 d7d5 g7g6 e7e5 e7e6     ...
           Normalized: M: d7d6 d7d5 g7g6 e7e5 e7e6 E: -0.62 -0.49 -0.68 -0.51 -0.89...
    Gen 3: Train_LP= -652.0, Ref_LP=  -35.8
           Text:                             M: d7d6 e7e5 d7d5 g7g6 e7e6     ...
           Normalized: M: d7d6 e7e5 d7d5 g7g6 e7e6 E: -0.59 -0.49 -0.59 -0.65 -0.64...
    Gen 4: Train_LP= -412.0, Ref_LP=  -24.1
           Text:                             M: d7d6 e7e5 e7e6 d7d5 g7g6     ...
           Normalized: M: d7d6 e7e5 e7e6 d7d5 g7g6 E: -0.67 -0.71 -0.7 -0.38 -0.67 ...

==================== PHASE 4: REWARD CALCULATION (step 10) ====================

üéØ Scoring Prompt 1 completions:
  Gen 1: Reward= 0.379 | M: b6a5 a6c5 c8b8 h8h3 e7h4 E: -3.72 -3....
  Gen 2: Reward= 0.179 | M: a6c5 h8h4 c8b8 c6f3 h8h3 E: -3.34 -3....
  Gen 3: Reward= 0.202 | M: e7d7 c6d7 c8b8 e7h4 d4d3 E: -4.16 -4....
  Gen 4: Reward= 0.195 | M: c6d7 a6c5 d4d3 e7d7 e7e8 E: -3.89 -3....
  üìä Average reward: 0.238

üéØ Scoring Prompt 2 completions:
  Gen 1: Reward= 0.369 | M: e2f2 e2f3 c8e7 e2d3 c8a7 E: 0.02 0.02...
  Gen 2: Reward= 0.540 | M: e2d1 c8e7 e2f3 e2f2 c8a7 E: 0.0 0.0 0...
  Gen 3: Reward= 0.349 | M: c8a7 e2f2 e2f1 e2f3 c8e7 E: 0.07 0.09...
  Gen 4: Reward= 0.469 | M: e2f2 c8e7 e2f3 e2f1 c8a7 E: 0.13 0.17...
  üìä Average reward: 0.431

üéØ Scoring Prompt 3 completions:
  Gen 1: Reward= 0.630 | 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b...
  Gen 2: Reward= 0.630 | 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b...
  Gen 3: Reward= 0.630 | 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b...
  Gen 4: Reward= 0.630 | 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b...
  üìä Average reward: 0.630

üéØ Scoring Prompt 4 completions:
  Gen 1: Reward= 0.741 | M: d7d5 e7e5 d7d6 e7e6 g7g6 E: -0.53 -0....
  Gen 2: Reward= 0.363 | M: d7d6 d7d5 g7g6 e7e5 e7e6 E: -0.62 -0....
  Gen 3: Reward= 0.708 | M: d7d6 e7e5 d7d5 g7g6 e7e6 E: -0.59 -0....
  Gen 4: Reward= 0.358 | M: d7d6 e7e5 e7e6 d7d5 g7g6 E: -0.67 -0....
  üìä Average reward: 0.542

==================== PHASE 5: TRL EXACT ADVANTAGE CALCULATION (step 10) ====================
üî¢ TRL Advantage Calculation (exact formula):
  Total rewards shape: torch.Size([16])
  Rewards: [0.37857143 0.17857143 0.2018     0.195      0.36857143 0.54
 0.34857143 0.46857143 0.63       0.63       0.63       0.63
 0.74097143 0.36257143 0.70817143 0.35777143]

==================== PHASE 6-8: CORRECTED GRPO LOSS CALCULATION (step 10) ====================
üîÑ Computing GRPO loss with proper token-level KL...

üéØ Prompt 1 - Corrected GRPO Loss:
  Gen 1: A=+1.492, logp/len=-7.562, kl=-5.656
         PG=11.312, KL_penalty=-0.566, total=10.750
  Gen 2: A=-0.638, logp/len=-7.625, kl=-5.875
         PG=-4.875, KL_penalty=-0.586, total=-5.469
  Gen 3: A=-0.391, logp/len=-5.469, kl=-3.766
         PG=-2.141, KL_penalty=-0.377, total=-2.516
  Gen 4: A=-0.463, logp/len=-7.375, kl=-5.438
         PG=-3.422, KL_penalty=-0.543, total=-3.969
  üìä Prompt averages: PG=0.223, KL=-0.520

üéØ Prompt 2 - Corrected GRPO Loss:
  Gen 1: A=-0.703, logp/len=-7.594, kl=-5.812
         PG=-5.344, KL_penalty=-0.582, total=-5.938
  Gen 2: A=+1.214, logp/len=-6.219, kl=-4.688
         PG=7.562, KL_penalty=-0.469, total=7.094
  Gen 3: A=-0.927, logp/len=-6.750, kl=-5.031
         PG=-6.250, KL_penalty=-0.504, total=-6.750
  Gen 4: A=+0.415, logp/len=-5.594, kl=-3.781
         PG=2.328, KL_penalty=-0.379, total=1.953
  üìä Prompt averages: PG=-0.426, KL=-0.480

üéØ Prompt 3 - Corrected GRPO Loss:
  Gen 1: A=+0.000, logp/len=-5.812, kl=-5.438
         PG=0.000, KL_penalty=-0.543, total=-0.543
  Gen 2: A=+0.000, logp/len=-7.406, kl=-7.031
         PG=0.000, KL_penalty=-0.703, total=-0.703
  Gen 3: A=+0.000, logp/len=-5.250, kl=-4.875
         PG=0.000, KL_penalty=-0.488, total=-0.488
  Gen 4: A=+0.000, logp/len=-5.844, kl=-5.500
         PG=0.000, KL_penalty=-0.551, total=-0.551
  üìä Prompt averages: PG=0.000, KL=-0.570

üéØ Prompt 4 - Corrected GRPO Loss:
  Gen 1: A=+0.942, logp/len=-7.625, kl=-6.062
         PG=7.188, KL_penalty=-0.605, total=6.594
  Gen 2: A=-0.853, logp/len=-6.375, kl=-4.625
         PG=-5.438, KL_penalty=-0.463, total=-5.906
  Gen 3: A=+0.786, logp/len=-7.000, kl=-5.469
         PG=5.500, KL_penalty=-0.547, total=4.938
  Gen 4: A=-0.876, logp/len=-6.906, kl=-5.250
         PG=-6.062, KL_penalty=-0.523, total=-6.594
  üìä Prompt averages: PG=0.297, KL=-0.535

üî¢ CORRECTED Loss Breakdown:
  Policy Gradient Loss:    0.023
  KL Loss (with gradients):   -0.527
  Total Loss:          =  -0.504
  KL penalty ratio: 95.5%

üîÑ Performing corrected gradient update (step 10)...
  Total gradient norm: 18.29
  ‚úÖ Gradient update applied with proper KL regularization

==================== PHASE 9: POST-UPDATE PERFORMANCE (step 10) ====================
  Prompt 1: avg reward = 0.150
  Prompt 2: avg reward = 0.387
  Prompt 3: avg reward = 0.630
  Prompt 4: avg reward = 0.503
üìä POST-UPDATE (step 10) Performance:
  Average reward: 0.4175
  Positive ratio: 100.0%

==================== PHASE 10: ANALYSIS (step 10) ====================
üîç GRPO Step Impact (step 10):
  Post-update performance: 0.4175
  Step performance change: +0.0002

======================================================================
üß≠ SEQUENTIAL STEP 11/50
======================================================================
üßæ Overfit mode: reusing initial batch (no dataset slice advance)

ü§ñ Generating completions for GRPO training (step 11)...

üéØ Prompt 1: P: 2k4r/ppp1q1p1/nbb1N3/4PP2/3p2P1/PB5P/1PPK3R/3RQ3 b - - 3 ...
    Gen 1: Train_LP= -380.0, Ref_LP=  -34.8
           Text:                                M: e7d7 a6c5 d4d3 b6a5 h8h3  ...
           Normalized: M: e7d7 a6c5 d4d3 b6a5 h8h3 E: -4.24 -3.6 -3.68 -3.82 -4.01 ...
    Gen 2: Train_LP= -660.0, Ref_LP=  -44.8
           Text:                                M: a6c5 d4d3 e7d7 e7h4 e7e8  ...
           Normalized: M: a6c5 d4d3 e7d7 e7h4 e7e8 E: -4.21 -4.21 -4.25 -4.26 -4.26...
    Gen 3: Train_LP= -480.0, Ref_LP=  -34.5
           Text:                                M: d4d3 a6c5 c6d7 c8b8 h8h6  ...
           Normalized: M: d4d3 a6c5 c6d7 c8b8 h8h6 E: -4.18 -3.7 -4.25 -4.03 -4.26 ...
    Gen 4: Train_LP= -452.0, Ref_LP=  -35.2
           Text:                                M: c8b8 a6c5 e7h4 h8h4 b6a5  ...
           Normalized: M: c8b8 a6c5 e7h4 h8h4 b6a5 E: -4.17 -3.78 -4.17 -4.06 -4.1 ...

üéØ Prompt 2: P: 2N5/2k5/3p1p2/3P1BbP/4Pp2/1n6/4K3/8 w - - 9 49...
    Gen 1: Train_LP= -728.0, Ref_LP=  -78.0
           Text:                                             M: e2d3 e2f2 e2f...
           Normalized: M: e2d3 e2f2 e2f3 c8e7 c8a7 E: 0.24 0.27 0.27 0.27 0.28 B: c...
    Gen 2: Train_LP= -836.0, Ref_LP=  -79.5
           Text:                                             M: e2f2 c8e7 e2f...
           Normalized: M: e2f2 c8e7 e2f3 c8a7 e2f1 E: 0.39 0.51 0.39 0.37 0.39 B: c...
    Gen 3: Train_LP= -420.0, Ref_LP=  -14.8
           Text:                                             M: e2f1 e2f2 c8e...
           Normalized: M: e2f1 e2f2 c8e7 e2f3 e2d3 E: 0.0 0.0 0.0 0.0 0.0 B: e2f3...
    Gen 4: Train_LP= -568.0, Ref_LP=  -70.0
           Text:                                             M: f5e6 e2f3 f5g...
           Normalized: M: f5e6 e2f3 f5g4 c8e7 e2f2 E: 0.26 0.29 0.3 0.29 0.26 B: f5...

üéØ Prompt 3: A: 8/2k2b2/R1p4r/3p1p2/P7/2P2B1P/5PPK/4R3 w - - 1 31+a6a5+f7...
    Gen 1: Train_LP= -312.0, Ref_LP=   -0.0
           Text: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
           Normalized: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
    Gen 2: Train_LP= -300.0, Ref_LP=   -0.0
           Text: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
           Normalized: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
    Gen 3: Train_LP= -376.0, Ref_LP=   -0.0
           Text: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
           Normalized: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
    Gen 4: Train_LP= -276.0, Ref_LP=   -0.0
           Text: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
           Normalized: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...

üéØ Prompt 4: P: rnbqkbnr/pp1ppppp/2p5/8/4P3/2N5/PPPP1PPP/R1BQKBNR b KQkq ...
    Gen 1: Train_LP= -512.0, Ref_LP=  -35.0
           Text:                             M: d7d6 e7e5 d7d5 g7g6 c6c5     ...
           Normalized: M: d7d6 e7e5 d7d5 g7g6 c6c5 E: -0.65 -0.63 -0.34 -0.68 -0.69...
    Gen 2: Train_LP= -780.0, Ref_LP=  -23.2
           Text:                             M: d7d5 d7d6 e7e5 e7e6 g7g6     ...
           Normalized: M: d7d5 d7d6 e7e5 e7e6 g7g6 E: -0.46 -0.74 -0.57 -0.79 -0.7 ...
    Gen 3: Train_LP= -548.0, Ref_LP=  -35.2
           Text:                             M: d7d5 d7d6 g7g6 e7e5 c6c5     ...
           Normalized: M: d7d5 d7d6 g7g6 e7e5 c6c5 E: -0.37 -0.57 -0.76 -0.52 -0.82...
    Gen 4: Train_LP= -652.0, Ref_LP=  -36.8
           Text:                             M: d7d6 e7e5 d7d5 d8c7 e7e6     ...
           Normalized: M: d7d6 e7e5 d7d5 d8c7 e7e6 E: -0.75 -0.73 -0.45 -0.93 -0.82...

==================== PHASE 4: REWARD CALCULATION (step 11) ====================

üéØ Scoring Prompt 1 completions:
  Gen 1: Reward= 0.199 | M: e7d7 a6c5 d4d3 b6a5 h8h3 E: -4.24 -3....
  Gen 2: Reward= 0.199 | M: a6c5 d4d3 e7d7 e7h4 e7e8 E: -4.21 -4....
  Gen 3: Reward= 0.329 | M: d4d3 a6c5 c6d7 c8b8 h8h6 E: -4.18 -3....
  Gen 4: Reward= 0.349 | M: c8b8 a6c5 e7h4 h8h4 b6a5 E: -4.17 -3....
  üìä Average reward: 0.269

üéØ Scoring Prompt 2 completions:
  Gen 1: Reward= 0.369 | M: e2d3 e2f2 e2f3 c8e7 c8a7 E: 0.24 0.27...
  Gen 2: Reward= 0.469 | M: e2f2 c8e7 e2f3 c8a7 e2f1 E: 0.39 0.51...
  Gen 3: Reward=-0.055 | M: e2f1 e2f2 c8e7 e2f3 e2d3 E: 0.0 0.0 0...
  Gen 4: Reward=-0.075 | M: f5e6 e2f3 f5g4 c8e7 e2f2 E: 0.26 0.29...
  üìä Average reward: 0.177

üéØ Scoring Prompt 3 completions:
  Gen 1: Reward= 0.630 | 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b...
  Gen 2: Reward= 0.630 | 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b...
  Gen 3: Reward= 0.630 | 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b...
  Gen 4: Reward= 0.630 | 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b...
  üìä Average reward: 0.630

üéØ Scoring Prompt 4 completions:
  Gen 1: Reward= 0.681 | M: d7d6 e7e5 d7d5 g7g6 c6c5 E: -0.65 -0....
  Gen 2: Reward= 0.395 | M: d7d5 d7d6 e7e5 e7e6 g7g6 E: -0.46 -0....
  Gen 3: Reward= 0.724 | M: d7d5 d7d6 g7g6 e7e5 c6c5 E: -0.37 -0....
  Gen 4: Reward= 0.304 | M: d7d6 e7e5 d7d5 d8c7 e7e6 E: -0.75 -0....
  üìä Average reward: 0.526

==================== PHASE 5: TRL EXACT ADVANTAGE CALCULATION (step 11) ====================
üî¢ TRL Advantage Calculation (exact formula):
  Total rewards shape: torch.Size([16])
  Rewards: [ 0.19857143  0.1986      0.32897143  0.34857143  0.36857143  0.46857143
 -0.055      -0.075       0.63        0.63        0.63        0.63
  0.6812      0.39537143  0.7236      0.3038    ]

==================== PHASE 6-8: CORRECTED GRPO LOSS CALCULATION (step 11) ====================
üîÑ Computing GRPO loss with proper token-level KL...

üéØ Prompt 1 - Corrected GRPO Loss:
  Gen 1: A=-0.862, logp/len=-6.406, kl=-4.625
         PG=-5.531, KL_penalty=-0.463, total=-6.000
  Gen 2: A=-0.862, logp/len=-8.312, kl=-6.438
         PG=-7.156, KL_penalty=-0.645, total=-7.812
  Gen 3: A=+0.741, logp/len=-7.375, kl=-5.594
         PG=5.469, KL_penalty=-0.559, total=4.906
  Gen 4: A=+0.982, logp/len=-5.969, kl=-4.281
         PG=5.875, KL_penalty=-0.428, total=5.438
  üìä Prompt averages: PG=-0.336, KL=-0.523

üéØ Prompt 2 - Corrected GRPO Loss:
  Gen 1: A=+0.679, logp/len=-6.531, kl=-4.781
         PG=4.438, KL_penalty=-0.479, total=3.953
  Gen 2: A=+1.034, logp/len=-7.031, kl=-5.094
         PG=7.281, KL_penalty=-0.508, total=6.781
  Gen 3: A=-0.821, logp/len=-7.719, kl=-5.906
         PG=-6.344, KL_penalty=-0.590, total=-6.938
  Gen 4: A=-0.892, logp/len=-6.625, kl=-4.594
         PG=-5.906, KL_penalty=-0.459, total=-6.375
  üìä Prompt averages: PG=-0.125, KL=-0.508

üéØ Prompt 3 - Corrected GRPO Loss:
  Gen 1: A=+0.000, logp/len=-5.438, kl=-5.062
         PG=0.000, KL_penalty=-0.508, total=-0.508
  Gen 2: A=+0.000, logp/len=-5.250, kl=-4.875
         PG=0.000, KL_penalty=-0.488, total=-0.488
  Gen 3: A=+0.000, logp/len=-5.750, kl=-5.406
         PG=0.000, KL_penalty=-0.539, total=-0.539
  Gen 4: A=+0.000, logp/len=-6.438, kl=-6.062
         PG=0.000, KL_penalty=-0.605, total=-0.605
  üìä Prompt averages: PG=0.000, KL=-0.535

üéØ Prompt 4 - Corrected GRPO Loss:
  Gen 1: A=+0.747, logp/len=-6.625, kl=-5.000
         PG=4.938, KL_penalty=-0.500, total=4.438
  Gen 2: A=-0.629, logp/len=-6.188, kl=-4.594
         PG=-3.891, KL_penalty=-0.459, total=-4.344
  Gen 3: A=+0.951, logp/len=-7.125, kl=-5.312
         PG=6.781, KL_penalty=-0.531, total=6.250
  Gen 4: A=-1.069, logp/len=-7.531, kl=-5.938
         PG=-8.062, KL_penalty=-0.594, total=-8.625
  üìä Prompt averages: PG=-0.062, KL=-0.523

üî¢ CORRECTED Loss Breakdown:
  Policy Gradient Loss:   -0.131
  KL Loss (with gradients):   -0.523
  Total Loss:          =  -0.656
  KL penalty ratio: 79.5%

üîÑ Performing corrected gradient update (step 11)...
  Total gradient norm: 18.97
  ‚úÖ Gradient update applied with proper KL regularization

==================== PHASE 9: POST-UPDATE PERFORMANCE (step 11) ====================
  Prompt 1: avg reward = 0.150
  Prompt 2: avg reward = 0.387
  Prompt 3: avg reward = 0.630
  Prompt 4: avg reward = 0.503
üìä POST-UPDATE (step 11) Performance:
  Average reward: 0.4174
  Positive ratio: 100.0%

==================== PHASE 10: ANALYSIS (step 11) ====================
üîç GRPO Step Impact (step 11):
  Post-update performance: 0.4174
  Step performance change: -0.0001

======================================================================
üß≠ SEQUENTIAL STEP 12/50
======================================================================
üßæ Overfit mode: reusing initial batch (no dataset slice advance)

ü§ñ Generating completions for GRPO training (step 12)...

üéØ Prompt 1: P: 2k4r/ppp1q1p1/nbb1N3/4PP2/3p2P1/PB5P/1PPK3R/3RQ3 b - - 3 ...
    Gen 1: Train_LP= -772.0, Ref_LP=  -34.0
           Text:                                M: c8b8 a6c5 e7h4 c6f3 d4d3  ...
           Normalized: M: c8b8 a6c5 e7h4 c6f3 d4d3 E: -4.0 -3.65 -4.1 -4.0 -3.81 B:...
    Gen 2: Train_LP= -302.0, Ref_LP=  -65.5
           Text:                                M: d4d3 h8e8 a6c5 g7g6 e7d7  ...
           Normalized: M: d4d3 h8e8 a6c5 g7g6 e7d7 E: -3.65 -3.77 -3.43 -3.65 -3.75...
    Gen 3: Train_LP= -532.0, Ref_LP=  -43.8
           Text:                                M: b6a5 c8b8 e7d7 a6c5 d4d3  ...
           Normalized: M: b6a5 c8b8 e7d7 a6c5 d4d3 E: -3.39 -3.36 -3.3 -3.25 -3.4 B...
    Gen 4: Train_LP= -428.0, Ref_LP=  -67.0
           Text:                                M: c8b8 h8e8 a6c5 c6d7 e7d7  ...
           Normalized: M: c8b8 h8e8 a6c5 c6d7 e7d7 E: -4.01 -3.77 -3.49 -3.88 -3.89...

üéØ Prompt 2: P: 2N5/2k5/3p1p2/3P1BbP/4Pp2/1n6/4K3/8 w - - 9 49...
    Gen 1: Train_LP= -350.0, Ref_LP=  -79.0
           Text:                                             M: e2d3 c8e7 e2f...
           Normalized: M: e2d3 c8e7 e2f2 e2f3 f5g6 E: 0.25 0.25 0.24 0.25 0.24 B: e...
    Gen 2: Train_LP= -560.0, Ref_LP=  -16.6
           Text:                                             M: e2f2 e2f3 f5g...
           Normalized: M: e2f2 e2f3 f5g6 c8e7 e2d3 E: 0.0 0.0 0.0 0.0 0.0 B: c8e7...
    Gen 3: Train_LP= -736.0, Ref_LP=  -14.8
           Text:                                             M: e2f3 e2f2 c8e...
           Normalized: M: e2f3 e2f2 c8e7 e2e1 e2f1 E: 0.0 0.0 0.0 0.0 0.0 B: e2f2...
    Gen 4: Train_LP= -672.0, Ref_LP=  -78.5
           Text:                                             M: e2f1 e2f2 e2f...
           Normalized: M: e2f1 e2f2 e2f3 c8e7 f5g6 E: 0.06 0.06 0.05 0.06 0.06 B: e...

üéØ Prompt 3: A: 8/2k2b2/R1p4r/3p1p2/P7/2P2B1P/5PPK/4R3 w - - 1 31+a6a5+f7...
    Gen 1: Train_LP= -356.0, Ref_LP=   -0.0
           Text: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
           Normalized: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
    Gen 2: Train_LP= -334.0, Ref_LP=   -0.0
           Text: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
           Normalized: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
    Gen 3: Train_LP= -290.0, Ref_LP=   -0.0
           Text: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
           Normalized: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
    Gen 4: Train_LP= -272.0, Ref_LP=   -0.0
           Text: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
           Normalized: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...

üéØ Prompt 4: P: rnbqkbnr/pp1ppppp/2p5/8/4P3/2N5/PPPP1PPP/R1BQKBNR b KQkq ...
    Gen 1: Train_LP= -700.0, Ref_LP=  -24.5
           Text:                             M: d7d6 g7g6 d7d5 e7e6 e7e5     ...
           Normalized: M: d7d6 g7g6 d7d5 e7e6 e7e5 E: -0.68 -0.67 -0.33 -0.72 -0.48...
    Gen 2: Train_LP= -434.0, Ref_LP=  -23.9
           Text:                             M: d7d5 d7d6 e7e5 g7g6 c6c5     ...
           Normalized: M: d7d5 d7d6 e7e5 g7g6 c6c5 E: -0.43 -0.74 -0.54 -0.77 -0.73...
    Gen 3: Train_LP= -548.0, Ref_LP=  -24.2
           Text:                             M: d7d6 e7e5 g7g6 d7d5 e7e6     ...
           Normalized: M: d7d6 e7e5 g7g6 d7d5 e7e6 E: -0.59 -0.49 -0.63 -0.32 -0.82...
    Gen 4: Train_LP= -450.0, Ref_LP=  -23.5
           Text:                             M: d7d6 d7d5 e7e5 g7g6 c6c5     ...
           Normalized: M: d7d6 d7d5 e7e5 g7g6 c6c5 E: -0.56 -0.38 -0.59 -0.68 -0.73...

==================== PHASE 4: REWARD CALCULATION (step 12) ====================

üéØ Scoring Prompt 1 completions:
  Gen 1: Reward= 0.365 | M: c8b8 a6c5 e7h4 c6f3 d4d3 E: -4.0 -3.6...
  Gen 2: Reward= 0.152 | M: d4d3 h8e8 a6c5 g7g6 e7d7 E: -3.65 -3....
  Gen 3: Reward= 0.340 | M: b6a5 c8b8 e7d7 a6c5 d4d3 E: -3.39 -3....
  Gen 4: Reward= 0.327 | M: c8b8 h8e8 a6c5 c6d7 e7d7 E: -4.01 -3....
  üìä Average reward: 0.296

üéØ Scoring Prompt 2 completions:
  Gen 1: Reward=-0.035 | M: e2d3 c8e7 e2f2 e2f3 f5g6 E: 0.25 0.25...
  Gen 2: Reward= 0.375 | M: e2f2 e2f3 f5g6 c8e7 e2d3 E: 0.0 0.0 0...
  Gen 3: Reward= 0.295 | M: e2f3 e2f2 c8e7 e2e1 e2f1 E: 0.0 0.0 0...
  Gen 4: Reward=-0.055 | M: e2f1 e2f2 e2f3 c8e7 f5g6 E: 0.06 0.06...
  üìä Average reward: 0.145

üéØ Scoring Prompt 3 completions:
  Gen 1: Reward= 0.630 | 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b...
  Gen 2: Reward= 0.630 | 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b...
  Gen 3: Reward= 0.630 | 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b...
  Gen 4: Reward= 0.630 | 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b...
  üìä Average reward: 0.630

üéØ Scoring Prompt 4 completions:
  Gen 1: Reward= 0.379 | M: d7d6 g7g6 d7d5 e7e6 e7e5 E: -0.68 -0....
  Gen 2: Reward= 0.819 | M: d7d5 d7d6 e7e5 g7g6 c6c5 E: -0.43 -0....
  Gen 3: Reward= 0.363 | M: d7d6 e7e5 g7g6 d7d5 e7e6 E: -0.59 -0....
  Gen 4: Reward= 0.689 | M: d7d6 d7d5 e7e5 g7g6 c6c5 E: -0.56 -0....
  üìä Average reward: 0.562

==================== PHASE 5: TRL EXACT ADVANTAGE CALCULATION (step 12) ====================
üî¢ TRL Advantage Calculation (exact formula):
  Total rewards shape: torch.Size([16])
  Rewards: [ 0.3652      0.1518      0.34        0.3266     -0.035       0.375
  0.295      -0.055       0.63        0.63        0.63        0.63
  0.37897143  0.8188      0.36257143  0.6892    ]

==================== PHASE 6-8: CORRECTED GRPO LOSS CALCULATION (step 12) ====================
üîÑ Computing GRPO loss with proper token-level KL...

üéØ Prompt 1 - Corrected GRPO Loss:
  Gen 1: A=+0.712, logp/len=-8.125, kl=-6.438
         PG=5.781, KL_penalty=-0.645, total=5.125
  Gen 2: A=-1.480, logp/len=-6.531, kl=-4.781
         PG=-9.688, KL_penalty=-0.479, total=-10.188
  Gen 3: A=+0.453, logp/len=-6.625, kl=-4.938
         PG=3.000, KL_penalty=-0.494, total=2.500
  Gen 4: A=+0.315, logp/len=-6.250, kl=-4.250
         PG=1.969, KL_penalty=-0.426, total=1.547
  üìä Prompt averages: PG=0.266, KL=-0.512

üéØ Prompt 2 - Corrected GRPO Loss:
  Gen 1: A=-0.811, logp/len=-6.844, kl=-4.938
         PG=-5.562, KL_penalty=-0.494, total=-6.062
  Gen 2: A=+1.036, logp/len=-5.500, kl=-3.734
         PG=5.688, KL_penalty=-0.373, total=5.312
  Gen 3: A=+0.676, logp/len=-7.688, kl=-5.969
         PG=5.188, KL_penalty=-0.598, total=4.594
  Gen 4: A=-0.901, logp/len=-6.562, kl=-4.781
         PG=-5.906, KL_penalty=-0.479, total=-6.375
  üìä Prompt averages: PG=-0.148, KL=-0.486

üéØ Prompt 3 - Corrected GRPO Loss:
  Gen 1: A=+0.000, logp/len=-5.750, kl=-5.406
         PG=0.000, KL_penalty=-0.539, total=-0.539
  Gen 2: A=+0.000, logp/len=-5.906, kl=-5.562
         PG=0.000, KL_penalty=-0.555, total=-0.555
  Gen 3: A=+0.000, logp/len=-6.875, kl=-6.500
         PG=0.000, KL_penalty=-0.648, total=-0.648
  Gen 4: A=+0.000, logp/len=-5.844, kl=-5.469
         PG=0.000, KL_penalty=-0.547, total=-0.547
  üìä Prompt averages: PG=0.000, KL=-0.570

üéØ Prompt 4 - Corrected GRPO Loss:
  Gen 1: A=-0.806, logp/len=-7.281, kl=-5.594
         PG=-5.875, KL_penalty=-0.559, total=-6.438
  Gen 2: A=+1.127, logp/len=-5.750, kl=-4.094
         PG=6.469, KL_penalty=-0.410, total=6.062
  Gen 3: A=-0.878, logp/len=-7.531, kl=-5.844
         PG=-6.625, KL_penalty=-0.586, total=-7.219
  Gen 4: A=+0.557, logp/len=-8.062, kl=-6.500
         PG=4.500, KL_penalty=-0.648, total=3.844
  üìä Prompt averages: PG=-0.383, KL=-0.551

üî¢ CORRECTED Loss Breakdown:
  Policy Gradient Loss:   -0.066
  KL Loss (with gradients):   -0.531
  Total Loss:          =  -0.598
  KL penalty ratio: 89.0%

üîÑ Performing corrected gradient update (step 12)...
  Total gradient norm: 18.16
  ‚úÖ Gradient update applied with proper KL regularization

==================== PHASE 9: POST-UPDATE PERFORMANCE (step 12) ====================
  Prompt 1: avg reward = 0.150
  Prompt 2: avg reward = 0.387
  Prompt 3: avg reward = 0.630
  Prompt 4: avg reward = 0.555
üìä POST-UPDATE (step 12) Performance:
  Average reward: 0.4306
  Positive ratio: 100.0%

==================== PHASE 10: ANALYSIS (step 12) ====================
üîç GRPO Step Impact (step 12):
  Post-update performance: 0.4306
  Step performance change: +0.0132

======================================================================
üß≠ SEQUENTIAL STEP 13/50
======================================================================
üßæ Overfit mode: reusing initial batch (no dataset slice advance)

ü§ñ Generating completions for GRPO training (step 13)...

üéØ Prompt 1: P: 2k4r/ppp1q1p1/nbb1N3/4PP2/3p2P1/PB5P/1PPK3R/3RQ3 b - - 3 ...
    Gen 1: Train_LP= -512.0, Ref_LP=  -32.8
           Text:                                M: d4d3 a6c5 c8b8 b6a5 h8e8  ...
           Normalized: M: d4d3 a6c5 c8b8 b6a5 h8e8 E: -3.68 -3.34 -3.68 -3.42 -3.54...
    Gen 2: Train_LP= -498.0, Ref_LP=  -33.8
           Text:                                M: d4d3 a6c5 c8b8 h8h6 e7d7  ...
           Normalized: M: d4d3 a6c5 c8b8 h8h6 e7d7 E: -3.53 -3.46 -3.77 -3.78 -3.68...
    Gen 3: Train_LP= -454.0, Ref_LP=  -33.8
           Text:                                M: d4d3 a6c5 c6f3 c8b8 h8h6  ...
           Normalized: M: d4d3 a6c5 c6f3 c8b8 h8h6 E: -3.61 -3.61 -3.91 -3.92 -3.96...
    Gen 4: Train_LP= -528.0, Ref_LP=  -35.5
           Text:                                M: b6a5 c6d7 e7h4 a6c5 d4d3  ...
           Normalized: M: b6a5 c6d7 e7h4 a6c5 d4d3 E: -3.97 -3.68 -4.13 -3.66 -3.64...

üéØ Prompt 2: P: 2N5/2k5/3p1p2/3P1BbP/4Pp2/1n6/4K3/8 w - - 9 49...
    Gen 1: Train_LP= -516.0, Ref_LP=  -23.4
           Text:                                             M: e2f3 e2f2 e2f...
           Normalized: M: e2f3 e2f2 e2f1 c8e7 e2d3 E: 0.3 0.3 0.3 0.3 0.3 B: e2f2...
    Gen 2: Train_LP= -688.0, Ref_LP=  -79.0
           Text:                                             M: e2f2 e2f1 e2e...
           Normalized: M: e2f2 e2f1 e2e1 c8e7 e2f3 E: 0.09 0.08 0.11 0.15 0.08 B: c...
    Gen 3: Train_LP= -660.0, Ref_LP=  -78.5
           Text:                                             M: c8e7 e2f2 e2f...
           Normalized: M: c8e7 e2f2 e2f3 f5g4 c8a7 E: 0.17 0.13 0.13 0.12 0.16 B: c...
    Gen 4: Train_LP= -420.0, Ref_LP=  -14.8
           Text:                                             M: e2d3 e2f1 e2f...
           Normalized: M: e2d3 e2f1 e2f2 c8e7 c8a7 E: 0.0 0.0 0.0 0.0 0.0 B: c8a7...

üéØ Prompt 3: A: 8/2k2b2/R1p4r/3p1p2/P7/2P2B1P/5PPK/4R3 w - - 1 31+a6a5+f7...
    Gen 1: Train_LP= -310.0, Ref_LP=   -0.0
           Text: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
           Normalized: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
    Gen 2: Train_LP= -328.0, Ref_LP=   -0.0
           Text: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
           Normalized: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
    Gen 3: Train_LP= -292.0, Ref_LP=   -0.0
           Text: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
           Normalized: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
    Gen 4: Train_LP= -334.0, Ref_LP=   -0.0
           Text: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
           Normalized: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...

üéØ Prompt 4: P: rnbqkbnr/pp1ppppp/2p5/8/4P3/2N5/PPPP1PPP/R1BQKBNR b KQkq ...
    Gen 1: Train_LP= -512.0, Ref_LP=  -25.5
           Text:                             M: e7e5 g7g6 d7d5 d7d6 d8c7     ...
           Normalized: M: e7e5 g7g6 d7d5 d7d6 d8c7 E: -0.53 -0.63 -0.35 -0.61 -0.69...
    Gen 2: Train_LP= -600.0, Ref_LP=  -22.8
           Text:                             M: e7e5 d7d6 d7d5 e7e6 g7g6     ...
           Normalized: M: e7e5 d7d6 d7d5 e7e6 g7g6 E: -0.53 -0.62 -0.48 -0.73 -0.74...
    Gen 3: Train_LP= -584.0, Ref_LP=  -23.8
           Text:                             M: d7d6 d7d5 e7e5 e7e6 g7g6     ...
           Normalized: M: d7d6 d7d5 e7e5 e7e6 g7g6 E: -0.74 -0.42 -0.52 -0.84 -0.73...
    Gen 4: Train_LP= -528.0, Ref_LP=  -24.5
           Text:                             M: d7d5 d7d6 e7e5 e7e6 g7g6     ...
           Normalized: M: d7d5 d7d6 e7e5 e7e6 g7g6 E: -0.49 -0.68 -0.73 -0.87 -0.66...

==================== PHASE 4: REWARD CALCULATION (step 13) ====================

üéØ Scoring Prompt 1 completions:
  Gen 1: Reward= 0.458 | M: d4d3 a6c5 c8b8 b6a5 h8e8 E: -3.68 -3....
  Gen 2: Reward= 0.260 | M: d4d3 a6c5 c8b8 h8h6 e7d7 E: -3.53 -3....
  Gen 3: Reward= 0.400 | M: d4d3 a6c5 c6f3 c8b8 h8h6 E: -3.61 -3....
  Gen 4: Reward= 0.209 | M: b6a5 c6d7 e7h4 a6c5 d4d3 E: -3.97 -3....
  üìä Average reward: 0.332

üéØ Scoring Prompt 2 completions:
  Gen 1: Reward= 0.335 | M: e2f3 e2f2 e2f1 c8e7 e2d3 E: 0.3 0.3 0...
  Gen 2: Reward= 0.415 | M: e2f2 e2f1 e2e1 c8e7 e2f3 E: 0.09 0.08...
  Gen 3: Reward= 0.519 | M: c8e7 e2f2 e2f3 f5g4 c8a7 E: 0.17 0.13...
  Gen 4: Reward= 0.433 | M: e2d3 e2f1 e2f2 c8e7 c8a7 E: 0.0 0.0 0...
  üìä Average reward: 0.425

üéØ Scoring Prompt 3 completions:
  Gen 1: Reward= 0.630 | 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b...
  Gen 2: Reward= 0.630 | 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b...
  Gen 3: Reward= 0.630 | 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b...
  Gen 4: Reward= 0.630 | 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b...
  üìä Average reward: 0.630

üéØ Scoring Prompt 4 completions:
  Gen 1: Reward= 0.380 | M: e7e5 g7g6 d7d5 d7d6 d8c7 E: -0.53 -0....
  Gen 2: Reward= 0.717 | M: e7e5 d7d6 d7d5 e7e6 g7g6 E: -0.53 -0....
  Gen 3: Reward= 0.366 | M: d7d6 d7d5 e7e5 e7e6 g7g6 E: -0.74 -0....
  Gen 4: Reward= 0.385 | M: d7d5 d7d6 e7e5 e7e6 g7g6 E: -0.49 -0....
  üìä Average reward: 0.462

==================== PHASE 5: TRL EXACT ADVANTAGE CALCULATION (step 13) ====================
üî¢ TRL Advantage Calculation (exact formula):
  Total rewards shape: torch.Size([16])
  Rewards: [0.4584     0.25977143 0.4004     0.20857143 0.335      0.415
 0.51857143 0.43337143 0.63       0.63       0.63       0.63
 0.37977143 0.71657143 0.36577143 0.38537143]

==================== PHASE 6-8: CORRECTED GRPO LOSS CALCULATION (step 13) ====================
üîÑ Computing GRPO loss with proper token-level KL...

üéØ Prompt 1 - Corrected GRPO Loss:
  Gen 1: A=+1.082, logp/len=-6.469, kl=-4.812
         PG=7.000, KL_penalty=-0.480, total=6.531
  Gen 2: A=-0.615, logp/len=-7.750, kl=-5.938
         PG=-4.781, KL_penalty=-0.594, total=-5.375
  Gen 3: A=+0.586, logp/len=-6.625, kl=-4.969
         PG=3.891, KL_penalty=-0.496, total=3.391
  Gen 4: A=-1.053, logp/len=-7.031, kl=-5.281
         PG=-7.406, KL_penalty=-0.527, total=-7.938
  üìä Prompt averages: PG=-0.320, KL=-0.527

üéØ Prompt 2 - Corrected GRPO Loss:
  Gen 1: A=-1.201, logp/len=-7.656, kl=-5.719
         PG=-9.188, KL_penalty=-0.570, total=-9.750
  Gen 2: A=-0.139, logp/len=-6.438, kl=-4.531
         PG=-0.895, KL_penalty=-0.453, total=-1.344
  Gen 3: A=+1.236, logp/len=-6.875, kl=-5.062
         PG=8.500, KL_penalty=-0.508, total=8.000
  Gen 4: A=+0.105, logp/len=-7.562, kl=-6.000
         PG=0.793, KL_penalty=-0.602, total=0.191
  üìä Prompt averages: PG=-0.192, KL=-0.531

üéØ Prompt 3 - Corrected GRPO Loss:
  Gen 1: A=+0.000, logp/len=-5.625, kl=-5.281
         PG=0.000, KL_penalty=-0.527, total=-0.527
  Gen 2: A=+0.000, logp/len=-6.406, kl=-6.062
         PG=0.000, KL_penalty=-0.605, total=-0.605
  Gen 3: A=+0.000, logp/len=-5.188, kl=-4.844
         PG=0.000, KL_penalty=-0.484, total=-0.484
  Gen 4: A=+0.000, logp/len=-5.250, kl=-4.875
         PG=0.000, KL_penalty=-0.488, total=-0.488
  üìä Prompt averages: PG=0.000, KL=-0.527

üéØ Prompt 4 - Corrected GRPO Loss:
  Gen 1: A=-0.483, logp/len=-6.875, kl=-5.219
         PG=-3.312, KL_penalty=-0.523, total=-3.844
  Gen 2: A=+1.498, logp/len=-8.125, kl=-6.562
         PG=12.188, KL_penalty=-0.656, total=11.500
  Gen 3: A=-0.565, logp/len=-7.094, kl=-5.500
         PG=-4.000, KL_penalty=-0.551, total=-4.562
  Gen 4: A=-0.450, logp/len=-7.812, kl=-6.188
         PG=-3.516, KL_penalty=-0.617, total=-4.125
  üìä Prompt averages: PG=0.340, KL=-0.586

üî¢ CORRECTED Loss Breakdown:
  Policy Gradient Loss:   -0.043
  KL Loss (with gradients):   -0.547
  Total Loss:          =  -0.590
  KL penalty ratio: 92.5%

üîÑ Performing corrected gradient update (step 13)...
  Total gradient norm: 21.58
  ‚úÖ Gradient update applied with proper KL regularization

==================== PHASE 9: POST-UPDATE PERFORMANCE (step 13) ====================
  Prompt 1: avg reward = 0.155
  Prompt 2: avg reward = 0.387
  Prompt 3: avg reward = 0.630
  Prompt 4: avg reward = 0.560
üìä POST-UPDATE (step 13) Performance:
  Average reward: 0.4328
  Positive ratio: 100.0%

==================== PHASE 10: ANALYSIS (step 13) ====================
üîç GRPO Step Impact (step 13):
  Post-update performance: 0.4328
  Step performance change: +0.0022

======================================================================
üß≠ SEQUENTIAL STEP 14/50
======================================================================
üßæ Overfit mode: reusing initial batch (no dataset slice advance)

ü§ñ Generating completions for GRPO training (step 14)...

üéØ Prompt 1: P: 2k4r/ppp1q1p1/nbb1N3/4PP2/3p2P1/PB5P/1PPK3R/3RQ3 b - - 3 ...
    Gen 1: Train_LP= -404.0, Ref_LP=  -34.5
           Text:                                M: c6f3 c6d7 a6c5 h8e8 c8b8  ...
           Normalized: M: c6f3 c6d7 a6c5 h8e8 c8b8 E: -3.8 -3.75 -3.56 -3.86 -3.76 ...
    Gen 2: Train_LP= -564.0, Ref_LP=  -33.0
           Text:                                M: e7d7 a6c5 d4d3 c8b8 b6a5  ...
           Normalized: M: e7d7 a6c5 d4d3 c8b8 b6a5 E: -4.01 -3.68 -3.8 -3.97 -4.08 ...
    Gen 3: Train_LP= -358.0, Ref_LP=  -35.0
           Text:                                M: c6d7 e7d7 h8h6 a6c5 d4d3  ...
           Normalized: M: c6d7 e7d7 h8h6 a6c5 d4d3 E: -4.0 -3.99 -4.11 -3.95 -4.01 ...
    Gen 4: Train_LP= -520.0, Ref_LP=  -33.8
           Text:                                M: d4d3 e7d7 e7e8 a6c5 b6a5  ...
           Normalized: M: d4d3 e7d7 e7e8 a6c5 b6a5 E: -3.42 -3.53 -3.6 -3.19 -3.33 ...

üéØ Prompt 2: P: 2N5/2k5/3p1p2/3P1BbP/4Pp2/1n6/4K3/8 w - - 9 49...
    Gen 1: Train_LP= -444.0, Ref_LP=  -78.0
           Text:                                             M: e2f1 e2f2 c8e...
           Normalized: M: e2f1 e2f2 c8e7 e2f3 c8a7 E: 0.22 0.27 0.27 0.27 0.27 B: e...
    Gen 2: Train_LP= -616.0, Ref_LP=  -16.0
           Text:                                             M: e2f1 e2f2 c8e...
           Normalized: M: e2f1 e2f2 c8e7 e2f3 f5g4 E: 0.0 0.0 0.0 0.0 0.0 B: e2f1...
    Gen 3: Train_LP= -286.0, Ref_LP=  -13.5
           Text:                                             M: e2f2 c8e7 e2f...
           Normalized: M: e2f2 c8e7 e2f1 e2f3 c8a7 E: 0.0 0.0 0.0 0.0 0.0 B: e2f2...
    Gen 4: Train_LP= -680.0, Ref_LP=  -13.6
           Text:                                             M: c8a7 e2f2 c8e...
           Normalized: M: c8a7 e2f2 c8e7 e2f3 e2f1 E: 0.0 0.0 0.0 0.0 0.0 B: c8a7...

üéØ Prompt 3: A: 8/2k2b2/R1p4r/3p1p2/P7/2P2B1P/5PPK/4R3 w - - 1 31+a6a5+f7...
    Gen 1: Train_LP= -282.0, Ref_LP=   -0.0
           Text: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
           Normalized: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
    Gen 2: Train_LP= -336.0, Ref_LP=   -0.0
           Text: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
           Normalized: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
    Gen 3: Train_LP= -288.0, Ref_LP=   -0.0
           Text: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
           Normalized: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
    Gen 4: Train_LP= -362.0, Ref_LP=   -0.0
           Text: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
           Normalized: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...

üéØ Prompt 4: P: rnbqkbnr/pp1ppppp/2p5/8/4P3/2N5/PPPP1PPP/R1BQKBNR b KQkq ...
    Gen 1: Train_LP= -668.0, Ref_LP=  -35.5
           Text:                             M: d7d6 g7g6 d7d5 e7e6 e7e5     ...
           Normalized: M: d7d6 g7g6 d7d5 e7e6 e7e5 E: -0.68 -0.69 -0.56 -0.75 -0.52...
    Gen 2: Train_LP= -510.0, Ref_LP=  -34.5
           Text:                             M: d7d6 d7d5 g7g6 e7e5 c6c5     ...
           Normalized: M: d7d6 d7d5 g7g6 e7e5 c6c5 E: -0.56 -0.38 -0.63 -0.58 -0.65...
    Gen 3: Train_LP= -512.0, Ref_LP=  -24.5
           Text:                             M: e7e6 d7d5 d7d6 e7e5 g7g6     ...
           Normalized: M: e7e6 d7d5 d7d6 e7e5 g7g6 E: -0.73 -0.35 -0.56 -0.55 -0.7 ...
    Gen 4: Train_LP= -504.0, Ref_LP=  -25.0
           Text:                             M: e7e6 d7d5 g7g6 d7d6 e7e5     ...
           Normalized: M: e7e6 d7d5 g7g6 d7d6 e7e5 E: -0.92 -0.4 -0.93 -0.77 -0.76 ...

==================== PHASE 4: REWARD CALCULATION (step 14) ====================

üéØ Scoring Prompt 1 completions:
  Gen 1: Reward= 0.248 | M: c6f3 c6d7 a6c5 h8e8 c8b8 E: -3.8 -3.7...
  Gen 2: Reward= 0.785 | M: e7d7 a6c5 d4d3 c8b8 b6a5 E: -4.01 -3....
  Gen 3: Reward= 0.218 | M: c6d7 e7d7 h8h6 a6c5 d4d3 E: -4.0 -3.9...
  Gen 4: Reward= 0.179 | M: d4d3 e7d7 e7e8 a6c5 b6a5 E: -3.42 -3....
  üìä Average reward: 0.357

üéØ Scoring Prompt 2 completions:
  Gen 1: Reward= 0.389 | M: e2f1 e2f2 c8e7 e2f3 c8a7 E: 0.22 0.27...
  Gen 2: Reward=-0.055 | M: e2f1 e2f2 c8e7 e2f3 f5g4 E: 0.0 0.0 0...
  Gen 3: Reward= 0.329 | M: e2f2 c8e7 e2f1 e2f3 c8a7 E: 0.0 0.0 0...
  Gen 4: Reward= 0.249 | M: c8a7 e2f2 c8e7 e2f3 e2f1 E: 0.0 0.0 0...
  üìä Average reward: 0.228

üéØ Scoring Prompt 3 completions:
  Gen 1: Reward= 0.630 | 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b...
  Gen 2: Reward= 0.630 | 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b...
  Gen 3: Reward= 0.630 | 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b...
  Gen 4: Reward= 0.630 | 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b...
  üìä Average reward: 0.630

üéØ Scoring Prompt 4 completions:
  Gen 1: Reward= 0.383 | M: d7d6 g7g6 d7d5 e7e6 e7e5 E: -0.68 -0....
  Gen 2: Reward= 0.690 | M: d7d6 d7d5 g7g6 e7e5 c6c5 E: -0.56 -0....
  Gen 3: Reward= 0.369 | M: e7e6 d7d5 d7d6 e7e5 g7g6 E: -0.73 -0....
  Gen 4: Reward= 0.699 | M: e7e6 d7d5 g7g6 d7d6 e7e5 E: -0.92 -0....
  üìä Average reward: 0.535

==================== PHASE 5: TRL EXACT ADVANTAGE CALCULATION (step 14) ====================
üî¢ TRL Advantage Calculation (exact formula):
  Total rewards shape: torch.Size([16])
  Rewards: [ 0.24817143  0.7852      0.2178      0.17857143  0.38857143 -0.055
  0.32857143  0.24857143  0.63        0.63        0.63        0.63
  0.38257143  0.69        0.36937143  0.69897143]

==================== PHASE 6-8: CORRECTED GRPO LOSS CALCULATION (step 14) ====================
üîÑ Computing GRPO loss with proper token-level KL...

üéØ Prompt 1 - Corrected GRPO Loss:
  Gen 1: A=-0.381, logp/len=-6.750, kl=-4.969
         PG=-2.578, KL_penalty=-0.496, total=-3.078
  Gen 2: A=+1.493, logp/len=-6.438, kl=-4.781
         PG=9.625, KL_penalty=-0.479, total=9.125
  Gen 3: A=-0.487, logp/len=-7.750, kl=-6.062
         PG=-3.781, KL_penalty=-0.605, total=-4.375
  Gen 4: A=-0.624, logp/len=-7.844, kl=-6.125
         PG=-4.906, KL_penalty=-0.613, total=-5.531
  üìä Prompt averages: PG=-0.406, KL=-0.547

üéØ Prompt 2 - Corrected GRPO Loss:
  Gen 1: A=+0.817, logp/len=-6.125, kl=-4.344
         PG=5.000, KL_penalty=-0.434, total=4.562
  Gen 2: A=-1.435, logp/len=-6.750, kl=-5.188
         PG=-9.688, KL_penalty=-0.520, total=-10.188
  Gen 3: A=+0.512, logp/len=-7.156, kl=-5.625
         PG=3.672, KL_penalty=-0.562, total=3.109
  Gen 4: A=+0.106, logp/len=-8.000, kl=-6.188
         PG=0.848, KL_penalty=-0.617, total=0.230
  üìä Prompt averages: PG=-0.042, KL=-0.531

üéØ Prompt 3 - Corrected GRPO Loss:
  Gen 1: A=+0.000, logp/len=-4.969, kl=-4.594
         PG=0.000, KL_penalty=-0.459, total=-0.459
  Gen 2: A=+0.000, logp/len=-5.969, kl=-5.625
         PG=0.000, KL_penalty=-0.562, total=-0.562
  Gen 3: A=+0.000, logp/len=-5.750, kl=-5.406
         PG=0.000, KL_penalty=-0.539, total=-0.539
  Gen 4: A=+0.000, logp/len=-4.875, kl=-4.500
         PG=0.000, KL_penalty=-0.449, total=-0.449
  üìä Prompt averages: PG=0.000, KL=-0.504

üéØ Prompt 4 - Corrected GRPO Loss:
  Gen 1: A=-0.830, logp/len=-7.406, kl=-5.688
         PG=-6.156, KL_penalty=-0.570, total=-6.719
  Gen 2: A=+0.841, logp/len=-5.906, kl=-4.219
         PG=4.969, KL_penalty=-0.422, total=4.562
  Gen 3: A=-0.901, logp/len=-7.125, kl=-5.438
         PG=-6.438, KL_penalty=-0.543, total=-6.969
  Gen 4: A=+0.890, logp/len=-7.625, kl=-6.031
         PG=6.781, KL_penalty=-0.602, total=6.188
  üìä Prompt averages: PG=-0.211, KL=-0.531

üî¢ CORRECTED Loss Breakdown:
  Policy Gradient Loss:   -0.165
  KL Loss (with gradients):   -0.527
  Total Loss:          =  -0.691
  KL penalty ratio: 76.0%

üîÑ Performing corrected gradient update (step 14)...
  Total gradient norm: 23.75
  ‚úÖ Gradient update applied with proper KL regularization

==================== PHASE 9: POST-UPDATE PERFORMANCE (step 14) ====================
  Prompt 1: avg reward = 0.157
  Prompt 2: avg reward = 0.387
  Prompt 3: avg reward = 0.630
  Prompt 4: avg reward = 0.397
üìä POST-UPDATE (step 14) Performance:
  Average reward: 0.3925
  Positive ratio: 100.0%

==================== PHASE 10: ANALYSIS (step 14) ====================
üîç GRPO Step Impact (step 14):
  Post-update performance: 0.3925
  Step performance change: -0.0403

======================================================================
üß≠ SEQUENTIAL STEP 15/50
======================================================================
üßæ Overfit mode: reusing initial batch (no dataset slice advance)

ü§ñ Generating completions for GRPO training (step 15)...

üéØ Prompt 1: P: 2k4r/ppp1q1p1/nbb1N3/4PP2/3p2P1/PB5P/1PPK3R/3RQ3 b - - 3 ...
    Gen 1: Train_LP= -420.0, Ref_LP=  -34.2
           Text:                                M: a6c5 c8b8 e7d7 e7e8 c6f3  ...
           Normalized: M: a6c5 c8b8 e7d7 e7e8 c6f3 E: -3.7 -3.98 -4.01 -4.15 -4.25 ...
    Gen 2: Train_LP= -458.0, Ref_LP=  -43.5
           Text:                                M: e7d7 c8b8 d4d3 a6c5 b6a5  ...
           Normalized: M: e7d7 c8b8 d4d3 a6c5 b6a5 E: -3.62 -3.68 -3.49 -3.43 -3.63...
    Gen 3: Train_LP= -684.0, Ref_LP=  -44.5
           Text:                                M: h8e8 e7d7 a6c5 d4d3 b6a5  ...
           Normalized: M: h8e8 e7d7 a6c5 d4d3 b6a5 E: -3.78 -3.58 -3.33 -3.68 -3.47...
    Gen 4: Train_LP= -452.0, Ref_LP=  -44.2
           Text:                                M: e7d7 a6c5 c8b8 e7e8 c6f3  ...
           Normalized: M: e7d7 a6c5 c8b8 e7e8 c6f3 E: -3.84 -3.76 -3.83 -3.84 -3.83...

üéØ Prompt 2: P: 2N5/2k5/3p1p2/3P1BbP/4Pp2/1n6/4K3/8 w - - 9 49...
    Gen 1: Train_LP= -560.0, Ref_LP=  -14.4
           Text:                                             M: c8e7 e2f3 e2f...
           Normalized: M: c8e7 e2f3 e2f1 c8a7 e2f2 E: 0.0 0.0 0.0 0.0 0.0 B: c8a7...
    Gen 2: Train_LP= -510.0, Ref_LP=  -14.6
           Text:                                             M: c8e7 e2f3 e2f...
           Normalized: M: c8e7 e2f3 e2f1 e2f2 c8a7 E: 0.0 0.0 0.0 0.0 0.0 B: e2f3...
    Gen 3: Train_LP= -604.0, Ref_LP=  -79.0
           Text:                                             M: e2f3 e2d3 c8a...
           Normalized: M: e2f3 e2d3 c8a7 c8e7 e2f2 E: 0.11 0.13 0.15 0.11 0.14 B: c...
    Gen 4: Train_LP= -644.0, Ref_LP=  -79.5
           Text:                                             M: f5e6 e2f2 e2f...
           Normalized: M: f5e6 e2f2 e2f3 f5g4 c8e7 E: 0.21 0.24 0.21 0.23 0.23 B: e...

üéØ Prompt 3: A: 8/2k2b2/R1p4r/3p1p2/P7/2P2B1P/5PPK/4R3 w - - 1 31+a6a5+f7...
    Gen 1: Train_LP= -262.0, Ref_LP=   -0.0
           Text: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
           Normalized: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
    Gen 2: Train_LP= -280.0, Ref_LP=   -0.0
           Text: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
           Normalized: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
    Gen 3: Train_LP= -314.0, Ref_LP=   -0.0
           Text: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
           Normalized: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
    Gen 4: Train_LP= -260.0, Ref_LP=   -0.0
           Text: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
           Normalized: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...

üéØ Prompt 4: P: rnbqkbnr/pp1ppppp/2p5/8/4P3/2N5/PPPP1PPP/R1BQKBNR b KQkq ...
    Gen 1: Train_LP= -540.0, Ref_LP=  -24.0
           Text:                             M: d7d6 g7g6 d7d5 e7e5 c6c5     ...
           Normalized: M: d7d6 g7g6 d7d5 e7e5 c6c5 E: -0.65 -0.6 -0.51 -0.62 -0.68 ...
    Gen 2: Train_LP= -420.0, Ref_LP=  -35.8
           Text:                             M: e7e5 d7d5 d7d6 e7e6 c6c5     ...
           Normalized: M: e7e5 d7d5 d7d6 e7e6 c6c5 E: -0.68 -0.56 -0.73 -0.84 -0.89...
    Gen 3: Train_LP= -624.0, Ref_LP=  -36.0
           Text:                             M: d7d5 e7e5 g7g6 d7d6 a7a6     ...
           Normalized: M: d7d5 e7e5 g7g6 d7d6 a7a6 E: -0.37 -0.54 -0.63 -0.54 -0.92...
    Gen 4: Train_LP= -482.0, Ref_LP=  -36.5
           Text:                             M: d7d6 e7e5 d7d5 g7g6 a7a6     ...
           Normalized: M: d7d6 e7e5 d7d5 g7g6 a7a6 E: -0.58 -0.65 -0.46 -0.74 -0.74...

==================== PHASE 4: REWARD CALCULATION (step 15) ====================

üéØ Scoring Prompt 1 completions:
  Gen 1: Reward= 0.521 | M: a6c5 c8b8 e7d7 e7e8 c6f3 E: -3.7 -3.9...
  Gen 2: Reward= 0.434 | M: e7d7 c8b8 d4d3 a6c5 b6a5 E: -3.62 -3....
  Gen 3: Reward= 0.204 | M: h8e8 e7d7 a6c5 d4d3 b6a5 E: -3.78 -3....
  Gen 4: Reward= 0.285 | M: e7d7 a6c5 c8b8 e7e8 c6f3 E: -3.84 -3....
  üìä Average reward: 0.361

üéØ Scoring Prompt 2 completions:
  Gen 1: Reward= 0.259 | M: c8e7 e2f3 e2f1 c8a7 e2f2 E: 0.0 0.0 0...
  Gen 2: Reward= 0.009 | M: c8e7 e2f3 e2f1 e2f2 c8a7 E: 0.0 0.0 0...
  Gen 3: Reward= 0.359 | M: e2f3 e2d3 c8a7 c8e7 e2f2 E: 0.11 0.13...
  Gen 4: Reward= 0.295 | M: f5e6 e2f2 e2f3 f5g4 c8e7 E: 0.21 0.24...
  üìä Average reward: 0.230

üéØ Scoring Prompt 3 completions:
  Gen 1: Reward= 0.630 | 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b...
  Gen 2: Reward= 0.630 | 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b...
  Gen 3: Reward= 0.630 | 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b...
  Gen 4: Reward= 0.630 | 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b...
  üìä Average reward: 0.630

üéØ Scoring Prompt 4 completions:
  Gen 1: Reward= 0.702 | M: d7d6 g7g6 d7d5 e7e5 c6c5 E: -0.65 -0....
  Gen 2: Reward= 0.600 | M: e7e5 d7d5 d7d6 e7e6 c6c5 E: -0.68 -0....
  Gen 3: Reward= 0.394 | M: d7d5 e7e5 g7g6 d7d6 a7a6 E: -0.37 -0....
  Gen 4: Reward= 0.366 | M: d7d6 e7e5 d7d5 g7g6 a7a6 E: -0.58 -0....
  üìä Average reward: 0.515

==================== PHASE 5: TRL EXACT ADVANTAGE CALCULATION (step 15) ====================
üî¢ TRL Advantage Calculation (exact formula):
  Total rewards shape: torch.Size([16])
  Rewards: [0.52057143 0.4336     0.20377143 0.28457143 0.25857143 0.00857143
 0.35857143 0.295      0.63       0.63       0.63       0.63
 0.702      0.59977143 0.39377143 0.36617143]

==================== PHASE 6-8: CORRECTED GRPO LOSS CALCULATION (step 15) ====================
üîÑ Computing GRPO loss with proper token-level KL...

üéØ Prompt 1 - Corrected GRPO Loss:
  Gen 1: A=+1.119, logp/len=-6.031, kl=-4.188
         PG=6.750, KL_penalty=-0.418, total=6.344
  Gen 2: A=+0.511, logp/len=-5.469, kl=-3.812
         PG=2.797, KL_penalty=-0.381, total=2.422
  Gen 3: A=-1.097, logp/len=-8.062, kl=-6.281
         PG=-8.875, KL_penalty=-0.629, total=-9.500
  Gen 4: A=-0.532, logp/len=-7.531, kl=-5.781
         PG=-4.000, KL_penalty=-0.578, total=-4.562
  üìä Prompt averages: PG=-0.828, KL=-0.500

üéØ Prompt 2 - Corrected GRPO Loss:
  Gen 1: A=+0.185, logp/len=-5.531, kl=-3.781
         PG=1.023, KL_penalty=-0.379, total=0.645
  Gen 2: A=-1.445, logp/len=-7.719, kl=-6.188
         PG=-11.125, KL_penalty=-0.617, total=-11.750
  Gen 3: A=+0.837, logp/len=-4.812, kl=-2.797
         PG=4.031, KL_penalty=-0.279, total=3.750
  Gen 4: A=+0.423, logp/len=-6.406, kl=-4.594
         PG=2.703, KL_penalty=-0.459, total=2.250
  üìä Prompt averages: PG=-0.848, KL=-0.434

üéØ Prompt 3 - Corrected GRPO Loss:
  Gen 1: A=+0.000, logp/len=-6.031, kl=-5.656
         PG=0.000, KL_penalty=-0.566, total=-0.566
  Gen 2: A=+0.000, logp/len=-5.156, kl=-4.812
         PG=0.000, KL_penalty=-0.480, total=-0.480
  Gen 3: A=+0.000, logp/len=-5.406, kl=-5.031
         PG=0.000, KL_penalty=-0.504, total=-0.504
  Gen 4: A=+0.000, logp/len=-4.906, kl=-4.562
         PG=0.000, KL_penalty=-0.457, total=-0.457
  üìä Prompt averages: PG=0.000, KL=-0.500

üéØ Prompt 4 - Corrected GRPO Loss:
  Gen 1: A=+1.150, logp/len=-6.594, kl=-4.844
         PG=7.594, KL_penalty=-0.484, total=7.125
  Gen 2: A=+0.520, logp/len=-7.438, kl=-5.656
         PG=3.859, KL_penalty=-0.566, total=3.297
  Gen 3: A=-0.750, logp/len=-6.750, kl=-5.062
         PG=-5.062, KL_penalty=-0.508, total=-5.562
  Gen 4: A=-0.920, logp/len=-6.406, kl=-4.844
         PG=-5.906, KL_penalty=-0.484, total=-6.375
  üìä Prompt averages: PG=0.117, KL=-0.508

üî¢ CORRECTED Loss Breakdown:
  Policy Gradient Loss:   -0.389
  KL Loss (with gradients):   -0.486
  Total Loss:          =  -0.875
  KL penalty ratio: 55.5%

üîÑ Performing corrected gradient update (step 15)...
  Total gradient norm: 36.48
  ‚úÖ Gradient update applied with proper KL regularization

==================== PHASE 9: POST-UPDATE PERFORMANCE (step 15) ====================
  Prompt 1: avg reward = 0.155
  Prompt 2: avg reward = 0.387
  Prompt 3: avg reward = 0.630
  Prompt 4: avg reward = 0.559
üìä POST-UPDATE (step 15) Performance:
  Average reward: 0.4329
  Positive ratio: 100.0%

==================== PHASE 10: ANALYSIS (step 15) ====================
üîç GRPO Step Impact (step 15):
  Post-update performance: 0.4329
  Step performance change: +0.0404

======================================================================
üß≠ SEQUENTIAL STEP 16/50
======================================================================
üßæ Overfit mode: reusing initial batch (no dataset slice advance)

ü§ñ Generating completions for GRPO training (step 16)...

üéØ Prompt 1: P: 2k4r/ppp1q1p1/nbb1N3/4PP2/3p2P1/PB5P/1PPK3R/3RQ3 b - - 3 ...
    Gen 1: Train_LP= -474.0, Ref_LP=  -35.2
           Text:                                M: a6c5 e7d7 c6f3 h8e8 c6d7  ...
           Normalized: M: a6c5 e7d7 c6f3 h8e8 c6d7 E: -3.7 -4.23 -4.03 -3.99 -4.1 B...
    Gen 2: Train_LP= -364.0, Ref_LP=  -56.5
           Text:                                M: a6c5 c6d7 c8b8 h8h6 d4d3  ...
           Normalized: M: a6c5 c6d7 c8b8 h8h6 d4d3 E: -3.58 -3.99 -4.22 -4.08 -3.98...
    Gen 3: Train_LP= -604.0, Ref_LP=  -47.0
           Text:                                M: b6a5 h8h3 a6c5 h8h6 d4d3  ...
           Normalized: M: b6a5 h8h3 a6c5 h8h6 d4d3 E: -4.0 -4.34 -3.95 -4.05 -3.72 ...
    Gen 4: Train_LP= -620.0, Ref_LP=  -57.0
           Text:                                M: h8h4 a6c5 h8e8 e7h4 c8b8  ...
           Normalized: M: h8h4 a6c5 h8e8 e7h4 c8b8 E: -4.12 -4.06 -4.21 -4.21 -4.16...

üéØ Prompt 2: P: 2N5/2k5/3p1p2/3P1BbP/4Pp2/1n6/4K3/8 w - - 9 49...
    Gen 1: Train_LP= -868.0, Ref_LP=  -67.5
           Text:                                             M: e2f3 c8e7 e2f...
           Normalized: M: e2f3 c8e7 e2f2 e2d3 c8a7 E: 0.09 0.09 0.09 0.0 0.09 B: c8...
    Gen 2: Train_LP= -358.0, Ref_LP=  -79.5
           Text:                                             M: e2d1 e2f2 c8a...
           Normalized: M: e2d1 e2f2 c8a7 e2f3 e2f1 E: 0.15 0.13 0.13 0.12 0.13 B: e...
    Gen 3: Train_LP= -532.0, Ref_LP=  -68.0
           Text:                                             M: e2f2 c8e7 f5h...
           Normalized: M: e2f2 c8e7 f5h3 e2f3 f5g4 E: 0.12 0.11 0.1 0.11 0.12 B: e2...
    Gen 4: Train_LP= -470.0, Ref_LP=  -15.1
           Text:                                             M: f5g6 c8e7 e2f...
           Normalized: M: f5g6 c8e7 e2f3 e2f2 c8a7 E: 0.0 0.0 0.0 0.0 0.0 B: f5g6...

üéØ Prompt 3: A: 8/2k2b2/R1p4r/3p1p2/P7/2P2B1P/5PPK/4R3 w - - 1 31+a6a5+f7...
    Gen 1: Train_LP= -296.0, Ref_LP=   -0.0
           Text: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
           Normalized: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
    Gen 2: Train_LP= -314.0, Ref_LP=   -0.0
           Text: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
           Normalized: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
    Gen 3: Train_LP= -304.0, Ref_LP=   -0.0
           Text: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
           Normalized: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
    Gen 4: Train_LP= -298.0, Ref_LP=   -0.0
           Text: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
           Normalized: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...

üéØ Prompt 4: P: rnbqkbnr/pp1ppppp/2p5/8/4P3/2N5/PPPP1PPP/R1BQKBNR b KQkq ...
    Gen 1: Train_LP= -332.0, Ref_LP=  -37.0
           Text:                             M: d7d6 e7e5 d7d5 c6c5 d8c7     ...
           Normalized: M: d7d6 e7e5 d7d5 c6c5 d8c7 E: -0.63 -0.48 -0.4 -0.64 -0.85 ...
    Gen 2: Train_LP= -676.0, Ref_LP=  -46.2
           Text:                             M: d7d5 d7d6 e7e5 e7e6 c6c5     ...
           Normalized: M: d7d5 d7d6 e7e5 e7e6 c6c5 E: -0.49 -0.67 -0.63 -0.71 -0.84...
    Gen 3: Train_LP= -596.0, Ref_LP=  -35.5
           Text:                             M: e7e6 d7d5 e7e5 d7d6 g7g6     ...
           Normalized: M: e7e6 d7d5 e7e5 d7d6 g7g6 E: -0.7 -0.33 -0.51 -0.63 -0.62 ...
    Gen 4: Train_LP= -552.0, Ref_LP=  -23.6
           Text:                             M: e7e5 d7d5 d7d6 e7e6 g7g6     ...
           Normalized: M: e7e5 d7d5 d7d6 e7e6 g7g6 E: -0.56 -0.48 -0.67 -0.7 -0.7 B...

==================== PHASE 4: REWARD CALCULATION (step 16) ====================

üéØ Scoring Prompt 1 completions:
  Gen 1: Reward= 0.223 | M: a6c5 e7d7 c6f3 h8e8 c6d7 E: -3.7 -4.2...
  Gen 2: Reward= 0.441 | M: a6c5 c6d7 c8b8 h8h6 d4d3 E: -3.58 -3....
  Gen 3: Reward= 0.209 | M: b6a5 h8h3 a6c5 h8h6 d4d3 E: -4.0 -4.3...
  Gen 4: Reward= 0.164 | M: h8h4 a6c5 h8e8 e7h4 c8b8 E: -4.12 -4....
  üìä Average reward: 0.259

üéØ Scoring Prompt 2 completions:
  Gen 1: Reward= 0.349 | M: e2f3 c8e7 e2f2 e2d3 c8a7 E: 0.09 0.09...
  Gen 2: Reward= 0.199 | M: e2d1 e2f2 c8a7 e2f3 e2f1 E: 0.15 0.13...
  Gen 3: Reward= 0.275 | M: e2f2 c8e7 f5h3 e2f3 f5g4 E: 0.12 0.11...
  Gen 4: Reward=-0.021 | M: f5g6 c8e7 e2f3 e2f2 c8a7 E: 0.0 0.0 0...
  üìä Average reward: 0.200

üéØ Scoring Prompt 3 completions:
  Gen 1: Reward= 0.630 | 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b...
  Gen 2: Reward= 0.630 | 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b...
  Gen 3: Reward= 0.630 | 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b...
  Gen 4: Reward= 0.630 | 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b...
  üìä Average reward: 0.630

üéØ Scoring Prompt 4 completions:
  Gen 1: Reward= 0.370 | M: d7d6 e7e5 d7d5 c6c5 d8c7 E: -0.63 -0....
  Gen 2: Reward= 0.638 | M: d7d5 d7d6 e7e5 e7e6 c6c5 E: -0.49 -0....
  Gen 3: Reward= 0.359 | M: e7e6 d7d5 e7e5 d7d6 g7g6 E: -0.7 -0.3...
  Gen 4: Reward= 0.365 | M: e7e5 d7d5 d7d6 e7e6 g7g6 E: -0.56 -0....
  üìä Average reward: 0.433

==================== PHASE 5: TRL EXACT ADVANTAGE CALCULATION (step 16) ====================
üî¢ TRL Advantage Calculation (exact formula):
  Total rewards shape: torch.Size([16])
  Rewards: [ 0.2226      0.44137143  0.20857143  0.1638      0.34857143  0.19857143
  0.275      -0.02142857  0.63        0.63        0.63        0.63
  0.37017143  0.63817143  0.35937143  0.36497143]

==================== PHASE 6-8: CORRECTED GRPO LOSS CALCULATION (step 16) ====================
üîÑ Computing GRPO loss with proper token-level KL...

üéØ Prompt 1 - Corrected GRPO Loss:
  Gen 1: A=-0.294, logp/len=-7.594, kl=-5.750
         PG=-2.234, KL_penalty=-0.574, total=-2.812
  Gen 2: A=+1.469, logp/len=-7.688, kl=-5.938
         PG=11.312, KL_penalty=-0.594, total=10.750
  Gen 3: A=-0.407, logp/len=-7.250, kl=-5.500
         PG=-2.953, KL_penalty=-0.551, total=-3.500
  Gen 4: A=-0.768, logp/len=-8.062, kl=-6.375
         PG=-6.188, KL_penalty=-0.637, total=-6.812
  üìä Prompt averages: PG=-0.016, KL=-0.590

üéØ Prompt 2 - Corrected GRPO Loss:
  Gen 1: A=+0.928, logp/len=-7.688, kl=-5.812
         PG=7.125, KL_penalty=-0.582, total=6.531
  Gen 2: A=-0.010, logp/len=-7.125, kl=-5.281
         PG=-0.072, KL_penalty=-0.527, total=-0.598
  Gen 3: A=+0.468, logp/len=-8.312, kl=-6.469
         PG=3.891, KL_penalty=-0.648, total=3.250
  Gen 4: A=-1.386, logp/len=-7.906, kl=-6.250
         PG=-10.938, KL_penalty=-0.625, total=-11.562
  üìä Prompt averages: PG=0.000, KL=-0.594

üéØ Prompt 3 - Corrected GRPO Loss:
  Gen 1: A=+0.000, logp/len=-5.188, kl=-4.844
         PG=0.000, KL_penalty=-0.484, total=-0.484
  Gen 2: A=+0.000, logp/len=-6.125, kl=-5.750
         PG=0.000, KL_penalty=-0.574, total=-0.574
  Gen 3: A=+0.000, logp/len=-5.750, kl=-5.375
         PG=0.000, KL_penalty=-0.539, total=-0.539
  Gen 4: A=+0.000, logp/len=-5.875, kl=-5.531
         PG=0.000, KL_penalty=-0.555, total=-0.555
  üìä Prompt averages: PG=0.000, KL=-0.539

üéØ Prompt 4 - Corrected GRPO Loss:
  Gen 1: A=-0.461, logp/len=-6.688, kl=-5.031
         PG=-3.078, KL_penalty=-0.504, total=-3.578
  Gen 2: A=+1.499, logp/len=-7.875, kl=-6.188
         PG=11.812, KL_penalty=-0.617, total=11.188
  Gen 3: A=-0.540, logp/len=-7.125, kl=-5.500
         PG=-3.844, KL_penalty=-0.551, total=-4.406
  Gen 4: A=-0.499, logp/len=-7.375, kl=-5.719
         PG=-3.672, KL_penalty=-0.570, total=-4.250
  üìä Prompt averages: PG=0.309, KL=-0.562

üî¢ CORRECTED Loss Breakdown:
  Policy Gradient Loss:    0.073
  KL Loss (with gradients):   -0.570
  Total Loss:          =  -0.496
  KL penalty ratio: 88.5%

üîÑ Performing corrected gradient update (step 16)...
  Total gradient norm: 17.25
  ‚úÖ Gradient update applied with proper KL regularization

==================== PHASE 9: POST-UPDATE PERFORMANCE (step 16) ====================
  Prompt 1: avg reward = 0.150
  Prompt 2: avg reward = 0.387
  Prompt 3: avg reward = 0.630
  Prompt 4: avg reward = 0.559
üìä POST-UPDATE (step 16) Performance:
  Average reward: 0.4317
  Positive ratio: 100.0%

==================== PHASE 10: ANALYSIS (step 16) ====================
üîç GRPO Step Impact (step 16):
  Post-update performance: 0.4317
  Step performance change: -0.0012

======================================================================
üß≠ SEQUENTIAL STEP 17/50
======================================================================
üßæ Overfit mode: reusing initial batch (no dataset slice advance)

ü§ñ Generating completions for GRPO training (step 17)...

üéØ Prompt 1: P: 2k4r/ppp1q1p1/nbb1N3/4PP2/3p2P1/PB5P/1PPK3R/3RQ3 b - - 3 ...
    Gen 1: Train_LP= -524.0, Ref_LP=  -35.5
           Text:                                M: b6a5 a6c5 h8h3 c6f3 d4d3  ...
           Normalized: M: b6a5 a6c5 h8h3 c6f3 d4d3 E: -3.65 -3.33 -3.6 -3.77 -3.72 ...
    Gen 2: Train_LP= -402.0, Ref_LP=  -34.8
           Text:                                M: e7h4 d4d3 a6c5 e7e8 e7d7  ...
           Normalized: M: e7h4 d4d3 a6c5 e7e8 e7d7 E: -4.52 -4.19 -3.7 -4.35 -4.39 ...
    Gen 3: Train_LP= -576.0, Ref_LP=  -45.8
           Text:                                M: h8e8 d4d3 c6f3 a6c5 c6b5  ...
           Normalized: M: h8e8 d4d3 c6f3 a6c5 c6b5 E: -4.03 -3.72 -4.09 -3.74 -4.03...
    Gen 4: Train_LP= -408.0, Ref_LP=  -45.2
           Text:                                M: e7e8 c8b8 e7d7 a6c5 e7f7  ...
           Normalized: M: e7e8 c8b8 e7d7 a6c5 e7f7 E: -3.59 -3.66 -3.49 -3.51 -3.53...

üéØ Prompt 2: P: 2N5/2k5/3p1p2/3P1BbP/4Pp2/1n6/4K3/8 w - - 9 49...
    Gen 1: Train_LP= -472.0, Ref_LP=  -13.4
           Text:                                             M: e2f2 e2f1 e2f...
           Normalized: M: e2f2 e2f1 e2f3 c8e7 c8a7 E: 0.0 0.0 0.0 0.0 0.0 B: e2f3...
    Gen 2: Train_LP= -844.0, Ref_LP=  -77.5
           Text:                                             M: e2f3 c8e7 e2f...
           Normalized: M: e2f3 c8e7 e2f2 e2d3 c8a7 E: 0.08 0.07 0.07 0.04 0.07 B: e...
    Gen 3: Train_LP= -576.0, Ref_LP=  -69.0
           Text:                                             M: e2f3 c8a7 e2d...
           Normalized: M: e2f3 c8a7 e2d3 e2f2 c8e7 E: 0.33 0.27 0.3 0.32 0.31 B: e2...
    Gen 4: Train_LP= -402.0, Ref_LP=  -78.5
           Text:                                             M: c8e7 e2f3 e2f...
           Normalized: M: c8e7 e2f3 e2f2 e2d3 c8a7 E: 0.16 0.15 0.17 0.17 0.16 B: e...

üéØ Prompt 3: A: 8/2k2b2/R1p4r/3p1p2/P7/2P2B1P/5PPK/4R3 w - - 1 31+a6a5+f7...
    Gen 1: Train_LP= -352.0, Ref_LP=   -0.0
           Text: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
           Normalized: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
    Gen 2: Train_LP= -256.0, Ref_LP=   -0.0
           Text: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
           Normalized: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
    Gen 3: Train_LP= -330.0, Ref_LP=   -0.0
           Text: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
           Normalized: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
    Gen 4: Train_LP= -280.0, Ref_LP=   -0.0
           Text: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
           Normalized: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...

üéØ Prompt 4: P: rnbqkbnr/pp1ppppp/2p5/8/4P3/2N5/PPPP1PPP/R1BQKBNR b KQkq ...
    Gen 1: Train_LP= -604.0, Ref_LP=  -36.0
           Text:                             M: d7d5 d7d6 e7e5 c6c5 g7g6     ...
           Normalized: M: d7d5 d7d6 e7e5 c6c5 g7g6 E: -0.38 -0.67 -0.64 -0.59 -0.78...
    Gen 2: Train_LP= -410.0, Ref_LP=  -25.0
           Text:                             M: d7d5 d7d6 e7e5 g7g6 c6c5     ...
           Normalized: M: d7d5 d7d6 e7e5 g7g6 c6c5 E: -0.36 -0.59 -0.54 -0.54 -0.6 ...
    Gen 3: Train_LP= -428.0, Ref_LP=  -34.8
           Text:                             M: d7d6 e7e6 e7e5 d7d5 g7g6     ...
           Normalized: M: d7d6 e7e6 e7e5 d7d5 g7g6 E: -0.65 -0.68 -0.57 -0.45 -0.68...
    Gen 4: Train_LP= -620.0, Ref_LP=  -36.2
           Text:                             M: d7d6 e7e6 d7d5 e7e5 c6c5     ...
           Normalized: M: d7d6 e7e6 d7d5 e7e5 c6c5 E: -0.72 -0.73 -0.48 -0.65 -0.85...

==================== PHASE 4: REWARD CALCULATION (step 17) ====================

üéØ Scoring Prompt 1 completions:
  Gen 1: Reward= 0.550 | M: b6a5 a6c5 h8h3 c6f3 d4d3 E: -3.65 -3....
  Gen 2: Reward= 0.247 | M: e7h4 d4d3 a6c5 e7e8 e7d7 E: -4.52 -4....
  Gen 3: Reward= 0.522 | M: h8e8 d4d3 c6f3 a6c5 c6b5 E: -4.03 -3....
  Gen 4: Reward= 0.275 | M: e7e8 c8b8 e7d7 a6c5 e7f7 E: -3.59 -3....
  üìä Average reward: 0.398

üéØ Scoring Prompt 2 completions:
  Gen 1: Reward=-0.021 | M: e2f2 e2f1 e2f3 c8e7 c8a7 E: 0.0 0.0 0...
  Gen 2: Reward= 0.099 | M: e2f3 c8e7 e2f2 e2d3 c8a7 E: 0.08 0.07...
  Gen 3: Reward= 0.099 | M: e2f3 c8a7 e2d3 e2f2 c8e7 E: 0.33 0.27...
  Gen 4: Reward= 0.129 | M: c8e7 e2f3 e2f2 e2d3 c8a7 E: 0.16 0.15...
  üìä Average reward: 0.076

üéØ Scoring Prompt 3 completions:
  Gen 1: Reward= 0.630 | 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b...
  Gen 2: Reward= 0.630 | 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b...
  Gen 3: Reward= 0.630 | 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b...
  Gen 4: Reward= 0.630 | 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b...
  üìä Average reward: 0.630

üéØ Scoring Prompt 4 completions:
  Gen 1: Reward= 0.816 | M: d7d5 d7d6 e7e5 c6c5 g7g6 E: -0.38 -0....
  Gen 2: Reward= 0.713 | M: d7d5 d7d6 e7e5 g7g6 c6c5 E: -0.36 -0....
  Gen 3: Reward= 0.715 | M: d7d6 e7e6 e7e5 d7d5 g7g6 E: -0.65 -0....
  Gen 4: Reward= 0.606 | M: d7d6 e7e6 d7d5 e7e5 c6c5 E: -0.72 -0....
  üìä Average reward: 0.713

==================== PHASE 5: TRL EXACT ADVANTAGE CALCULATION (step 17) ====================
üî¢ TRL Advantage Calculation (exact formula):
  Total rewards shape: torch.Size([16])
  Rewards: [ 0.55        0.2466      0.52177143  0.275      -0.02142857  0.09857143
  0.09857143  0.12857143  0.63        0.63        0.63        0.63
  0.8164      0.7128      0.71497143  0.60617143]

==================== PHASE 6-8: CORRECTED GRPO LOSS CALCULATION (step 17) ====================
üîÑ Computing GRPO loss with proper token-level KL...

üéØ Prompt 1 - Corrected GRPO Loss:
  Gen 1: A=+0.950, logp/len=-7.031, kl=-5.281
         PG=6.688, KL_penalty=-0.527, total=6.156
  Gen 2: A=-0.950, logp/len=-5.312, kl=-3.344
         PG=-5.062, KL_penalty=-0.334, total=-5.406
  Gen 3: A=+0.773, logp/len=-5.719, kl=-3.859
         PG=4.406, KL_penalty=-0.387, total=4.031
  Gen 4: A=-0.773, logp/len=-6.469, kl=-4.625
         PG=-5.000, KL_penalty=-0.463, total=-5.469
  üìä Prompt averages: PG=0.258, KL=-0.428

üéØ Prompt 2 - Corrected GRPO Loss:
  Gen 1: A=-1.466, logp/len=-6.844, kl=-5.312
         PG=-10.000, KL_penalty=-0.531, total=-10.500
  Gen 2: A=+0.338, logp/len=-6.750, kl=-4.938
         PG=2.281, KL_penalty=-0.494, total=1.789
  Gen 3: A=+0.338, logp/len=-7.125, kl=-5.281
         PG=2.406, KL_penalty=-0.527, total=1.875
  Gen 4: A=+0.789, logp/len=-6.938, kl=-5.062
         PG=5.469, KL_penalty=-0.508, total=4.969
  üìä Prompt averages: PG=0.039, KL=-0.516

üéØ Prompt 3 - Corrected GRPO Loss:
  Gen 1: A=+0.000, logp/len=-5.594, kl=-5.250
         PG=0.000, KL_penalty=-0.523, total=-0.523
  Gen 2: A=+0.000, logp/len=-7.125, kl=-6.781
         PG=0.000, KL_penalty=-0.680, total=-0.680
  Gen 3: A=+0.000, logp/len=-6.156, kl=-5.812
         PG=0.000, KL_penalty=-0.582, total=-0.582
  Gen 4: A=+0.000, logp/len=-5.625, kl=-5.281
         PG=0.000, KL_penalty=-0.527, total=-0.527
  üìä Prompt averages: PG=0.000, KL=-0.578

üéØ Prompt 4 - Corrected GRPO Loss:
  Gen 1: A=+1.209, logp/len=-6.531, kl=-4.781
         PG=7.906, KL_penalty=-0.479, total=7.438
  Gen 2: A=+0.002, logp/len=-7.625, kl=-6.000
         PG=0.019, KL_penalty=-0.602, total=-0.582
  Gen 3: A=+0.028, logp/len=-7.562, kl=-5.938
         PG=0.210, KL_penalty=-0.594, total=-0.383
  Gen 4: A=-1.240, logp/len=-7.250, kl=-5.500
         PG=-9.000, KL_penalty=-0.551, total=-9.562
  üìä Prompt averages: PG=-0.219, KL=-0.555

üî¢ CORRECTED Loss Breakdown:
  Policy Gradient Loss:    0.020
  KL Loss (with gradients):   -0.520
  Total Loss:          =  -0.500
  KL penalty ratio: 96.5%

üîÑ Performing corrected gradient update (step 17)...
  Total gradient norm: 27.07
  ‚úÖ Gradient update applied with proper KL regularization

==================== PHASE 9: POST-UPDATE PERFORMANCE (step 17) ====================
  Prompt 1: avg reward = 0.155
  Prompt 2: avg reward = 0.387
  Prompt 3: avg reward = 0.630
  Prompt 4: avg reward = 0.505
üìä POST-UPDATE (step 17) Performance:
  Average reward: 0.4193
  Positive ratio: 100.0%

==================== PHASE 10: ANALYSIS (step 17) ====================
üîç GRPO Step Impact (step 17):
  Post-update performance: 0.4193
  Step performance change: -0.0124

======================================================================
üß≠ SEQUENTIAL STEP 18/50
======================================================================
üßæ Overfit mode: reusing initial batch (no dataset slice advance)

ü§ñ Generating completions for GRPO training (step 18)...

üéØ Prompt 1: P: 2k4r/ppp1q1p1/nbb1N3/4PP2/3p2P1/PB5P/1PPK3R/3RQ3 b - - 3 ...
    Gen 1: Train_LP= -528.0, Ref_LP=  -36.5
           Text:                                M: a6c5 c6d7 h8h4 e7h4 d4d3  ...
           Normalized: M: a6c5 c6d7 h8h4 e7h4 d4d3 E: -3.7 -4.12 -3.99 -3.87 -3.93 ...
    Gen 2: Train_LP= -672.0, Ref_LP=  -44.2
           Text:                                M: a6c5 c6d7 d4d3 c8b8 b6a5  ...
           Normalized: M: a6c5 c6d7 d4d3 c8b8 b6a5 E: -3.28 -3.54 -3.36 -3.41 -3.54...
    Gen 3: Train_LP= -426.0, Ref_LP=  -45.8
           Text:                                M: d4d3 e7e8 e7d7 a6c5 h8h3  ...
           Normalized: M: d4d3 e7e8 e7d7 a6c5 h8h3 E: -4.04 -4.38 -4.12 -3.75 -4.36...
    Gen 4: Train_LP= -376.0, Ref_LP=  -46.5
           Text:                                M: c8b8 h8h6 a6c5 c6d7 e7d7  ...
           Normalized: M: c8b8 h8h6 a6c5 c6d7 e7d7 E: -4.17 -4.07 -3.85 -4.21 -4.02...

üéØ Prompt 2: P: 2N5/2k5/3p1p2/3P1BbP/4Pp2/1n6/4K3/8 w - - 9 49...
    Gen 1: Train_LP= -444.0, Ref_LP=  -69.0
           Text:                                             M: e2d3 e2f1 c8e...
           Normalized: M: e2d3 e2f1 c8e7 e2f2 c8a7 E: 0.27 0.19 0.27 0.27 0.28 B: c...
    Gen 2: Train_LP= -808.0, Ref_LP=  -16.4
           Text:                                             M: e2d3 e2f2 c8e...
           Normalized: M: e2d3 e2f2 c8e7 e2f3 c8a7 E: 0.01 0.0 0.0 0.0 0.0 B: e2d3...
    Gen 3: Train_LP= -508.0, Ref_LP=  -69.0
           Text:                                             M: e2f2 c8e7 e2f...
           Normalized: M: e2f2 c8e7 e2f1 e2f3 c8a7 E: 0.31 0.47 0.33 0.32 0.31 B: c...
    Gen 4: Train_LP= -596.0, Ref_LP=  -67.5
           Text:                                             M: e2f2 e2d3 c8e...
           Normalized: M: e2f2 e2d3 c8e7 e2f3 c8a7 E: 0.33 0.34 0.34 0.34 0.34 B: e...

üéØ Prompt 3: A: 8/2k2b2/R1p4r/3p1p2/P7/2P2B1P/5PPK/4R3 w - - 1 31+a6a5+f7...
    Gen 1: Train_LP= -278.0, Ref_LP=   -0.0
           Text: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
           Normalized: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
    Gen 2: Train_LP= -250.0, Ref_LP=   -0.0
           Text: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
           Normalized: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
    Gen 3: Train_LP= -256.0, Ref_LP=   -0.0
           Text: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
           Normalized: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
    Gen 4: Train_LP= -384.0, Ref_LP=   -0.0
           Text: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
           Normalized: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...

üéØ Prompt 4: P: rnbqkbnr/pp1ppppp/2p5/8/4P3/2N5/PPPP1PPP/R1BQKBNR b KQkq ...
    Gen 1: Train_LP= -308.0, Ref_LP=  -23.5
           Text:                             M: d7d6 g7g6 d7d5 e7e6 e7e5     ...
           Normalized: M: d7d6 g7g6 d7d5 e7e6 e7e5 E: -0.59 -0.7 -0.42 -0.77 -0.45 ...
    Gen 2: Train_LP= -350.0, Ref_LP=  -36.2
           Text:                             M: d7d5 e7e5 d7d6 g7g6 d8c7     ...
           Normalized: M: d7d5 e7e5 d7d6 g7g6 d8c7 E: -0.37 -0.63 -0.72 -0.82 -0.85...
    Gen 3: Train_LP= -544.0, Ref_LP=  -37.0
           Text:                             M: d7d6 d7d5 e7e5 c6c5 d8c7     ...
           Normalized: M: d7d6 d7d5 e7e5 c6c5 d8c7 E: -0.72 -0.42 -0.65 -0.73 -0.87...
    Gen 4: Train_LP= -580.0, Ref_LP=  -35.8
           Text:                             M: e7e5 d7d5 d7d6 e7e6 g7g6     ...
           Normalized: M: e7e5 d7d5 d7d6 e7e6 g7g6 E: -0.71 -0.56 -0.57 -0.71 -0.78...

==================== PHASE 4: REWARD CALCULATION (step 18) ====================

üéØ Scoring Prompt 1 completions:
  Gen 1: Reward= 0.125 | M: a6c5 c6d7 h8h4 e7h4 d4d3 E: -3.7 -4.1...
  Gen 2: Reward= 0.700 | M: a6c5 c6d7 d4d3 c8b8 b6a5 E: -3.28 -3....
  Gen 3: Reward= 0.157 | M: d4d3 e7e8 e7d7 a6c5 h8h3 E: -4.04 -4....
  Gen 4: Reward= 0.230 | M: c8b8 h8h6 a6c5 c6d7 e7d7 E: -4.17 -4....
  üìä Average reward: 0.303

üéØ Scoring Prompt 2 completions:
  Gen 1: Reward= 0.490 | M: e2d3 e2f1 c8e7 e2f2 c8a7 E: 0.27 0.19...
  Gen 2: Reward= 0.039 | M: e2d3 e2f2 c8e7 e2f3 c8a7 E: 0.01 0.0 ...
  Gen 3: Reward= 0.469 | M: e2f2 c8e7 e2f1 e2f3 c8a7 E: 0.31 0.47...
  Gen 4: Reward= 0.099 | M: e2f2 e2d3 c8e7 e2f3 c8a7 E: 0.33 0.34...
  üìä Average reward: 0.274

üéØ Scoring Prompt 3 completions:
  Gen 1: Reward= 0.630 | 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b...
  Gen 2: Reward= 0.630 | 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b...
  Gen 3: Reward= 0.630 | 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b...
  Gen 4: Reward= 0.630 | 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b...
  üìä Average reward: 0.630

üéØ Scoring Prompt 4 completions:
  Gen 1: Reward= 0.388 | M: d7d6 g7g6 d7d5 e7e6 e7e5 E: -0.59 -0....
  Gen 2: Reward= 0.393 | M: d7d5 e7e5 d7d6 g7g6 d8c7 E: -0.37 -0....
  Gen 3: Reward= 0.362 | M: d7d6 d7d5 e7e5 c6c5 d8c7 E: -0.72 -0....
  Gen 4: Reward= 0.713 | M: e7e5 d7d5 d7d6 e7e6 g7g6 E: -0.71 -0....
  üìä Average reward: 0.464

==================== PHASE 5: TRL EXACT ADVANTAGE CALCULATION (step 18) ====================
üî¢ TRL Advantage Calculation (exact formula):
  Total rewards shape: torch.Size([16])
  Rewards: [0.125      0.7        0.157      0.2298     0.49017143 0.03857143
 0.46857143 0.09857143 0.63       0.63       0.63       0.63
 0.38817143 0.39297143 0.36217143 0.71337143]

==================== PHASE 6-8: CORRECTED GRPO LOSS CALCULATION (step 18) ====================
üîÑ Computing GRPO loss with proper token-level KL...

üéØ Prompt 1 - Corrected GRPO Loss:
  Gen 1: A=-0.663, logp/len=-7.188, kl=-5.438
         PG=-4.781, KL_penalty=-0.543, total=-5.312
  Gen 2: A=+1.480, logp/len=-7.469, kl=-5.781
         PG=11.062, KL_penalty=-0.578, total=10.500
  Gen 3: A=-0.544, logp/len=-6.812, kl=-5.000
         PG=-3.703, KL_penalty=-0.500, total=-4.188
  Gen 4: A=-0.273, logp/len=-8.750, kl=-6.719
         PG=-2.391, KL_penalty=-0.672, total=-3.062
  üìä Prompt averages: PG=0.047, KL=-0.574

üéØ Prompt 2 - Corrected GRPO Loss:
  Gen 1: A=+0.906, logp/len=-8.062, kl=-6.281
         PG=7.312, KL_penalty=-0.629, total=6.688
  Gen 2: A=-0.987, logp/len=-7.656, kl=-5.969
         PG=-7.562, KL_penalty=-0.598, total=-8.188
  Gen 3: A=+0.816, logp/len=-5.906, kl=-4.062
         PG=4.812, KL_penalty=-0.406, total=4.406
  Gen 4: A=-0.735, logp/len=-7.125, kl=-5.312
         PG=-5.250, KL_penalty=-0.531, total=-5.781
  üìä Prompt averages: PG=-0.172, KL=-0.539

üéØ Prompt 3 - Corrected GRPO Loss:
  Gen 1: A=+0.000, logp/len=-5.812, kl=-5.438
         PG=0.000, KL_penalty=-0.543, total=-0.543
  Gen 2: A=+0.000, logp/len=-6.750, kl=-6.375
         PG=0.000, KL_penalty=-0.637, total=-0.637
  Gen 3: A=+0.000, logp/len=-5.281, kl=-4.938
         PG=0.000, KL_penalty=-0.494, total=-0.494
  Gen 4: A=+0.000, logp/len=-5.688, kl=-5.312
         PG=0.000, KL_penalty=-0.531, total=-0.531
  üìä Prompt averages: PG=0.000, KL=-0.551

üéØ Prompt 4 - Corrected GRPO Loss:
  Gen 1: A=-0.456, logp/len=-7.250, kl=-5.562
         PG=-3.312, KL_penalty=-0.555, total=-3.875
  Gen 2: A=-0.427, logp/len=-6.250, kl=-4.688
         PG=-2.672, KL_penalty=-0.469, total=-3.141
  Gen 3: A=-0.612, logp/len=-7.812, kl=-6.188
         PG=-4.781, KL_penalty=-0.617, total=-5.406
  Gen 4: A=+1.495, logp/len=-7.094, kl=-5.406
         PG=10.625, KL_penalty=-0.539, total=10.062
  üìä Prompt averages: PG=-0.031, KL=-0.547

üî¢ CORRECTED Loss Breakdown:
  Policy Gradient Loss:   -0.039
  KL Loss (with gradients):   -0.551
  Total Loss:          =  -0.590
  KL penalty ratio: 93.5%

üîÑ Performing corrected gradient update (step 18)...
  Total gradient norm: 20.74
  ‚úÖ Gradient update applied with proper KL regularization

==================== PHASE 9: POST-UPDATE PERFORMANCE (step 18) ====================
  Prompt 1: avg reward = 0.150
  Prompt 2: avg reward = 0.387
  Prompt 3: avg reward = 0.630
  Prompt 4: avg reward = 0.559
üìä POST-UPDATE (step 18) Performance:
  Average reward: 0.4317
  Positive ratio: 100.0%

==================== PHASE 10: ANALYSIS (step 18) ====================
üîç GRPO Step Impact (step 18):
  Post-update performance: 0.4317
  Step performance change: +0.0124

======================================================================
üß≠ SEQUENTIAL STEP 19/50
======================================================================
üßæ Overfit mode: reusing initial batch (no dataset slice advance)

ü§ñ Generating completions for GRPO training (step 19)...

üéØ Prompt 1: P: 2k4r/ppp1q1p1/nbb1N3/4PP2/3p2P1/PB5P/1PPK3R/3RQ3 b - - 3 ...
    Gen 1: Train_LP= -466.0, Ref_LP=  -33.0
           Text:                                M: a6c5 c8b8 d4d3 c6f3 e7d7  ...
           Normalized: M: a6c5 c8b8 d4d3 c6f3 e7d7 E: -3.39 -3.65 -3.53 -3.67 -3.64...
    Gen 2: Train_LP= -454.0, Ref_LP=  -34.2
           Text:                                M: a6c5 c8b8 e7e8 e7d7 c6f3  ...
           Normalized: M: a6c5 c8b8 e7e8 e7d7 c6f3 E: -3.69 -3.72 -3.65 -3.82 -3.69...
    Gen 3: Train_LP= -580.0, Ref_LP=  -33.8
           Text:                                M: h8h4 c8b8 a6c5 d4d3 e7h4  ...
           Normalized: M: h8h4 c8b8 a6c5 d4d3 e7h4 E: -4.17 -4.04 -3.74 -3.78 -4.16...
    Gen 4: Train_LP= -344.0, Ref_LP=  -35.2
           Text:                                M: h8h4 d4d3 a6c5 e7h4 h8e8  ...
           Normalized: M: h8h4 d4d3 a6c5 e7h4 h8e8 E: -4.29 -4.37 -4.17 -4.41 -4.33...

üéØ Prompt 2: P: 2N5/2k5/3p1p2/3P1BbP/4Pp2/1n6/4K3/8 w - - 9 49...
    Gen 1: Train_LP= -386.0, Ref_LP=  -16.5
           Text:                                             M: e2f2 e2f3 f5g...
           Normalized: M: e2f2 e2f3 f5g6 c8e7 f5h7 E: 0.0 0.0 0.0 0.0 0.0 B: e2f2...
    Gen 2: Train_LP= -304.0, Ref_LP=  -78.0
           Text:                                             M: e2f3 e2f2 c8e...
           Normalized: M: e2f3 e2f2 c8e7 e2f1 e2e1 E: 0.17 0.17 0.15 0.15 0.15 B: e...
    Gen 3: Train_LP= -498.0, Ref_LP=  -57.0
           Text:                                             M: e2d3 c8a7 e2f...
           Normalized: M: e2d3 c8a7 e2f3 c8e7 e2f2 E: 0.3 0.28 0.31 0.31 0.3 B: c8e...
    Gen 4: Train_LP= -330.0, Ref_LP=  -79.5
           Text:                                             M: e2d3 f5h3 c8a...
           Normalized: M: e2d3 f5h3 c8a7 c8e7 f5g4 E: 0.34 0.34 0.34 0.36 0.34 B: c...

üéØ Prompt 3: A: 8/2k2b2/R1p4r/3p1p2/P7/2P2B1P/5PPK/4R3 w - - 1 31+a6a5+f7...
    Gen 1: Train_LP= -314.0, Ref_LP=   -0.0
           Text: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
           Normalized: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
    Gen 2: Train_LP= -370.0, Ref_LP=   -0.0
           Text: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
           Normalized: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
    Gen 3: Train_LP= -376.0, Ref_LP=   -0.0
           Text: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
           Normalized: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
    Gen 4: Train_LP= -310.0, Ref_LP=   -0.0
           Text: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
           Normalized: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...

üéØ Prompt 4: P: rnbqkbnr/pp1ppppp/2p5/8/4P3/2N5/PPPP1PPP/R1BQKBNR b KQkq ...
    Gen 1: Train_LP= -592.0, Ref_LP=  -35.0
           Text:                             M: e7e6 d7d6 g7g6 e7e5 d7d5     ...
           Normalized: M: e7e6 d7d6 g7g6 e7e5 d7d5 E: -0.79 -0.68 -0.67 -0.63 -0.35...
    Gen 2: Train_LP= -536.0, Ref_LP=  -35.5
           Text:                             M: d7d5 e7e5 d7d6 g7g6 e7e6     ...
           Normalized: M: d7d5 e7e5 d7d6 g7g6 e7e6 E: -0.36 -0.56 -0.59 -0.59 -0.85...
    Gen 3: Train_LP= -640.0, Ref_LP=  -24.1
           Text:                             M: d7d6 d7d5 e7e5 c6c5 g7g6     ...
           Normalized: M: d7d6 d7d5 e7e5 c6c5 g7g6 E: -0.6 -0.37 -0.57 -0.78 -0.71 ...
    Gen 4: Train_LP= -448.0, Ref_LP=  -35.0
           Text:                             M: d7d5 g7g6 e7e5 d7d6 e7e6     ...
           Normalized: M: d7d5 g7g6 e7e5 d7d6 e7e6 E: -0.57 -0.75 -0.57 -0.73 -0.83...

==================== PHASE 4: REWARD CALCULATION (step 19) ====================

üéØ Scoring Prompt 1 completions:
  Gen 1: Reward= 0.586 | M: a6c5 c8b8 d4d3 c6f3 e7d7 E: -3.39 -3....
  Gen 2: Reward= 0.255 | M: a6c5 c8b8 e7e8 e7d7 c6f3 E: -3.69 -3....
  Gen 3: Reward= 0.214 | M: h8h4 c8b8 a6c5 d4d3 e7h4 E: -4.17 -4....
  Gen 4: Reward= 0.161 | M: h8h4 d4d3 a6c5 e7h4 h8e8 E: -4.29 -4....
  üìä Average reward: 0.304

üéØ Scoring Prompt 2 completions:
  Gen 1: Reward= 0.275 | M: e2f2 e2f3 f5g6 c8e7 f5h7 E: 0.0 0.0 0...
  Gen 2: Reward= 0.335 | M: e2f3 e2f2 c8e7 e2f1 e2e1 E: 0.17 0.17...
  Gen 3: Reward= 0.549 | M: e2d3 c8a7 e2f3 c8e7 e2f2 E: 0.3 0.28 ...
  Gen 4: Reward= 0.425 | M: e2d3 f5h3 c8a7 c8e7 f5g4 E: 0.34 0.34...
  üìä Average reward: 0.396

üéØ Scoring Prompt 3 completions:
  Gen 1: Reward= 0.630 | 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b...
  Gen 2: Reward= 0.630 | 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b...
  Gen 3: Reward= 0.630 | 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b...
  Gen 4: Reward= 0.630 | 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b...
  üìä Average reward: 0.630

üéØ Scoring Prompt 4 completions:
  Gen 1: Reward= 0.362 | M: e7e6 d7d6 g7g6 e7e5 d7d5 E: -0.79 -0....
  Gen 2: Reward= 0.393 | M: d7d5 e7e5 d7d6 g7g6 e7e6 E: -0.36 -0....
  Gen 3: Reward= 0.687 | M: d7d6 d7d5 e7e5 c6c5 g7g6 E: -0.6 -0.3...
  Gen 4: Reward= 0.413 | M: d7d5 g7g6 e7e5 d7d6 e7e6 E: -0.57 -0....
  üìä Average reward: 0.464

==================== PHASE 5: TRL EXACT ADVANTAGE CALCULATION (step 19) ====================
üî¢ TRL Advantage Calculation (exact formula):
  Total rewards shape: torch.Size([16])
  Rewards: [0.5864     0.25457143 0.21377143 0.161      0.275      0.335
 0.54857143 0.425      0.63       0.63       0.63       0.63
 0.36217143 0.39337143 0.6868     0.41257143]

==================== PHASE 6-8: CORRECTED GRPO LOSS CALCULATION (step 19) ====================
üîÑ Computing GRPO loss with proper token-level KL...

üéØ Prompt 1 - Corrected GRPO Loss:
  Gen 1: A=+1.470, logp/len=-6.031, kl=-4.219
         PG=8.875, KL_penalty=-0.422, total=8.438
  Gen 2: A=-0.257, logp/len=-8.125, kl=-6.281
         PG=-2.094, KL_penalty=-0.629, total=-2.719
  Gen 3: A=-0.469, logp/len=-7.125, kl=-5.281
         PG=-3.344, KL_penalty=-0.527, total=-3.875
  Gen 4: A=-0.744, logp/len=-7.312, kl=-5.625
         PG=-5.438, KL_penalty=-0.562, total=-6.000
  üìä Prompt averages: PG=-0.500, KL=-0.535

üéØ Prompt 2 - Corrected GRPO Loss:
  Gen 1: A=-1.016, logp/len=-6.562, kl=-5.000
         PG=-6.656, KL_penalty=-0.500, total=-7.156
  Gen 2: A=-0.512, logp/len=-5.719, kl=-3.734
         PG=-2.922, KL_penalty=-0.373, total=-3.297
  Gen 3: A=+1.283, logp/len=-6.750, kl=-4.719
         PG=8.688, KL_penalty=-0.473, total=8.188
  Gen 4: A=+0.245, logp/len=-6.344, kl=-4.500
         PG=1.555, KL_penalty=-0.449, total=1.109
  üìä Prompt averages: PG=0.170, KL=-0.449

üéØ Prompt 3 - Corrected GRPO Loss:
  Gen 1: A=+0.000, logp/len=-5.094, kl=-4.719
         PG=0.000, KL_penalty=-0.473, total=-0.473
  Gen 2: A=+0.000, logp/len=-5.094, kl=-4.719
         PG=0.000, KL_penalty=-0.473, total=-0.473
  Gen 3: A=+0.000, logp/len=-5.250, kl=-4.875
         PG=0.000, KL_penalty=-0.488, total=-0.488
  Gen 4: A=+0.000, logp/len=-5.250, kl=-4.875
         PG=0.000, KL_penalty=-0.488, total=-0.488
  üìä Prompt averages: PG=0.000, KL=-0.480

üéØ Prompt 4 - Corrected GRPO Loss:
  Gen 1: A=-0.676, logp/len=-8.750, kl=-7.062
         PG=-5.906, KL_penalty=-0.707, total=-6.625
  Gen 2: A=-0.469, logp/len=-6.125, kl=-4.531
         PG=-2.875, KL_penalty=-0.453, total=-3.328
  Gen 3: A=+1.486, logp/len=-8.625, kl=-7.031
         PG=12.812, KL_penalty=-0.703, total=12.125
  Gen 4: A=-0.341, logp/len=-6.688, kl=-4.969
         PG=-2.281, KL_penalty=-0.496, total=-2.781
  üìä Prompt averages: PG=0.445, KL=-0.590

üî¢ CORRECTED Loss Breakdown:
  Policy Gradient Loss:    0.029
  KL Loss (with gradients):   -0.516
  Total Loss:          =  -0.486
  KL penalty ratio: 95.0%

üîÑ Performing corrected gradient update (step 19)...
  Total gradient norm: 23.66
  ‚úÖ Gradient update applied with proper KL regularization

==================== PHASE 9: POST-UPDATE PERFORMANCE (step 19) ====================
  Prompt 1: avg reward = 0.155
  Prompt 2: avg reward = 0.387
  Prompt 3: avg reward = 0.630
  Prompt 4: avg reward = 0.505
üìä POST-UPDATE (step 19) Performance:
  Average reward: 0.4193
  Positive ratio: 100.0%

==================== PHASE 10: ANALYSIS (step 19) ====================
üîç GRPO Step Impact (step 19):
  Post-update performance: 0.4193
  Step performance change: -0.0124

======================================================================
üß≠ SEQUENTIAL STEP 20/50
======================================================================
üßæ Overfit mode: reusing initial batch (no dataset slice advance)

ü§ñ Generating completions for GRPO training (step 20)...

üéØ Prompt 1: P: 2k4r/ppp1q1p1/nbb1N3/4PP2/3p2P1/PB5P/1PPK3R/3RQ3 b - - 3 ...
    Gen 1: Train_LP= -496.0, Ref_LP=  -46.2
           Text:                                M: c6f3 a6c5 e7h4 d4d3 c6d7  ...
           Normalized: M: c6f3 a6c5 e7h4 d4d3 c6d7 E: -3.76 -3.82 -4.21 -3.91 -4.12...
    Gen 2: Train_LP= -446.0, Ref_LP=  -35.0
           Text:                                M: a6c5 c6d7 c6b5 e7e8 c8b8  ...
           Normalized: M: a6c5 c6d7 c6b5 e7e8 c8b8 E: -3.57 -4.05 -4.11 -4.24 -4.1 ...
    Gen 3: Train_LP= -520.0, Ref_LP=  -45.2
           Text:                                M: d4d3 c6f3 a6c5 c6d7 h8e8  ...
           Normalized: M: d4d3 c6f3 a6c5 c6d7 h8e8 E: -3.67 -3.96 -3.67 -3.72 -3.92...
    Gen 4: Train_LP= -446.0, Ref_LP=  -44.8
           Text:                                M: a6c5 c6d7 e7d7 d4d3 h8e8  ...
           Normalized: M: a6c5 c6d7 e7d7 d4d3 h8e8 E: -3.46 -3.58 -3.57 -3.61 -3.46...

üéØ Prompt 2: P: 2N5/2k5/3p1p2/3P1BbP/4Pp2/1n6/4K3/8 w - - 9 49...
    Gen 1: Train_LP= -568.0, Ref_LP=  -43.8
           Text:                                             M: e2f2 e2f3 c8e...
           Normalized: M: e2f2 e2f3 c8e7 e2f1 c8a7 E: 0.16 0.17 0.16 0.16 0.17 B: e...
    Gen 2: Train_LP= -660.0, Ref_LP=  -48.2
           Text:                                             M: f5g6 e2d3 e2f...
           Normalized: M: f5g6 e2d3 e2f3 c8e7 e2f2 E: 0.12 0.09 0.19 0.11 0.11 B: e...
    Gen 3: Train_LP= -548.0, Ref_LP=  -47.2
           Text:                                             M: e2d3 e2f1 e2f...
           Normalized: M: e2d3 e2f1 e2f2 c8e7 e2f3 E: 0.21 0.16 0.18 0.22 0.22 B: e...
    Gen 4: Train_LP= -388.0, Ref_LP=  -28.4
           Text:                                             M: f5g4 c8a7 e2f...
           Normalized: M: f5g4 c8a7 e2f3 e2f2 c8e7 E: 0.28 0.35 0.4 0.31 0.4 B: e2f...

üéØ Prompt 3: A: 8/2k2b2/R1p4r/3p1p2/P7/2P2B1P/5PPK/4R3 w - - 1 31+a6a5+f7...
    Gen 1: Train_LP= -274.0, Ref_LP=   -0.0
           Text: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
           Normalized: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
    Gen 2: Train_LP= -362.0, Ref_LP=   -0.0
           Text: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
           Normalized: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
    Gen 3: Train_LP= -290.0, Ref_LP=   -0.0
           Text: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
           Normalized: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
    Gen 4: Train_LP= -272.0, Ref_LP=   -0.0
           Text: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
           Normalized: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...

üéØ Prompt 4: P: rnbqkbnr/pp1ppppp/2p5/8/4P3/2N5/PPPP1PPP/R1BQKBNR b KQkq ...
    Gen 1: Train_LP= -502.0, Ref_LP=  -47.2
           Text:                             M: d7d5 d7d6 g7g6 e7e5 d8c7     ...
           Normalized: M: d7d5 d7d6 g7g6 e7e5 d8c7 E: -0.32 -0.74 -0.84 -0.56 -0.76...
    Gen 2: Train_LP= -322.0, Ref_LP=  -46.5
           Text:                             M: d7d5 d7d6 e7e5 c6c5 e7e6     ...
           Normalized: M: d7d5 d7d6 e7e5 c6c5 e7e6 E: -0.57 -0.57 -0.57 -0.64 -0.79...
    Gen 3: Train_LP= -408.0, Ref_LP=  -24.9
           Text:                             M: d7d5 d7d6 e7e5 g7g6 c6c5     ...
           Normalized: M: d7d5 d7d6 e7e5 g7g6 c6c5 E: -0.4 -0.51 -0.5 -0.77 -0.76 B...
    Gen 4: Train_LP= -468.0, Ref_LP=  -34.8
           Text:                             M: d7d6 d7d5 e7e5 e7e6 g7g6     ...
           Normalized: M: d7d6 d7d5 e7e5 e7e6 g7g6 E: -0.64 -0.34 -0.6 -0.87 -0.82 ...

==================== PHASE 4: REWARD CALCULATION (step 20) ====================

üéØ Scoring Prompt 1 completions:
  Gen 1: Reward= 0.296 | M: c6f3 a6c5 e7h4 d4d3 c6d7 E: -3.76 -3....
  Gen 2: Reward= 0.224 | M: a6c5 c6d7 c6b5 e7e8 c8b8 E: -3.57 -4....
  Gen 3: Reward= 0.497 | M: d4d3 c6f3 a6c5 c6d7 h8e8 E: -3.67 -3....
  Gen 4: Reward= 0.175 | M: a6c5 c6d7 e7d7 d4d3 h8e8 E: -3.46 -3....
  üìä Average reward: 0.298

üéØ Scoring Prompt 2 completions:
  Gen 1: Reward= 0.019 | M: e2f2 e2f3 c8e7 e2f1 c8a7 E: 0.16 0.17...
  Gen 2: Reward=-0.035 | M: f5g6 e2d3 e2f3 c8e7 e2f2 E: 0.12 0.09...
  Gen 3: Reward=-0.035 | M: e2d3 e2f1 e2f2 c8e7 e2f3 E: 0.21 0.16...
  Gen 4: Reward= 0.019 | M: f5g4 c8a7 e2f3 e2f2 c8e7 E: 0.28 0.35...
  üìä Average reward: -0.008

üéØ Scoring Prompt 3 completions:
  Gen 1: Reward= 0.630 | 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b...
  Gen 2: Reward= 0.630 | 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b...
  Gen 3: Reward= 0.630 | 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b...
  Gen 4: Reward= 0.630 | 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b...
  üìä Average reward: 0.630

üéØ Scoring Prompt 4 completions:
  Gen 1: Reward= 0.741 | M: d7d5 d7d6 g7g6 e7e5 d8c7 E: -0.32 -0....
  Gen 2: Reward= 0.401 | M: d7d5 d7d6 e7e5 c6c5 e7e6 E: -0.57 -0....
  Gen 3: Reward= 0.820 | M: d7d5 d7d6 e7e5 g7g6 c6c5 E: -0.4 -0.5...
  Gen 4: Reward= 0.364 | M: d7d6 d7d5 e7e5 e7e6 g7g6 E: -0.64 -0....
  üìä Average reward: 0.582

==================== PHASE 5: TRL EXACT ADVANTAGE CALCULATION (step 20) ====================
üî¢ TRL Advantage Calculation (exact formula):
  Total rewards shape: torch.Size([16])
  Rewards: [ 0.29617143  0.2242      0.49697143  0.1754      0.01857143 -0.035
 -0.035       0.01857143  0.63        0.63        0.63        0.63
  0.74097143  0.40097143  0.8204      0.36417143]

==================== PHASE 6-8: CORRECTED GRPO LOSS CALCULATION (step 20) ====================
üîÑ Computing GRPO loss with proper token-level KL...

üéØ Prompt 1 - Corrected GRPO Loss:
  Gen 1: A=-0.014, logp/len=-7.625, kl=-5.812
         PG=-0.108, KL_penalty=-0.582, total=-0.691
  Gen 2: A=-0.523, logp/len=-6.312, kl=-4.500
         PG=-3.297, KL_penalty=-0.449, total=-3.750
  Gen 3: A=+1.405, logp/len=-5.625, kl=-3.812
         PG=7.906, KL_penalty=-0.381, total=7.531
  Gen 4: A=-0.868, logp/len=-7.750, kl=-6.031
         PG=-6.719, KL_penalty=-0.602, total=-7.312
  üìä Prompt averages: PG=-0.555, KL=-0.504

üéØ Prompt 2 - Corrected GRPO Loss:
  Gen 1: A=+0.866, logp/len=-7.375, kl=-5.625
         PG=6.375, KL_penalty=-0.562, total=5.812
  Gen 2: A=-0.866, logp/len=-6.719, kl=-4.594
         PG=-5.812, KL_penalty=-0.459, total=-6.281
  Gen 3: A=-0.866, logp/len=-6.312, kl=-4.344
         PG=-5.469, KL_penalty=-0.434, total=-5.906
  Gen 4: A=+0.866, logp/len=-7.656, kl=-5.719
         PG=6.625, KL_penalty=-0.570, total=6.062
  üìä Prompt averages: PG=0.430, KL=-0.508

üéØ Prompt 3 - Corrected GRPO Loss:
  Gen 1: A=+0.000, logp/len=-5.562, kl=-5.219
         PG=0.000, KL_penalty=-0.523, total=-0.523
  Gen 2: A=+0.000, logp/len=-6.281, kl=-5.906
         PG=0.000, KL_penalty=-0.590, total=-0.590
  Gen 3: A=+0.000, logp/len=-6.406, kl=-6.031
         PG=0.000, KL_penalty=-0.602, total=-0.602
  Gen 4: A=+0.000, logp/len=-5.875, kl=-5.531
         PG=0.000, KL_penalty=-0.555, total=-0.555
  üìä Prompt averages: PG=0.000, KL=-0.566

üéØ Prompt 4 - Corrected GRPO Loss:
  Gen 1: A=+0.685, logp/len=-7.594, kl=-5.906
         PG=5.188, KL_penalty=-0.590, total=4.594
  Gen 2: A=-0.777, logp/len=-8.062, kl=-6.312
         PG=-6.250, KL_penalty=-0.633, total=-6.875
  Gen 3: A=+1.026, logp/len=-5.531, kl=-3.844
         PG=5.688, KL_penalty=-0.385, total=5.312
  Gen 4: A=-0.935, logp/len=-8.438, kl=-6.844
         PG=-7.875, KL_penalty=-0.684, total=-8.562
  üìä Prompt averages: PG=-0.812, KL=-0.570

üî¢ CORRECTED Loss Breakdown:
  Policy Gradient Loss:   -0.234
  KL Loss (with gradients):   -0.539
  Total Loss:          =  -0.773
  KL penalty ratio: 69.5%

üîÑ Performing corrected gradient update (step 20)...
  Total gradient norm: 18.92
  ‚úÖ Gradient update applied with proper KL regularization

==================== PHASE 9: POST-UPDATE PERFORMANCE (step 20) ====================
  Prompt 1: avg reward = 0.155
  Prompt 2: avg reward = 0.387
  Prompt 3: avg reward = 0.630
  Prompt 4: avg reward = 0.559
üìä POST-UPDATE (step 20) Performance:
  Average reward: 0.4329
  Positive ratio: 100.0%

==================== PHASE 10: ANALYSIS (step 20) ====================
üîç GRPO Step Impact (step 20):
  Post-update performance: 0.4329
  Step performance change: +0.0136

======================================================================
üß≠ SEQUENTIAL STEP 21/50
======================================================================
üßæ Overfit mode: reusing initial batch (no dataset slice advance)

ü§ñ Generating completions for GRPO training (step 21)...

üéØ Prompt 1: P: 2k4r/ppp1q1p1/nbb1N3/4PP2/3p2P1/PB5P/1PPK3R/3RQ3 b - - 3 ...
    Gen 1: Train_LP= -704.0, Ref_LP=  -47.0
           Text:                                M: c6d7 e7d7 a6c5 h8h6 c8b8  ...
           Normalized: M: c6d7 e7d7 a6c5 h8h6 c8b8 E: -3.61 -3.71 -3.22 -3.85 -3.51...
    Gen 2: Train_LP= -544.0, Ref_LP=  -34.0
           Text:                                M: a6c5 e7d7 h8h4 c8b8 h8e8  ...
           Normalized: M: a6c5 e7d7 h8h4 c8b8 h8e8 E: -3.52 -3.98 -4.02 -4.0 -3.98 ...
    Gen 3: Train_LP= -458.0, Ref_LP=  -46.5
           Text:                                M: d4d3 a6c5 h8h3 c6d7 c6f3  ...
           Normalized: M: d4d3 a6c5 h8h3 c6d7 c6f3 E: -3.71 -3.61 -3.72 -3.82 -3.62...
    Gen 4: Train_LP= -516.0, Ref_LP=  -32.8
           Text:                                M: a6c5 b6a5 c8b8 e7d7 d4d3  ...
           Normalized: M: a6c5 b6a5 c8b8 e7d7 d4d3 E: -3.59 -3.86 -3.77 -3.91 -3.6 ...

üéØ Prompt 2: P: 2N5/2k5/3p1p2/3P1BbP/4Pp2/1n6/4K3/8 w - - 9 49...
    Gen 1: Train_LP= -420.0, Ref_LP=  -79.5
           Text:                                             M: e2f2 e2d3 c8e...
           Normalized: M: e2f2 e2d3 c8e7 c8a7 e2f3 E: 0.22 0.22 0.28 0.26 0.27 B: c...
    Gen 2: Train_LP= -476.0, Ref_LP=  -17.2
           Text:                                             M: f5g6 e2d3 e2f...
           Normalized: M: f5g6 e2d3 e2f3 f5g4 e2f2 E: 0.0 0.0 0.0 0.0 0.0 B: f5g6...
    Gen 3: Train_LP= -720.0, Ref_LP=  -78.5
           Text:                                             M: e2f2 e2f3 e2f...
           Normalized: M: e2f2 e2f3 e2f1 c8e7 c8a7 E: 0.29 0.29 0.32 0.32 0.29 B: e...
    Gen 4: Train_LP= -624.0, Ref_LP=  -83.0
           Text:                                             M: e2f2 e2f3 e2d...
           Normalized: M: e2f2 e2f3 e2d3 c8a7 c8e7 E: 0.36 0.47 0.48 0.46 0.54 B: c...

üéØ Prompt 3: A: 8/2k2b2/R1p4r/3p1p2/P7/2P2B1P/5PPK/4R3 w - - 1 31+a6a5+f7...
    Gen 1: Train_LP= -312.0, Ref_LP=   -0.0
           Text: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
           Normalized: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
    Gen 2: Train_LP= -304.0, Ref_LP=   -0.0
           Text: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
           Normalized: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
    Gen 3: Train_LP= -286.0, Ref_LP=   -0.0
           Text: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
           Normalized: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
    Gen 4: Train_LP= -364.0, Ref_LP=   -0.0
           Text: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
           Normalized: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...

üéØ Prompt 4: P: rnbqkbnr/pp1ppppp/2p5/8/4P3/2N5/PPPP1PPP/R1BQKBNR b KQkq ...
    Gen 1: Train_LP= -462.0, Ref_LP=  -34.8
           Text:                             M: e7e6 d7d5 d7d6 g7g6 e7e5     ...
           Normalized: M: e7e6 d7d5 d7d6 g7g6 e7e5 E: -0.93 -0.51 -0.7 -0.81 -0.57 ...
    Gen 2: Train_LP= -532.0, Ref_LP=  -34.5
           Text:                             M: e7e5 g7g6 d7d5 d7d6 e7e6     ...
           Normalized: M: e7e5 g7g6 d7d5 d7d6 e7e6 E: -0.58 -0.7 -0.46 -0.64 -0.85 ...
    Gen 3: Train_LP= -358.0, Ref_LP=  -24.0
           Text:                             M: d7d6 d7d5 e7e5 g7g6 e7e6     ...
           Normalized: M: d7d6 d7d5 e7e5 g7g6 e7e6 E: -0.57 -0.5 -0.53 -0.7 -0.92 B...
    Gen 4: Train_LP= -480.0, Ref_LP=  -45.0
           Text:                             M: d7d5 g7g6 d7d6 e7e5 e7e6     ...
           Normalized: M: d7d5 g7g6 d7d6 e7e5 e7e6 E: -0.45 -0.69 -0.69 -0.63 -0.79...

==================== PHASE 4: REWARD CALCULATION (step 21) ====================

üéØ Scoring Prompt 1 completions:
  Gen 1: Reward= 0.140 | M: c6d7 e7d7 a6c5 h8h6 c8b8 E: -3.61 -3....
  Gen 2: Reward= 0.142 | M: a6c5 e7d7 h8h4 c8b8 h8e8 E: -3.52 -3....
  Gen 3: Reward= 0.199 | M: d4d3 a6c5 h8h3 c6d7 c6f3 E: -3.71 -3....
  Gen 4: Reward= 0.300 | M: a6c5 b6a5 c8b8 e7d7 d4d3 E: -3.59 -3....
  üìä Average reward: 0.195

üéØ Scoring Prompt 2 completions:
  Gen 1: Reward= 0.549 | M: e2f2 e2d3 c8e7 c8a7 e2f3 E: 0.22 0.22...
  Gen 2: Reward=-0.117 | M: f5g6 e2d3 e2f3 f5g4 e2f2 E: 0.0 0.0 0...
  Gen 3: Reward= 0.019 | M: e2f2 e2f3 e2f1 c8e7 c8a7 E: 0.29 0.29...
  Gen 4: Reward= 0.549 | M: e2f2 e2f3 e2d3 c8a7 c8e7 E: 0.36 0.47...
  üìä Average reward: 0.250

üéØ Scoring Prompt 3 completions:
  Gen 1: Reward= 0.630 | 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b...
  Gen 2: Reward= 0.630 | 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b...
  Gen 3: Reward= 0.630 | 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b...
  Gen 4: Reward= 0.630 | 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b...
  üìä Average reward: 0.630

üéØ Scoring Prompt 4 completions:
  Gen 1: Reward= 0.362 | M: e7e6 d7d5 d7d6 g7g6 e7e5 E: -0.93 -0....
  Gen 2: Reward= 0.387 | M: e7e5 g7g6 d7d5 d7d6 e7e6 E: -0.58 -0....
  Gen 3: Reward= 0.366 | M: d7d6 d7d5 e7e5 g7g6 e7e6 E: -0.57 -0....
  Gen 4: Reward= 0.416 | M: d7d5 g7g6 d7d6 e7e5 e7e6 E: -0.45 -0....
  üìä Average reward: 0.383

==================== PHASE 5: TRL EXACT ADVANTAGE CALCULATION (step 21) ====================
üî¢ TRL Advantage Calculation (exact formula):
  Total rewards shape: torch.Size([16])
  Rewards: [ 0.1402      0.1422      0.19857143  0.3004      0.54857143 -0.11666667
  0.01857143  0.54857143  0.63        0.63        0.63        0.63
  0.36217143  0.38737143  0.36617143  0.41577143]

==================== PHASE 6-8: CORRECTED GRPO LOSS CALCULATION (step 21) ====================
üîÑ Computing GRPO loss with proper token-level KL...

üéØ Prompt 1 - Corrected GRPO Loss:
  Gen 1: A=-0.734, logp/len=-7.594, kl=-5.719
         PG=-5.562, KL_penalty=-0.570, total=-6.125
  Gen 2: A=-0.708, logp/len=-5.281, kl=-3.625
         PG=-3.734, KL_penalty=-0.363, total=-4.094
  Gen 3: A=+0.043, logp/len=-8.188, kl=-6.281
         PG=0.352, KL_penalty=-0.629, total=-0.277
  Gen 4: A=+1.399, logp/len=-7.875, kl=-6.156
         PG=11.000, KL_penalty=-0.617, total=10.375
  üìä Prompt averages: PG=0.516, KL=-0.547

üéØ Prompt 2 - Corrected GRPO Loss:
  Gen 1: A=+0.855, logp/len=-6.750, kl=-4.750
         PG=5.781, KL_penalty=-0.475, total=5.312
  Gen 2: A=-1.049, logp/len=-7.812, kl=-5.969
         PG=-8.188, KL_penalty=-0.598, total=-8.812
  Gen 3: A=-0.662, logp/len=-7.344, kl=-5.594
         PG=-4.844, KL_penalty=-0.559, total=-5.406
  Gen 4: A=+0.855, logp/len=-7.594, kl=-5.656
         PG=6.500, KL_penalty=-0.566, total=5.938
  üìä Prompt averages: PG=-0.188, KL=-0.547

üéØ Prompt 3 - Corrected GRPO Loss:
  Gen 1: A=+0.000, logp/len=-5.125, kl=-4.750
         PG=0.000, KL_penalty=-0.475, total=-0.475
  Gen 2: A=+0.000, logp/len=-6.531, kl=-6.156
         PG=0.000, KL_penalty=-0.617, total=-0.617
  Gen 3: A=+0.000, logp/len=-6.000, kl=-5.656
         PG=0.000, KL_penalty=-0.566, total=-0.566
  Gen 4: A=+0.000, logp/len=-5.250, kl=-4.875
         PG=0.000, KL_penalty=-0.488, total=-0.488
  üìä Prompt averages: PG=0.000, KL=-0.535

üéØ Prompt 4 - Corrected GRPO Loss:
  Gen 1: A=-0.843, logp/len=-7.844, kl=-6.250
         PG=-6.625, KL_penalty=-0.625, total=-7.250
  Gen 2: A=+0.183, logp/len=-7.625, kl=-5.938
         PG=1.398, KL_penalty=-0.594, total=0.805
  Gen 3: A=-0.680, logp/len=-6.469, kl=-4.906
         PG=-4.406, KL_penalty=-0.490, total=-4.906
  Gen 4: A=+1.339, logp/len=-6.469, kl=-4.719
         PG=8.688, KL_penalty=-0.473, total=8.188
  üìä Prompt averages: PG=-0.234, KL=-0.547

üî¢ CORRECTED Loss Breakdown:
  Policy Gradient Loss:    0.023
  KL Loss (with gradients):   -0.543
  Total Loss:          =  -0.520
  KL penalty ratio: 95.5%

üîÑ Performing corrected gradient update (step 21)...
  Total gradient norm: 17.88
  ‚úÖ Gradient update applied with proper KL regularization

==================== PHASE 9: POST-UPDATE PERFORMANCE (step 21) ====================
  Prompt 1: avg reward = 0.155
  Prompt 2: avg reward = 0.387
  Prompt 3: avg reward = 0.630
  Prompt 4: avg reward = 0.505
üìä POST-UPDATE (step 21) Performance:
  Average reward: 0.4193
  Positive ratio: 100.0%

==================== PHASE 10: ANALYSIS (step 21) ====================
üîç GRPO Step Impact (step 21):
  Post-update performance: 0.4193
  Step performance change: -0.0136

======================================================================
üß≠ SEQUENTIAL STEP 22/50
======================================================================
üßæ Overfit mode: reusing initial batch (no dataset slice advance)

ü§ñ Generating completions for GRPO training (step 22)...

üéØ Prompt 1: P: 2k4r/ppp1q1p1/nbb1N3/4PP2/3p2P1/PB5P/1PPK3R/3RQ3 b - - 3 ...
    Gen 1: Train_LP= -572.0, Ref_LP=  -34.5
           Text:                                M: c8b8 a6c5 d4d3 b6a5 h8h4  ...
           Normalized: M: c8b8 a6c5 d4d3 b6a5 h8h4 E: -3.42 -3.36 -3.27 -3.43 -3.3 ...
    Gen 2: Train_LP= -616.0, Ref_LP=  -45.5
           Text:                                M: d4d3 a6c5 e7d7 h8h4 b6a5  ...
           Normalized: M: d4d3 a6c5 e7d7 h8h4 b6a5 E: -4.03 -3.61 -3.98 -4.09 -3.69...
    Gen 3: Train_LP= -584.0, Ref_LP=  -33.5
           Text:                                M: a6c5 e7d7 d4d3 e7e8 h8e8  ...
           Normalized: M: a6c5 e7d7 d4d3 e7e8 h8e8 E: -3.36 -3.8 -3.59 -3.92 -3.65 ...
    Gen 4: Train_LP= -516.0, Ref_LP=  -45.2
           Text:                                M: a6c5 c6d7 c8b8 c6f3 e7d7  ...
           Normalized: M: a6c5 c6d7 c8b8 c6f3 e7d7 E: -3.52 -3.54 -3.52 -3.49 -3.53...

üéØ Prompt 2: P: 2N5/2k5/3p1p2/3P1BbP/4Pp2/1n6/4K3/8 w - - 9 49...
    Gen 1: Train_LP= -426.0, Ref_LP=  -37.8
           Text:                                             M: f5g6 c8e7 e2f...
           Normalized: M: f5g6 c8e7 e2f2 e2f3 c8a7 E: 0.23 0.34 0.3 0.27 0.26 B: c8...
    Gen 2: Train_LP= -592.0, Ref_LP=  -45.0
           Text:                                             M: e2f2 c8e7 e2f...
           Normalized: M: e2f2 c8e7 e2f3 f5g4 c8a7 E: 0.27 0.27 0.27 0.27 0.27 B: c...
    Gen 3: Train_LP= -680.0, Ref_LP=  -47.2
           Text:                                             M: e2d3 c8e7 e2f...
           Normalized: M: e2d3 c8e7 e2f2 c8a7 e2f3 E: 0.26 0.31 0.33 0.32 0.32 B: e...
    Gen 4: Train_LP= -672.0, Ref_LP=  -28.4
           Text:                                             M: e2d3 c8e7 f5h...
           Normalized: M: e2d3 c8e7 f5h3 e2f2 f5g4 E: 0.2 0.33 0.17 0.22 0.2 B: c8e...

üéØ Prompt 3: A: 8/2k2b2/R1p4r/3p1p2/P7/2P2B1P/5PPK/4R3 w - - 1 31+a6a5+f7...
    Gen 1: Train_LP= -346.0, Ref_LP=   -0.0
           Text: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
           Normalized: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
    Gen 2: Train_LP= -324.0, Ref_LP=   -0.0
           Text: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
           Normalized: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
    Gen 3: Train_LP= -326.0, Ref_LP=   -0.0
           Text: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
           Normalized: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
    Gen 4: Train_LP= -296.0, Ref_LP=   -0.0
           Text: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
           Normalized: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...

üéØ Prompt 4: P: rnbqkbnr/pp1ppppp/2p5/8/4P3/2N5/PPPP1PPP/R1BQKBNR b KQkq ...
    Gen 1: Train_LP= -548.0, Ref_LP=  -24.4
           Text:                             M: d7d5 e7e5 d7d6 g7g6 c6c5     ...
           Normalized: M: d7d5 e7e5 d7d6 g7g6 c6c5 E: -0.4 -0.59 -0.69 -0.82 -0.81 ...
    Gen 2: Train_LP= -486.0, Ref_LP=  -23.9
           Text:                             M: d7d6 g7g6 d7d5 e7e5 e7e6     ...
           Normalized: M: d7d6 g7g6 d7d5 e7e5 e7e6 E: -0.6 -0.74 -0.36 -0.57 -0.68 ...
    Gen 3: Train_LP= -272.0, Ref_LP=  -23.5
           Text:                             M: e7e5 d7d6 d7d5 g7g6 e7e6     ...
           Normalized: M: e7e5 d7d6 d7d5 g7g6 e7e6 E: -0.59 -0.68 -0.49 -0.7 -0.69 ...
    Gen 4: Train_LP= -388.0, Ref_LP=  -35.5
           Text:                             M: d7d5 d7d6 e7e5 g7g6 d8c7     ...
           Normalized: M: d7d5 d7d6 e7e5 g7g6 d8c7 E: -0.46 -0.67 -0.48 -0.67 -0.67...

==================== PHASE 4: REWARD CALCULATION (step 22) ====================

üéØ Scoring Prompt 1 completions:
  Gen 1: Reward= 0.720 | M: c8b8 a6c5 d4d3 b6a5 h8h4 E: -3.42 -3....
  Gen 2: Reward= 0.210 | M: d4d3 a6c5 e7d7 h8h4 b6a5 E: -4.03 -3....
  Gen 3: Reward= 0.151 | M: a6c5 e7d7 d4d3 e7e8 h8e8 E: -3.36 -3....
  Gen 4: Reward= 0.219 | M: a6c5 c6d7 c8b8 c6f3 e7d7 E: -3.52 -3....
  üìä Average reward: 0.325

üéØ Scoring Prompt 2 completions:
  Gen 1: Reward= 0.469 | M: f5g6 c8e7 e2f2 e2f3 c8a7 E: 0.23 0.34...
  Gen 2: Reward= 0.269 | M: e2f2 c8e7 e2f3 f5g4 c8a7 E: 0.27 0.27...
  Gen 3: Reward= 0.449 | M: e2d3 c8e7 e2f2 c8a7 e2f3 E: 0.26 0.31...
  Gen 4: Reward= 0.415 | M: e2d3 c8e7 f5h3 e2f2 f5g4 E: 0.2 0.33 ...
  üìä Average reward: 0.400

üéØ Scoring Prompt 3 completions:
  Gen 1: Reward= 0.630 | 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b...
  Gen 2: Reward= 0.630 | 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b...
  Gen 3: Reward= 0.630 | 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b...
  Gen 4: Reward= 0.630 | 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b...
  üìä Average reward: 0.630

üéØ Scoring Prompt 4 completions:
  Gen 1: Reward= 0.814 | M: d7d5 e7e5 d7d6 g7g6 c6c5 E: -0.4 -0.5...
  Gen 2: Reward= 0.737 | M: d7d6 g7g6 d7d5 e7e5 e7e6 E: -0.6 -0.7...
  Gen 3: Reward= 0.713 | M: e7e5 d7d6 d7d5 g7g6 e7e6 E: -0.59 -0....
  Gen 4: Reward= 0.390 | M: d7d5 d7d6 e7e5 g7g6 d8c7 E: -0.46 -0....
  üìä Average reward: 0.663

==================== PHASE 5: TRL EXACT ADVANTAGE CALCULATION (step 22) ====================
üî¢ TRL Advantage Calculation (exact formula):
  Total rewards shape: torch.Size([16])
  Rewards: [0.72       0.21017143 0.1506     0.21857143 0.46857143 0.26857143
 0.44857143 0.415      0.63       0.63       0.63       0.63
 0.8136     0.73657143 0.71257143 0.38977143]

==================== PHASE 6-8: CORRECTED GRPO LOSS CALCULATION (step 22) ====================
üîÑ Computing GRPO loss with proper token-level KL...

üéØ Prompt 1 - Corrected GRPO Loss:
  Gen 1: A=+1.490, logp/len=-7.375, kl=-5.625
         PG=11.000, KL_penalty=-0.562, total=10.438
  Gen 2: A=-0.432, logp/len=-8.500, kl=-6.781
         PG=-3.672, KL_penalty=-0.680, total=-4.344
  Gen 3: A=-0.657, logp/len=-7.812, kl=-6.094
         PG=-5.125, KL_penalty=-0.609, total=-5.750
  Gen 4: A=-0.401, logp/len=-6.031, kl=-4.156
         PG=-2.422, KL_penalty=-0.416, total=-2.844
  üìä Prompt averages: PG=-0.059, KL=-0.566

üéØ Prompt 2 - Corrected GRPO Loss:
  Gen 1: A=+0.756, logp/len=-6.000, kl=-4.000
         PG=4.531, KL_penalty=-0.400, total=4.125
  Gen 2: A=-1.455, logp/len=-9.375, kl=-7.594
         PG=-13.625, KL_penalty=-0.758, total=-14.375
  Gen 3: A=+0.535, logp/len=-7.438, kl=-5.469
         PG=3.984, KL_penalty=-0.547, total=3.438
  Gen 4: A=+0.164, logp/len=-7.281, kl=-5.344
         PG=1.195, KL_penalty=-0.535, total=0.660
  üìä Prompt averages: PG=-0.984, KL=-0.559

üéØ Prompt 3 - Corrected GRPO Loss:
  Gen 1: A=+0.000, logp/len=-5.156, kl=-4.781
         PG=0.000, KL_penalty=-0.479, total=-0.479
  Gen 2: A=+0.000, logp/len=-4.875, kl=-4.500
         PG=0.000, KL_penalty=-0.449, total=-0.449
  Gen 3: A=+0.000, logp/len=-7.844, kl=-7.469
         PG=0.000, KL_penalty=-0.746, total=-0.746
  Gen 4: A=+0.000, logp/len=-5.562, kl=-5.219
         PG=0.000, KL_penalty=-0.523, total=-0.523
  üìä Prompt averages: PG=0.000, KL=-0.547

üéØ Prompt 4 - Corrected GRPO Loss:
  Gen 1: A=+0.804, logp/len=-6.812, kl=-5.125
         PG=5.469, KL_penalty=-0.512, total=4.969
  Gen 2: A=+0.392, logp/len=-7.469, kl=-5.688
         PG=2.922, KL_penalty=-0.570, total=2.344
  Gen 3: A=+0.264, logp/len=-8.062, kl=-6.500
         PG=2.125, KL_penalty=-0.648, total=1.477
  Gen 4: A=-1.460, logp/len=-6.031, kl=-4.562
         PG=-8.812, KL_penalty=-0.457, total=-9.250
  üìä Prompt averages: PG=0.422, KL=-0.547

üî¢ CORRECTED Loss Breakdown:
  Policy Gradient Loss:   -0.156
  KL Loss (with gradients):   -0.555
  Total Loss:          =  -0.711
  KL penalty ratio: 78.0%

üîÑ Performing corrected gradient update (step 22)...
  Total gradient norm: 69.54
  ‚úÖ Gradient update applied with proper KL regularization

==================== PHASE 9: POST-UPDATE PERFORMANCE (step 22) ====================
  Prompt 1: avg reward = 0.150
  Prompt 2: avg reward = 0.387
  Prompt 3: avg reward = 0.630
  Prompt 4: avg reward = 0.679
üìä POST-UPDATE (step 22) Performance:
  Average reward: 0.4616
  Positive ratio: 100.0%

==================== PHASE 10: ANALYSIS (step 22) ====================
üîç GRPO Step Impact (step 22):
  Post-update performance: 0.4616
  Step performance change: +0.0423

======================================================================
üß≠ SEQUENTIAL STEP 23/50
======================================================================
üßæ Overfit mode: reusing initial batch (no dataset slice advance)

ü§ñ Generating completions for GRPO training (step 23)...

üéØ Prompt 1: P: 2k4r/ppp1q1p1/nbb1N3/4PP2/3p2P1/PB5P/1PPK3R/3RQ3 b - - 3 ...
    Gen 1: Train_LP= -540.0, Ref_LP=  -44.8
           Text:                                M: h8e8 d4d3 a6c5 g7g6 c6d7  ...
           Normalized: M: h8e8 d4d3 a6c5 g7g6 c6d7 E: -3.57 -3.59 -3.34 -3.58 -3.54...
    Gen 2: Train_LP= -484.0, Ref_LP=  -45.5
           Text:                                M: c6d7 a6c5 b6a5 d4d3 e7d7  ...
           Normalized: M: c6d7 a6c5 b6a5 d4d3 e7d7 E: -3.61 -3.54 -3.83 -3.67 -3.83...
    Gen 3: Train_LP= -470.0, Ref_LP=  -44.5
           Text:                                M: c8b8 e7h4 a6c5 d4d3 e7d7  ...
           Normalized: M: c8b8 e7h4 a6c5 d4d3 e7d7 E: -4.06 -4.21 -3.86 -4.19 -4.24...
    Gen 4: Train_LP= -454.0, Ref_LP=  -34.8
           Text:                                M: a6c5 d4d3 e7h4 e7d7 e7f7  ...
           Normalized: M: a6c5 d4d3 e7h4 e7d7 e7f7 E: -4.11 -4.4 -4.43 -4.44 -4.51 ...

üéØ Prompt 2: P: 2N5/2k5/3p1p2/3P1BbP/4Pp2/1n6/4K3/8 w - - 9 49...
    Gen 1: Train_LP= -404.0, Ref_LP=  -14.6
           Text:                                             M: e2f3 c8a7 e2f...
           Normalized: M: e2f3 c8a7 e2f2 e2d3 c8e7 E: 0.0 0.0 0.0 0.0 0.0 B: e2f3...
    Gen 2: Train_LP= -676.0, Ref_LP=  -14.2
           Text:                                             M: c8a7 e2f2 e2f...
           Normalized: M: c8a7 e2f2 e2f3 e2f1 c8e7 E: 0.0 0.0 0.0 0.0 0.0 B: e2f1...
    Gen 3: Train_LP= -612.0, Ref_LP=  -16.4
           Text:                                             M: e2d1 c8e7 e2d...
           Normalized: M: e2d1 c8e7 e2d3 e2f1 e2f3 E: 0.0 0.0 0.0 0.0 0.0 B: e2f3...
    Gen 4: Train_LP= -616.0, Ref_LP=  -15.2
           Text:                                             M: f5g4 c8a7 e2f...
           Normalized: M: f5g4 c8a7 e2f3 e2f2 c8e7 E: 0.0 0.0 0.0 0.0 0.0 B: f5g4...

üéØ Prompt 3: A: 8/2k2b2/R1p4r/3p1p2/P7/2P2B1P/5PPK/4R3 w - - 1 31+a6a5+f7...
    Gen 1: Train_LP= -276.0, Ref_LP=   -0.0
           Text: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
           Normalized: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
    Gen 2: Train_LP= -290.0, Ref_LP=   -0.0
           Text: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
           Normalized: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
    Gen 3: Train_LP= -268.0, Ref_LP=   -0.0
           Text: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
           Normalized: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
    Gen 4: Train_LP= -330.0, Ref_LP=   -0.0
           Text: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
           Normalized: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...

üéØ Prompt 4: P: rnbqkbnr/pp1ppppp/2p5/8/4P3/2N5/PPPP1PPP/R1BQKBNR b KQkq ...
    Gen 1: Train_LP= -402.0, Ref_LP=  -35.2
           Text:                             M: d7d6 g7g6 e7e5 d7d5 e7e6     ...
           Normalized: M: d7d6 g7g6 e7e5 d7d5 e7e6 E: -0.78 -0.79 -0.68 -0.39 -0.77...
    Gen 2: Train_LP= -620.0, Ref_LP=  -24.1
           Text:                             M: d7d6 g7g6 d7d5 e7e5 c6c5     ...
           Normalized: M: d7d6 g7g6 d7d5 e7e5 c6c5 E: -0.68 -0.7 -0.59 -0.54 -0.78 ...
    Gen 3: Train_LP= -592.0, Ref_LP=  -23.1
           Text:                             M: d7d6 e7e5 e7e6 d7d5 g7g6     ...
           Normalized: M: d7d6 e7e5 e7e6 d7d5 g7g6 E: -0.63 -0.54 -0.75 -0.4 -0.67 ...
    Gen 4: Train_LP= -502.0, Ref_LP=  -24.5
           Text:                             M: d7d5 d7d6 e7e5 g7g6 c6c5     ...
           Normalized: M: d7d5 d7d6 e7e5 g7g6 c6c5 E: -0.37 -0.75 -0.53 -0.71 -0.7 ...

==================== PHASE 4: REWARD CALCULATION (step 23) ====================

üéØ Scoring Prompt 1 completions:
  Gen 1: Reward= 0.175 | M: h8e8 d4d3 a6c5 g7g6 c6d7 E: -3.57 -3....
  Gen 2: Reward= 0.212 | M: c6d7 a6c5 b6a5 d4d3 e7d7 E: -3.61 -3....
  Gen 3: Reward= 0.299 | M: c8b8 e7h4 a6c5 d4d3 e7d7 E: -4.06 -4....
  Gen 4: Reward= 0.221 | M: a6c5 d4d3 e7h4 e7d7 e7f7 E: -4.11 -4....
  üìä Average reward: 0.227

üéØ Scoring Prompt 2 completions:
  Gen 1: Reward= 0.019 | M: e2f3 c8a7 e2f2 e2d3 c8e7 E: 0.0 0.0 0...
  Gen 2: Reward=-0.001 | M: c8a7 e2f2 e2f3 e2f1 c8e7 E: 0.0 0.0 0...
  Gen 3: Reward=-0.075 | M: e2d1 c8e7 e2d3 e2f1 e2f3 E: 0.0 0.0 0...
  Gen 4: Reward=-0.021 | M: f5g4 c8a7 e2f3 e2f2 c8e7 E: 0.0 0.0 0...
  üìä Average reward: -0.020

üéØ Scoring Prompt 3 completions:
  Gen 1: Reward= 0.630 | 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b...
  Gen 2: Reward= 0.630 | 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b...
  Gen 3: Reward= 0.630 | 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b...
  Gen 4: Reward= 0.630 | 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b...
  üìä Average reward: 0.630

üéØ Scoring Prompt 4 completions:
  Gen 1: Reward= 0.731 | M: d7d6 g7g6 e7e5 d7d5 e7e6 E: -0.78 -0....
  Gen 2: Reward= 0.700 | M: d7d6 g7g6 d7d5 e7e5 c6c5 E: -0.68 -0....
  Gen 3: Reward= 0.367 | M: d7d6 e7e5 e7e6 d7d5 g7g6 E: -0.63 -0....
  Gen 4: Reward= 0.614 | M: d7d5 d7d6 e7e5 g7g6 c6c5 E: -0.37 -0....
  üìä Average reward: 0.603

==================== PHASE 5: TRL EXACT ADVANTAGE CALCULATION (step 23) ====================
üî¢ TRL Advantage Calculation (exact formula):
  Total rewards shape: torch.Size([16])
  Rewards: [ 0.175       0.21177143  0.29857143  0.221       0.01857143 -0.00142857
 -0.075      -0.02142857  0.63        0.63        0.63        0.63
  0.73097143  0.7004      0.36737143  0.614     ]

==================== PHASE 6-8: CORRECTED GRPO LOSS CALCULATION (step 23) ====================
üîÑ Computing GRPO loss with proper token-level KL...

üéØ Prompt 1 - Corrected GRPO Loss:
  Gen 1: A=-0.993, logp/len=-6.688, kl=-4.875
         PG=-6.656, KL_penalty=-0.488, total=-7.156
  Gen 2: A=-0.285, logp/len=-5.719, kl=-3.875
         PG=-1.633, KL_penalty=-0.387, total=-2.016
  Gen 3: A=+1.386, logp/len=-8.000, kl=-6.188
         PG=11.062, KL_penalty=-0.617, total=10.438
  Gen 4: A=-0.108, logp/len=-5.969, kl=-4.125
         PG=-0.641, KL_penalty=-0.412, total=-1.055
  üìä Prompt averages: PG=0.527, KL=-0.477

üéØ Prompt 2 - Corrected GRPO Loss:
  Gen 1: A=+0.954, logp/len=-5.906, kl=-4.312
         PG=5.625, KL_penalty=-0.432, total=5.188
  Gen 2: A=+0.457, logp/len=-8.062, kl=-6.469
         PG=3.688, KL_penalty=-0.648, total=3.031
  Gen 3: A=-1.371, logp/len=-5.906, kl=-4.188
         PG=-8.125, KL_penalty=-0.418, total=-8.562
  Gen 4: A=-0.040, logp/len=-7.031, kl=-5.438
         PG=-0.281, KL_penalty=-0.543, total=-0.824
  üìä Prompt averages: PG=0.227, KL=-0.512

üéØ Prompt 3 - Corrected GRPO Loss:
  Gen 1: A=+0.000, logp/len=-4.469, kl=-4.125
         PG=0.000, KL_penalty=-0.412, total=-0.412
  Gen 2: A=+0.000, logp/len=-5.375, kl=-5.000
         PG=0.000, KL_penalty=-0.500, total=-0.500
  Gen 3: A=+0.000, logp/len=-5.688, kl=-5.344
         PG=0.000, KL_penalty=-0.535, total=-0.535
  Gen 4: A=+0.000, logp/len=-6.812, kl=-6.438
         PG=0.000, KL_penalty=-0.645, total=-0.645
  üìä Prompt averages: PG=0.000, KL=-0.523

üéØ Prompt 4 - Corrected GRPO Loss:
  Gen 1: A=+0.775, logp/len=-7.812, kl=-6.094
         PG=6.062, KL_penalty=-0.609, total=5.438
  Gen 2: A=+0.590, logp/len=-8.688, kl=-6.906
         PG=5.125, KL_penalty=-0.691, total=4.438
  Gen 3: A=-1.431, logp/len=-6.812, kl=-5.156
         PG=-9.750, KL_penalty=-0.516, total=-10.250
  Gen 4: A=+0.066, logp/len=-6.156, kl=-4.531
         PG=0.404, KL_penalty=-0.453, total=-0.049
  üìä Prompt averages: PG=0.461, KL=-0.566

üî¢ CORRECTED Loss Breakdown:
  Policy Gradient Loss:    0.305
  KL Loss (with gradients):   -0.520
  Total Loss:          =  -0.215
  KL penalty ratio: 63.0%

üîÑ Performing corrected gradient update (step 23)...
  Total gradient norm: 29.11
  ‚úÖ Gradient update applied with proper KL regularization

==================== PHASE 9: POST-UPDATE PERFORMANCE (step 23) ====================
  Prompt 1: avg reward = 0.155
  Prompt 2: avg reward = 0.387
  Prompt 3: avg reward = 0.630
  Prompt 4: avg reward = 0.505
üìä POST-UPDATE (step 23) Performance:
  Average reward: 0.4193
  Positive ratio: 100.0%

==================== PHASE 10: ANALYSIS (step 23) ====================
üîç GRPO Step Impact (step 23):
  Post-update performance: 0.4193
  Step performance change: -0.0423

======================================================================
üß≠ SEQUENTIAL STEP 24/50
======================================================================
üßæ Overfit mode: reusing initial batch (no dataset slice advance)

ü§ñ Generating completions for GRPO training (step 24)...

üéØ Prompt 1: P: 2k4r/ppp1q1p1/nbb1N3/4PP2/3p2P1/PB5P/1PPK3R/3RQ3 b - - 3 ...
    Gen 1: Train_LP= -400.0, Ref_LP=  -49.2
           Text:                                M: h8h6 a6c5 c6f3 c6d7 e7h4  ...
           Normalized: M: h8h6 a6c5 c6f3 c6d7 e7h4 E: -3.91 -3.54 -4.08 -3.81 -3.72...
    Gen 2: Train_LP= -688.0, Ref_LP=  -34.0
           Text:                                M: e7d7 a6c5 e7e8 d4d3 e7f7  ...
           Normalized: M: e7d7 a6c5 e7e8 d4d3 e7f7 E: -4.14 -4.13 -4.25 -4.1 -4.25 ...
    Gen 3: Train_LP= -520.0, Ref_LP=  -35.2
           Text:                                M: a6c5 e7h4 d4d3 c8b8 h8h3  ...
           Normalized: M: a6c5 e7h4 d4d3 c8b8 h8h3 E: -4.08 -4.17 -4.04 -4.1 -3.76 ...
    Gen 4: Train_LP= -414.0, Ref_LP=  -36.0
           Text:                                M: h8h4 a6c5 e7e8 h8e8 d4d3  ...
           Normalized: M: h8h4 a6c5 e7e8 h8e8 d4d3 E: -4.36 -4.12 -4.51 -4.41 -4.3 ...

üéØ Prompt 2: P: 2N5/2k5/3p1p2/3P1BbP/4Pp2/1n6/4K3/8 w - - 9 49...
    Gen 1: Train_LP= -440.0, Ref_LP=  -58.5
           Text:                                             M: e2f3 f5g6 c8a...
           Normalized: M: e2f3 f5g6 c8a7 c8e7 e2f2 E: 0.3 0.26 0.26 0.3 0.27 B: e2f...
    Gen 2: Train_LP= -608.0, Ref_LP=  -16.5
           Text:                                             M: e2f2 e2f1 c8e...
           Normalized: M: e2f2 e2f1 c8e7 e2f3 f5h3 E: 0.0 0.0 0.0 0.0 0.0 B: e2f1...
    Gen 3: Train_LP= -632.0, Ref_LP=  -79.0
           Text:                                             M: e2f2 c8e7 e2f...
           Normalized: M: e2f2 c8e7 e2f3 e2d3 c8a7 E: 0.08 0.08 0.11 0.11 0.11 B: e...
    Gen 4: Train_LP= -588.0, Ref_LP=  -76.0
           Text:                                             M: e2f2 e2f3 c8e...
           Normalized: M: e2f2 e2f3 c8e7 f5g6 c8a7 E: 0.02 0.02 0.02 0.02 0.03 B: c...

üéØ Prompt 3: A: 8/2k2b2/R1p4r/3p1p2/P7/2P2B1P/5PPK/4R3 w - - 1 31+a6a5+f7...
    Gen 1: Train_LP= -272.0, Ref_LP=   -0.0
           Text: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
           Normalized: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
    Gen 2: Train_LP= -338.0, Ref_LP=   -0.0
           Text: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
           Normalized: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
    Gen 3: Train_LP= -274.0, Ref_LP=   -0.0
           Text: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
           Normalized: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
    Gen 4: Train_LP= -276.0, Ref_LP=   -0.0
           Text: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
           Normalized: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...

üéØ Prompt 4: P: rnbqkbnr/pp1ppppp/2p5/8/4P3/2N5/PPPP1PPP/R1BQKBNR b KQkq ...
    Gen 1: Train_LP= -356.0, Ref_LP=  -57.0
           Text:                             M: e7e5 d7d5 d7d6 e7e6 g7g6     ...
           Normalized: M: e7e5 d7d5 d7d6 e7e6 g7g6 E: -0.51 -0.37 -0.6 -0.64 -0.77 ...
    Gen 2: Train_LP= -506.0, Ref_LP=  -57.0
           Text:                             M: d7d6 d7d5 e7e5 c6c5 g7g6     ...
           Normalized: M: d7d6 d7d5 e7e5 c6c5 g7g6 E: -0.6 -0.36 -0.62 -0.82 -0.73 ...
    Gen 3: Train_LP= -472.0, Ref_LP=  -23.9
           Text:                             M: d7d5 d7d6 g7g6 e7e5 e7e6     ...
           Normalized: M: d7d5 d7d6 g7g6 e7e5 e7e6 E: -0.36 -0.7 -0.7 -0.5 -0.7 B: ...
    Gen 4: Train_LP= -724.0, Ref_LP=  -68.5
           Text:                             M: d7d5 d7d6 g7g6 e7e5 c6c5     ...
           Normalized: M: d7d5 d7d6 g7g6 e7e5 c6c5 E: -0.29 -0.74 -0.77 -0.76 -0.79...

==================== PHASE 4: REWARD CALCULATION (step 24) ====================

üéØ Scoring Prompt 1 completions:
  Gen 1: Reward= 0.437 | M: h8h6 a6c5 c6f3 c6d7 e7h4 E: -3.91 -3....
  Gen 2: Reward= 0.209 | M: e7d7 a6c5 e7e8 d4d3 e7f7 E: -4.14 -4....
  Gen 3: Reward= 0.179 | M: a6c5 e7h4 d4d3 c8b8 h8h3 E: -4.08 -4....
  Gen 4: Reward= 0.204 | M: h8h4 a6c5 e7e8 h8e8 d4d3 E: -4.36 -4....
  üìä Average reward: 0.257

üéØ Scoring Prompt 2 completions:
  Gen 1: Reward= 0.029 | M: e2f3 f5g6 c8a7 c8e7 e2f2 E: 0.3 0.26 ...
  Gen 2: Reward=-0.075 | M: e2f2 e2f1 c8e7 e2f3 f5h3 E: 0.0 0.0 0...
  Gen 3: Reward= 0.099 | M: e2f2 c8e7 e2f3 e2d3 c8a7 E: 0.08 0.08...
  Gen 4: Reward= 0.269 | M: e2f2 e2f3 c8e7 f5g6 c8a7 E: 0.02 0.02...
  üìä Average reward: 0.080

üéØ Scoring Prompt 3 completions:
  Gen 1: Reward= 0.630 | 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b...
  Gen 2: Reward= 0.630 | 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b...
  Gen 3: Reward= 0.630 | 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b...
  Gen 4: Reward= 0.630 | 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b...
  üìä Average reward: 0.630

üéØ Scoring Prompt 4 completions:
  Gen 1: Reward= 0.716 | M: e7e5 d7d5 d7d6 e7e6 g7g6 E: -0.51 -0....
  Gen 2: Reward= 0.684 | M: d7d6 d7d5 e7e5 c6c5 g7g6 E: -0.6 -0.3...
  Gen 3: Reward= 0.391 | M: d7d5 d7d6 g7g6 e7e5 e7e6 E: -0.36 -0....
  Gen 4: Reward= 0.702 | M: d7d5 d7d6 g7g6 e7e5 c6c5 E: -0.29 -0....
  üìä Average reward: 0.623

==================== PHASE 5: TRL EXACT ADVANTAGE CALCULATION (step 24) ====================
üî¢ TRL Advantage Calculation (exact formula):
  Total rewards shape: torch.Size([16])
  Rewards: [ 0.4366      0.209       0.17857143  0.2038      0.02857143 -0.075
  0.09857143  0.26857143  0.63        0.63        0.63        0.63
  0.71577143  0.6836      0.39057143  0.7024    ]

==================== PHASE 6-8: CORRECTED GRPO LOSS CALCULATION (step 24) ====================
üîÑ Computing GRPO loss with proper token-level KL...

üéØ Prompt 1 - Corrected GRPO Loss:
  Gen 1: A=+1.491, logp/len=-6.094, kl=-4.062
         PG=9.062, KL_penalty=-0.406, total=8.625
  Gen 2: A=-0.398, logp/len=-6.688, kl=-4.844
         PG=-2.672, KL_penalty=-0.484, total=-3.156
  Gen 3: A=-0.651, logp/len=-6.438, kl=-4.719
         PG=-4.188, KL_penalty=-0.473, total=-4.656
  Gen 4: A=-0.442, logp/len=-7.125, kl=-5.375
         PG=-3.141, KL_penalty=-0.539, total=-3.688
  üìä Prompt averages: PG=-0.238, KL=-0.475

üéØ Prompt 2 - Corrected GRPO Loss:
  Gen 1: A=-0.357, logp/len=-6.562, kl=-4.594
         PG=-2.344, KL_penalty=-0.459, total=-2.797
  Gen 2: A=-1.074, logp/len=-6.125, kl=-4.531
         PG=-6.594, KL_penalty=-0.453, total=-7.062
  Gen 3: A=+0.127, logp/len=-6.562, kl=-4.750
         PG=0.836, KL_penalty=-0.475, total=0.361
  Gen 4: A=+1.304, logp/len=-7.562, kl=-5.812
         PG=9.875, KL_penalty=-0.582, total=9.312
  üìä Prompt averages: PG=0.438, KL=-0.492

üéØ Prompt 3 - Corrected GRPO Loss:
  Gen 1: A=+0.000, logp/len=-5.281, kl=-4.906
         PG=0.000, KL_penalty=-0.490, total=-0.490
  Gen 2: A=+0.000, logp/len=-5.156, kl=-4.781
         PG=0.000, KL_penalty=-0.479, total=-0.479
  Gen 3: A=+0.000, logp/len=-5.625, kl=-5.250
         PG=0.000, KL_penalty=-0.523, total=-0.523
  Gen 4: A=+0.000, logp/len=-5.125, kl=-4.781
         PG=0.000, KL_penalty=-0.479, total=-0.479
  üìä Prompt averages: PG=0.000, KL=-0.492

üéØ Prompt 4 - Corrected GRPO Loss:
  Gen 1: A=+0.596, logp/len=-7.094, kl=-5.406
         PG=4.219, KL_penalty=-0.539, total=3.688
  Gen 2: A=+0.389, logp/len=-7.281, kl=-5.656
         PG=2.828, KL_penalty=-0.566, total=2.266
  Gen 3: A=-1.495, logp/len=-7.312, kl=-5.531
         PG=-10.938, KL_penalty=-0.555, total=-11.500
  Gen 4: A=+0.510, logp/len=-6.281, kl=-4.469
         PG=3.203, KL_penalty=-0.447, total=2.750
  üìä Prompt averages: PG=-0.168, KL=-0.527

üî¢ CORRECTED Loss Breakdown:
  Policy Gradient Loss:    0.008
  KL Loss (with gradients):   -0.496
  Total Loss:          =  -0.488
  KL penalty ratio: 98.5%

üîÑ Performing corrected gradient update (step 24)...
  Total gradient norm: 15.44
  ‚úÖ Gradient update applied with proper KL regularization

==================== PHASE 9: POST-UPDATE PERFORMANCE (step 24) ====================
  Prompt 1: avg reward = 0.155
  Prompt 2: avg reward = 0.387
  Prompt 3: avg reward = 0.630
  Prompt 4: avg reward = 0.505
üìä POST-UPDATE (step 24) Performance:
  Average reward: 0.4193
  Positive ratio: 100.0%

==================== PHASE 10: ANALYSIS (step 24) ====================
üîç GRPO Step Impact (step 24):
  Post-update performance: 0.4193
  Step performance change: +0.0000

======================================================================
üß≠ SEQUENTIAL STEP 25/50
======================================================================
üßæ Overfit mode: reusing initial batch (no dataset slice advance)

ü§ñ Generating completions for GRPO training (step 25)...

üéØ Prompt 1: P: 2k4r/ppp1q1p1/nbb1N3/4PP2/3p2P1/PB5P/1PPK3R/3RQ3 b - - 3 ...
    Gen 1: Train_LP= -316.0, Ref_LP=  -36.2
           Text:                                M: a6c5 h8h4 e7h4 c6d7 c8b8  ...
           Normalized: M: a6c5 h8h4 e7h4 c6d7 c8b8 E: -4.19 -4.51 -4.39 -4.29 -4.5 ...
    Gen 2: Train_LP= -592.0, Ref_LP=  -45.2
           Text:                                M: b6a5 h8e8 d4d3 c6d7 a6c5  ...
           Normalized: M: b6a5 h8e8 d4d3 c6d7 a6c5 E: -3.42 -3.82 -3.56 -3.51 -3.33...
    Gen 3: Train_LP= -446.0, Ref_LP=  -47.8
           Text:                                M: d4d3 c6b5 c6f3 h8h6 a6c5  ...
           Normalized: M: d4d3 c6b5 c6f3 h8h6 a6c5 E: -3.53 -3.88 -3.63 -3.87 -3.49...
    Gen 4: Train_LP= -468.0, Ref_LP=  -45.8
           Text:                                M: d4d3 a6c5 e7h4 b6a5 c6b5  ...
           Normalized: M: d4d3 a6c5 e7h4 b6a5 c6b5 E: -3.62 -3.67 -4.17 -3.99 -4.16...

üéØ Prompt 2: P: 2N5/2k5/3p1p2/3P1BbP/4Pp2/1n6/4K3/8 w - - 9 49...
    Gen 1: Train_LP= -676.0, Ref_LP=  -80.0
           Text:                                             M: c8e7 e2f3 f5g...
           Normalized: M: c8e7 e2f3 f5g6 e2f2 c8a7 E: 0.32 0.23 0.22 0.22 0.24 B: c...
    Gen 2: Train_LP= -676.0, Ref_LP=  -81.0
           Text:                                             M: e2d3 e2f3 e2f...
           Normalized: M: e2d3 e2f3 e2f2 e2f1 c8e7 E: 0.12 0.22 0.17 0.15 0.18 B: e...
    Gen 3: Train_LP= -748.0, Ref_LP=  -14.1
           Text:                                             M: e2f3 e2d3 c8e...
           Normalized: M: e2f3 e2d3 c8e7 c8a7 e2f2 E: 0.0 0.0 0.0 0.0 0.0 B: c8e7...
    Gen 4: Train_LP= -310.0, Ref_LP=  -77.5
           Text:                                             M: e2f2 c8a7 e2f...
           Normalized: M: e2f2 c8a7 e2f3 e2d3 c8e7 E: 0.17 0.16 0.17 0.17 0.17 B: e...

üéØ Prompt 3: A: 8/2k2b2/R1p4r/3p1p2/P7/2P2B1P/5PPK/4R3 w - - 1 31+a6a5+f7...
    Gen 1: Train_LP= -296.0, Ref_LP=   -0.0
           Text: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
           Normalized: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
    Gen 2: Train_LP= -286.0, Ref_LP=   -0.0
           Text: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
           Normalized: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
    Gen 3: Train_LP= -298.0, Ref_LP=   -0.0
           Text: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
           Normalized: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
    Gen 4: Train_LP= -249.0, Ref_LP=   -0.0
           Text: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
           Normalized: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...

üéØ Prompt 4: P: rnbqkbnr/pp1ppppp/2p5/8/4P3/2N5/PPPP1PPP/R1BQKBNR b KQkq ...
    Gen 1: Train_LP= -628.0, Ref_LP=  -36.2
           Text:                             M: d7d5 d7d6 e7e5 c6c5 g7g6     ...
           Normalized: M: d7d5 d7d6 e7e5 c6c5 g7g6 E: -0.42 -0.55 -0.54 -0.77 -0.54...
    Gen 2: Train_LP= -408.0, Ref_LP=  -26.4
           Text:                             M: d7d5 d7d6 d8c7 e7e5 e7e6     ...
           Normalized: M: d7d5 d7d6 d8c7 e7e5 e7e6 E: -0.46 -0.64 -0.9 -0.55 -0.75 ...
    Gen 3: Train_LP= -604.0, Ref_LP=  -23.2
           Text:                             M: d7d5 e7e5 d7d6 g7g6 e7e6     ...
           Normalized: M: d7d5 e7e5 d7d6 g7g6 e7e6 E: -0.38 -0.57 -0.6 -0.65 -0.73 ...
    Gen 4: Train_LP= -468.0, Ref_LP=  -34.2
           Text:                             M: e7e6 d7d6 e7e5 d7d5 g7g6     ...
           Normalized: M: e7e6 d7d6 e7e5 d7d5 g7g6 E: -0.73 -0.73 -0.69 -0.46 -0.78...

==================== PHASE 4: REWARD CALCULATION (step 25) ====================

üéØ Scoring Prompt 1 completions:
  Gen 1: Reward= 0.159 | M: a6c5 h8h4 e7h4 c6d7 c8b8 E: -4.19 -4....
  Gen 2: Reward= 0.213 | M: b6a5 h8e8 d4d3 c6d7 a6c5 E: -3.42 -3....
  Gen 3: Reward= 0.304 | M: d4d3 c6b5 c6f3 h8h6 a6c5 E: -3.53 -3....
  Gen 4: Reward= 0.250 | M: d4d3 a6c5 e7h4 b6a5 c6b5 E: -3.62 -3....
  üìä Average reward: 0.232

üéØ Scoring Prompt 2 completions:
  Gen 1: Reward= 0.499 | M: c8e7 e2f3 f5g6 e2f2 c8a7 E: 0.32 0.23...
  Gen 2: Reward=-0.035 | M: e2d3 e2f3 e2f2 e2f1 c8e7 E: 0.12 0.22...
  Gen 3: Reward= 0.469 | M: e2f3 e2d3 c8e7 c8a7 e2f2 E: 0.0 0.0 0...
  Gen 4: Reward= 0.099 | M: e2f2 c8a7 e2f3 e2d3 c8e7 E: 0.17 0.16...
  üìä Average reward: 0.258

üéØ Scoring Prompt 3 completions:
  Gen 1: Reward= 0.630 | 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b...
  Gen 2: Reward= 0.630 | 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b...
  Gen 3: Reward= 0.630 | 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b...
  Gen 4: Reward= 0.630 | 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b...
  üìä Average reward: 0.630

üéØ Scoring Prompt 4 completions:
  Gen 1: Reward= 0.715 | M: d7d5 d7d6 e7e5 c6c5 g7g6 E: -0.42 -0....
  Gen 2: Reward= 0.346 | M: d7d5 d7d6 d8c7 e7e5 e7e6 E: -0.46 -0....
  Gen 3: Reward= 0.396 | M: d7d5 e7e5 d7d6 g7g6 e7e6 E: -0.38 -0....
  Gen 4: Reward= 0.713 | M: e7e6 d7d6 e7e5 d7d5 g7g6 E: -0.73 -0....
  üìä Average reward: 0.542

==================== PHASE 5: TRL EXACT ADVANTAGE CALCULATION (step 25) ====================
üî¢ TRL Advantage Calculation (exact formula):
  Total rewards shape: torch.Size([16])
  Rewards: [ 0.159       0.21337143  0.30377143  0.25017143  0.49857143 -0.035
  0.46857143  0.09857143  0.63        0.63        0.63        0.63
  0.7148      0.3458      0.39577143  0.71257143]

==================== PHASE 6-8: CORRECTED GRPO LOSS CALCULATION (step 25) ====================
üîÑ Computing GRPO loss with proper token-level KL...

üéØ Prompt 1 - Corrected GRPO Loss:
  Gen 1: A=-1.190, logp/len=-7.188, kl=-5.406
         PG=-8.562, KL_penalty=-0.539, total=-9.125
  Gen 2: A=-0.299, logp/len=-6.438, kl=-4.719
         PG=-1.922, KL_penalty=-0.473, total=-2.391
  Gen 3: A=+1.184, logp/len=-6.000, kl=-4.156
         PG=7.094, KL_penalty=-0.416, total=6.688
  Gen 4: A=+0.305, logp/len=-6.812, kl=-4.969
         PG=2.078, KL_penalty=-0.496, total=1.578
  üìä Prompt averages: PG=-0.332, KL=-0.480

üéØ Prompt 2 - Corrected GRPO Loss:
  Gen 1: A=+0.903, logp/len=-7.781, kl=-6.000
         PG=7.031, KL_penalty=-0.602, total=6.438
  Gen 2: A=-1.097, logp/len=-6.281, kl=-4.469
         PG=-6.906, KL_penalty=-0.447, total=-7.344
  Gen 3: A=+0.791, logp/len=-6.844, kl=-5.062
         PG=5.406, KL_penalty=-0.508, total=4.906
  Gen 4: A=-0.596, logp/len=-7.188, kl=-5.438
         PG=-4.281, KL_penalty=-0.543, total=-4.812
  üìä Prompt averages: PG=0.312, KL=-0.523

üéØ Prompt 3 - Corrected GRPO Loss:
  Gen 1: A=+0.000, logp/len=-5.438, kl=-5.094
         PG=0.000, KL_penalty=-0.508, total=-0.508
  Gen 2: A=+0.000, logp/len=-6.281, kl=-5.906
         PG=0.000, KL_penalty=-0.590, total=-0.590
  Gen 3: A=+0.000, logp/len=-5.875, kl=-5.500
         PG=0.000, KL_penalty=-0.551, total=-0.551
  Gen 4: A=+0.000, logp/len=-4.969, kl=-4.625
         PG=0.000, KL_penalty=-0.463, total=-0.463
  üìä Prompt averages: PG=0.000, KL=-0.527

üéØ Prompt 4 - Corrected GRPO Loss:
  Gen 1: A=+0.867, logp/len=-7.750, kl=-6.062
         PG=6.719, KL_penalty=-0.605, total=6.125
  Gen 2: A=-0.987, logp/len=-7.844, kl=-6.031
         PG=-7.750, KL_penalty=-0.602, total=-8.375
  Gen 3: A=-0.736, logp/len=-6.750, kl=-5.156
         PG=-4.969, KL_penalty=-0.516, total=-5.500
  Gen 4: A=+0.856, logp/len=-6.438, kl=-4.844
         PG=5.500, KL_penalty=-0.484, total=5.000
  üìä Prompt averages: PG=-0.125, KL=-0.551

üî¢ CORRECTED Loss Breakdown:
  Policy Gradient Loss:   -0.036
  KL Loss (with gradients):   -0.520
  Total Loss:          =  -0.555
  KL penalty ratio: 94.0%

üîÑ Performing corrected gradient update (step 25)...
  Total gradient norm: 41.16
  ‚úÖ Gradient update applied with proper KL regularization

==================== PHASE 9: POST-UPDATE PERFORMANCE (step 25) ====================
  Prompt 1: avg reward = 0.150
  Prompt 2: avg reward = 0.212
  Prompt 3: avg reward = 0.630
  Prompt 4: avg reward = 0.505
üìä POST-UPDATE (step 25) Performance:
  Average reward: 0.3743
  Positive ratio: 87.5%

==================== PHASE 10: ANALYSIS (step 25) ====================
üîç GRPO Step Impact (step 25):
  Post-update performance: 0.3743
  Step performance change: -0.0450

======================================================================
üß≠ SEQUENTIAL STEP 26/50
======================================================================
üßæ Overfit mode: reusing initial batch (no dataset slice advance)

ü§ñ Generating completions for GRPO training (step 26)...

üéØ Prompt 1: P: 2k4r/ppp1q1p1/nbb1N3/4PP2/3p2P1/PB5P/1PPK3R/3RQ3 b - - 3 ...
    Gen 1: Train_LP= -548.0, Ref_LP=  -36.5
           Text:                                M: a6c5 b6a5 e7d7 c6b5 c6f3  ...
           Normalized: M: a6c5 b6a5 e7d7 c6b5 c6f3 E: -3.3 -3.65 -3.71 -3.73 -3.43 ...
    Gen 2: Train_LP= -564.0, Ref_LP=  -44.8
           Text:                                M: d4d3 a6c5 b6a5 c6f3 e7d7  ...
           Normalized: M: d4d3 a6c5 b6a5 c6f3 e7d7 E: -3.48 -3.49 -3.45 -3.47 -3.63...
    Gen 3: Train_LP= -348.0, Ref_LP=  -45.2
           Text:                                M: c6d7 a6c5 h8e8 d4d3 e7d7  ...
           Normalized: M: c6d7 a6c5 h8e8 d4d3 e7d7 E: -4.16 -3.59 -4.02 -3.67 -4.24...
    Gen 4: Train_LP= -556.0, Ref_LP=  -44.5
           Text:                                M: a6c5 c8b8 d4d3 h8e8 c6b5  ...
           Normalized: M: a6c5 c8b8 d4d3 h8e8 c6b5 E: -3.25 -3.74 -3.53 -3.79 -3.76...

üéØ Prompt 2: P: 2N5/2k5/3p1p2/3P1BbP/4Pp2/1n6/4K3/8 w - - 9 49...
    Gen 1: Train_LP= -388.0, Ref_LP=  -14.0
           Text:                                             M: e2f2 e2f1 e2f...
           Normalized: M: e2f2 e2f1 e2f3 c8a7 c8e7 E: 0.0 0.0 0.0 0.0 0.0 B: e2f1...
    Gen 2: Train_LP= -444.0, Ref_LP=  -13.3
           Text:                                             M: e2f1 e2f2 c8e...
           Normalized: M: e2f1 e2f2 c8e7 e2f3 c8a7 E: 0.0 0.0 0.0 0.0 0.0 B: e2f2...
    Gen 3: Train_LP= -744.0, Ref_LP=  -14.0
           Text:                                             M: e2f1 e2f2 c8e...
           Normalized: M: e2f1 e2f2 c8e7 e2f3 e2e1 E: 0.0 0.0 0.0 0.0 0.0 B: e2f3...
    Gen 4: Train_LP= -568.0, Ref_LP=  -15.2
           Text:                                             M: e2f3 e2f2 c8e...
           Normalized: M: e2f3 e2f2 c8e7 e2d3 e2f1 E: 0.0 0.0 0.0 0.0 0.0 B: e2f3...

üéØ Prompt 3: A: 8/2k2b2/R1p4r/3p1p2/P7/2P2B1P/5PPK/4R3 w - - 1 31+a6a5+f7...
    Gen 1: Train_LP= -284.0, Ref_LP=   -0.0
           Text: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
           Normalized: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
    Gen 2: Train_LP= -334.0, Ref_LP=   -0.0
           Text: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
           Normalized: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
    Gen 3: Train_LP= -316.0, Ref_LP=   -0.0
           Text: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
           Normalized: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
    Gen 4: Train_LP= -249.0, Ref_LP=   -0.0
           Text: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
           Normalized: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...

üéØ Prompt 4: P: rnbqkbnr/pp1ppppp/2p5/8/4P3/2N5/PPPP1PPP/R1BQKBNR b KQkq ...
    Gen 1: Train_LP= -528.0, Ref_LP=  -35.2
           Text:                             M: d7d5 e7e6 g7g6 e7e5 d7d6     ...
           Normalized: M: d7d5 e7e6 g7g6 e7e5 d7d6 E: -0.42 -0.76 -0.77 -0.43 -0.72...
    Gen 2: Train_LP= -340.0, Ref_LP=  -24.5
           Text:                             M: d7d6 d7d5 e7e5 e7e6 g7g6     ...
           Normalized: M: d7d6 d7d5 e7e5 e7e6 g7g6 E: -0.76 -0.42 -0.55 -0.8 -0.85 ...
    Gen 3: Train_LP= -536.0, Ref_LP=  -35.5
           Text:                             M: d7d5 d7d6 e7e5 e7e6 g7g6     ...
           Normalized: M: d7d5 d7d6 e7e5 e7e6 g7g6 E: -0.42 -0.61 -0.44 -0.69 -0.76...
    Gen 4: Train_LP= -544.0, Ref_LP=  -35.8
           Text:                             M: d7d5 d7d6 e7e5 g7g6 d8c7     ...
           Normalized: M: d7d5 d7d6 e7e5 g7g6 d8c7 E: -0.43 -0.61 -0.42 -0.79 -0.78...

==================== PHASE 4: REWARD CALCULATION (step 26) ====================

üéØ Scoring Prompt 1 completions:
  Gen 1: Reward= 0.239 | M: a6c5 b6a5 e7d7 c6b5 c6f3 E: -3.3 -3.6...
  Gen 2: Reward= 0.333 | M: d4d3 a6c5 b6a5 c6f3 e7d7 E: -3.48 -3....
  Gen 3: Reward= 0.225 | M: c6d7 a6c5 h8e8 d4d3 e7d7 E: -4.16 -3....
  Gen 4: Reward= 0.253 | M: a6c5 c8b8 d4d3 h8e8 c6b5 E: -3.25 -3....
  üìä Average reward: 0.263

üéØ Scoring Prompt 2 completions:
  Gen 1: Reward=-0.021 | M: e2f2 e2f1 e2f3 c8a7 c8e7 E: 0.0 0.0 0...
  Gen 2: Reward= 0.349 | M: e2f1 e2f2 c8e7 e2f3 c8a7 E: 0.0 0.0 0...
  Gen 3: Reward=-0.055 | M: e2f1 e2f2 c8e7 e2f3 e2e1 E: 0.0 0.0 0...
  Gen 4: Reward=-0.055 | M: e2f3 e2f2 c8e7 e2d3 e2f1 E: 0.0 0.0 0...
  üìä Average reward: 0.054

üéØ Scoring Prompt 3 completions:
  Gen 1: Reward= 0.630 | 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b...
  Gen 2: Reward= 0.630 | 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b...
  Gen 3: Reward= 0.630 | 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b...
  Gen 4: Reward= 0.630 | 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b...
  üìä Average reward: 0.630

üéØ Scoring Prompt 4 completions:
  Gen 1: Reward= 0.745 | M: d7d5 e7e6 g7g6 e7e5 d7d6 E: -0.42 -0....
  Gen 2: Reward= 0.716 | M: d7d6 d7d5 e7e5 e7e6 g7g6 E: -0.76 -0....
  Gen 3: Reward= 0.746 | M: d7d5 d7d6 e7e5 e7e6 g7g6 E: -0.42 -0....
  Gen 4: Reward= 0.749 | M: d7d5 d7d6 e7e5 g7g6 d8c7 E: -0.43 -0....
  üìä Average reward: 0.739

==================== PHASE 5: TRL EXACT ADVANTAGE CALCULATION (step 26) ====================
üî¢ TRL Advantage Calculation (exact formula):
  Total rewards shape: torch.Size([16])
  Rewards: [ 0.23937143  0.3332      0.2254      0.25337143 -0.02142857  0.34857143
 -0.055      -0.055       0.63        0.63        0.63        0.63
  0.74457143  0.71617143  0.74617143  0.74857143]

==================== PHASE 6-8: CORRECTED GRPO LOSS CALCULATION (step 26) ====================
üîÑ Computing GRPO loss with proper token-level KL...

üéØ Prompt 1 - Corrected GRPO Loss:
  Gen 1: A=-0.486, logp/len=-6.531, kl=-4.688
         PG=-3.172, KL_penalty=-0.469, total=-3.641
  Gen 2: A=+1.457, logp/len=-6.281, kl=-4.438
         PG=9.125, KL_penalty=-0.443, total=8.688
  Gen 3: A=-0.775, logp/len=-7.312, kl=-5.438
         PG=-5.656, KL_penalty=-0.543, total=-6.188
  Gen 4: A=-0.196, logp/len=-6.031, kl=-4.188
         PG=-1.180, KL_penalty=-0.418, total=-1.594
  üìä Prompt averages: PG=-0.225, KL=-0.469

üéØ Prompt 2 - Corrected GRPO Loss:
  Gen 1: A=-0.385, logp/len=-6.344, kl=-4.781
         PG=-2.438, KL_penalty=-0.479, total=-2.922
  Gen 2: A=+1.495, logp/len=-6.562, kl=-5.000
         PG=9.812, KL_penalty=-0.500, total=9.312
  Gen 3: A=-0.555, logp/len=-8.562, kl=-6.781
         PG=-4.750, KL_penalty=-0.680, total=-5.438
  Gen 4: A=-0.555, logp/len=-6.875, kl=-5.062
         PG=-3.812, KL_penalty=-0.508, total=-4.312
  üìä Prompt averages: PG=-0.297, KL=-0.539

üéØ Prompt 3 - Corrected GRPO Loss:
  Gen 1: A=+0.000, logp/len=-6.281, kl=-5.938
         PG=0.000, KL_penalty=-0.594, total=-0.594
  Gen 2: A=+0.000, logp/len=-5.688, kl=-5.312
         PG=0.000, KL_penalty=-0.531, total=-0.531
  Gen 3: A=+0.000, logp/len=-7.094, kl=-6.719
         PG=0.000, KL_penalty=-0.672, total=-0.672
  Gen 4: A=+0.000, logp/len=-6.375, kl=-5.969
         PG=0.000, KL_penalty=-0.598, total=-0.598
  üìä Prompt averages: PG=0.000, KL=-0.598

üéØ Prompt 4 - Corrected GRPO Loss:
  Gen 1: A=+0.374, logp/len=-6.625, kl=-4.969
         PG=2.484, KL_penalty=-0.496, total=1.984
  Gen 2: A=-1.491, logp/len=-7.375, kl=-5.750
         PG=-11.000, KL_penalty=-0.574, total=-11.562
  Gen 3: A=+0.480, logp/len=-7.531, kl=-5.875
         PG=3.609, KL_penalty=-0.586, total=3.031
  Gen 4: A=+0.637, logp/len=-5.625, kl=-4.125
         PG=3.578, KL_penalty=-0.412, total=3.172
  üìä Prompt averages: PG=-0.324, KL=-0.516

üî¢ CORRECTED Loss Breakdown:
  Policy Gradient Loss:   -0.212
  KL Loss (with gradients):   -0.531
  Total Loss:          =  -0.742
  KL penalty ratio: 71.5%

üîÑ Performing corrected gradient update (step 26)...
  Total gradient norm: 18.82
  ‚úÖ Gradient update applied with proper KL regularization

==================== PHASE 9: POST-UPDATE PERFORMANCE (step 26) ====================
  Prompt 1: avg reward = 0.155
  Prompt 2: avg reward = 0.387
  Prompt 3: avg reward = 0.630
  Prompt 4: avg reward = 0.559
üìä POST-UPDATE (step 26) Performance:
  Average reward: 0.4329
  Positive ratio: 100.0%

==================== PHASE 10: ANALYSIS (step 26) ====================
üîç GRPO Step Impact (step 26):
  Post-update performance: 0.4329
  Step performance change: +0.0586

======================================================================
üß≠ SEQUENTIAL STEP 27/50
======================================================================
üßæ Overfit mode: reusing initial batch (no dataset slice advance)

ü§ñ Generating completions for GRPO training (step 27)...

üéØ Prompt 1: P: 2k4r/ppp1q1p1/nbb1N3/4PP2/3p2P1/PB5P/1PPK3R/3RQ3 b - - 3 ...
    Gen 1: Train_LP= -696.0, Ref_LP=  -44.0
           Text:                                M: d4d3 a6c5 e7e8 c6d7 c8b8  ...
           Normalized: M: d4d3 a6c5 e7e8 c6d7 c8b8 E: -3.56 -3.48 -3.66 -3.55 -3.71...
    Gen 2: Train_LP= -768.0, Ref_LP=  -44.0
           Text:                                M: b6a5 d4d3 a6c5 e7d7 h8e8  ...
           Normalized: M: b6a5 d4d3 a6c5 e7d7 h8e8 E: -3.65 -3.78 -3.79 -4.02 -3.71...
    Gen 3: Train_LP= -278.0, Ref_LP=  -32.8
           Text:                                M: d4d3 e7d7 e7e8 a6c5 b6a5  ...
           Normalized: M: d4d3 e7d7 e7e8 a6c5 b6a5 E: -3.73 -3.82 -3.82 -3.76 -3.8 ...
    Gen 4: Train_LP= -536.0, Ref_LP=  -46.0
           Text:                                M: h8h6 c8b8 a6c5 e7e8 b6a5  ...
           Normalized: M: h8h6 c8b8 a6c5 e7e8 b6a5 E: -3.82 -3.88 -3.53 -3.94 -3.93...

üéØ Prompt 2: P: 2N5/2k5/3p1p2/3P1BbP/4Pp2/1n6/4K3/8 w - - 9 49...
    Gen 1: Train_LP= -484.0, Ref_LP=  -34.2
           Text:                                             M: e2f2 c8a7 e2f...
           Normalized: M: e2f2 c8a7 e2f3 c8e7 e2f1 E: 0.35 0.34 0.34 0.35 0.34 B: e...
    Gen 2: Train_LP= -418.0, Ref_LP=  -35.5
           Text:                                             M: e2f1 e2f2 e2d...
           Normalized: M: e2f1 e2f2 e2d3 c8e7 e2f3 E: 0.22 0.25 0.26 0.25 0.25 B: e...
    Gen 3: Train_LP= -324.0, Ref_LP=  -25.0
           Text:                                             M: c8a7 e2f3 e2f...
           Normalized: M: c8a7 e2f3 e2f2 c8e7 e2f1 E: 0.15 0.22 0.2 0.16 0.17 B: e2...
    Gen 4: Train_LP= -640.0, Ref_LP=  -24.9
           Text:                                             M: c8e7 e2d3 e2f...
           Normalized: M: c8e7 e2d3 e2f3 e2f2 e2f1 E: 0.11 0.1 0.11 0.08 0.11 B: e2...

üéØ Prompt 3: A: 8/2k2b2/R1p4r/3p1p2/P7/2P2B1P/5PPK/4R3 w - - 1 31+a6a5+f7...
    Gen 1: Train_LP= -316.0, Ref_LP=   -0.0
           Text: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
           Normalized: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
    Gen 2: Train_LP= -326.0, Ref_LP=   -0.0
           Text: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
           Normalized: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
    Gen 3: Train_LP= -272.0, Ref_LP=   -0.0
           Text: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
           Normalized: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
    Gen 4: Train_LP= -334.0, Ref_LP=   -0.0
           Text: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
           Normalized: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...

üéØ Prompt 4: P: rnbqkbnr/pp1ppppp/2p5/8/4P3/2N5/PPPP1PPP/R1BQKBNR b KQkq ...
    Gen 1: Train_LP= -376.0, Ref_LP=  -36.8
           Text:                             M: d7d6 g7g6 d7d5 e7e5 g8f6     ...
           Normalized: M: d7d6 g7g6 d7d5 e7e5 g8f6 E: -0.74 -0.68 -0.39 -0.68 -0.59...
    Gen 2: Train_LP= -352.0, Ref_LP=  -23.2
           Text:                             M: d7d6 d7d5 g7g6 e7e5 e7e6     ...
           Normalized: M: d7d6 d7d5 g7g6 e7e5 e7e6 E: -0.63 -0.51 -0.76 -0.51 -0.8 ...
    Gen 3: Train_LP= -430.0, Ref_LP=  -34.8
           Text:                             M: d7d5 d7d6 g7g6 e7e5 c6c5     ...
           Normalized: M: d7d5 d7d6 g7g6 e7e5 c6c5 E: -0.45 -0.53 -0.71 -0.56 -0.72...
    Gen 4: Train_LP= -502.0, Ref_LP=  -24.8
           Text:                             M: d7d5 d7d6 e7e5 g7g6 d8c7     ...
           Normalized: M: d7d5 d7d6 e7e5 g7g6 d8c7 E: -0.4 -0.59 -0.43 -0.78 -0.82 ...

==================== PHASE 4: REWARD CALCULATION (step 27) ====================

üéØ Scoring Prompt 1 completions:
  Gen 1: Reward= 0.364 | M: d4d3 a6c5 e7e8 c6d7 c8b8 E: -3.56 -3....
  Gen 2: Reward= 0.261 | M: b6a5 d4d3 a6c5 e7d7 h8e8 E: -3.65 -3....
  Gen 3: Reward= 0.212 | M: d4d3 e7d7 e7e8 a6c5 b6a5 E: -3.73 -3....
  Gen 4: Reward= 0.207 | M: h8h6 c8b8 a6c5 e7e8 b6a5 E: -3.82 -3....
  üìä Average reward: 0.261

üéØ Scoring Prompt 2 completions:
  Gen 1: Reward= 0.369 | M: e2f2 c8a7 e2f3 c8e7 e2f1 E: 0.35 0.34...
  Gen 2: Reward=-0.015 | M: e2f1 e2f2 e2d3 c8e7 e2f3 E: 0.22 0.25...
  Gen 3: Reward= 0.019 | M: c8a7 e2f3 e2f2 c8e7 e2f1 E: 0.15 0.22...
  Gen 4: Reward=-0.005 | M: c8e7 e2d3 e2f3 e2f2 e2f1 E: 0.11 0.1 ...
  üìä Average reward: 0.092

üéØ Scoring Prompt 3 completions:
  Gen 1: Reward= 0.630 | 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b...
  Gen 2: Reward= 0.630 | 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b...
  Gen 3: Reward= 0.630 | 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b...
  Gen 4: Reward= 0.630 | 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b...
  üìä Average reward: 0.630

üéØ Scoring Prompt 4 completions:
  Gen 1: Reward= 0.586 | M: d7d6 g7g6 d7d5 e7e5 g8f6 E: -0.74 -0....
  Gen 2: Reward= 0.370 | M: d7d6 d7d5 g7g6 e7e5 e7e6 E: -0.63 -0....
  Gen 3: Reward= 0.723 | M: d7d5 d7d6 g7g6 e7e5 c6c5 E: -0.45 -0....
  Gen 4: Reward= 0.401 | M: d7d5 d7d6 e7e5 g7g6 d8c7 E: -0.4 -0.5...
  üìä Average reward: 0.520

==================== PHASE 5: TRL EXACT ADVANTAGE CALCULATION (step 27) ====================
üî¢ TRL Advantage Calculation (exact formula):
  Total rewards shape: torch.Size([16])
  Rewards: [ 0.36417143  0.26137143  0.21177143  0.20657143  0.36857143 -0.015
  0.01857143 -0.005       0.63        0.63        0.63        0.63
  0.5864      0.36977143  0.7232      0.40057143]

==================== PHASE 6-8: CORRECTED GRPO LOSS CALCULATION (step 27) ====================
üîÑ Computing GRPO loss with proper token-level KL...

üéØ Prompt 1 - Corrected GRPO Loss:
  Gen 1: A=+1.412, logp/len=-7.562, kl=-5.781
         PG=10.688, KL_penalty=-0.578, total=10.125
  Gen 2: A=+0.005, logp/len=-7.594, kl=-5.938
         PG=0.042, KL_penalty=-0.594, total=-0.551
  Gen 3: A=-0.673, logp/len=-8.125, kl=-6.438
         PG=-5.469, KL_penalty=-0.645, total=-6.125
  Gen 4: A=-0.744, logp/len=-6.875, kl=-5.094
         PG=-5.125, KL_penalty=-0.508, total=-5.625
  üìä Prompt averages: PG=0.039, KL=-0.578

üéØ Prompt 2 - Corrected GRPO Loss:
  Gen 1: A=+1.496, logp/len=-7.781, kl=-5.844
         PG=11.625, KL_penalty=-0.586, total=11.062
  Gen 2: A=-0.577, logp/len=-6.938, kl=-5.000
         PG=-4.000, KL_penalty=-0.500, total=-4.500
  Gen 3: A=-0.396, logp/len=-6.000, kl=-4.031
         PG=-2.375, KL_penalty=-0.402, total=-2.781
  Gen 4: A=-0.523, logp/len=-6.812, kl=-4.812
         PG=-3.562, KL_penalty=-0.480, total=-4.031
  üìä Prompt averages: PG=0.422, KL=-0.492

üéØ Prompt 3 - Corrected GRPO Loss:
  Gen 1: A=+0.000, logp/len=-6.188, kl=-5.844
         PG=0.000, KL_penalty=-0.586, total=-0.586
  Gen 2: A=+0.000, logp/len=-5.281, kl=-4.906
         PG=0.000, KL_penalty=-0.490, total=-0.490
  Gen 3: A=+0.000, logp/len=-5.531, kl=-5.156
         PG=0.000, KL_penalty=-0.516, total=-0.516
  Gen 4: A=+0.000, logp/len=-6.125, kl=-5.750
         PG=0.000, KL_penalty=-0.574, total=-0.574
  üìä Prompt averages: PG=0.000, KL=-0.543

üéØ Prompt 4 - Corrected GRPO Loss:
  Gen 1: A=+0.400, logp/len=-7.844, kl=-6.281
         PG=3.141, KL_penalty=-0.629, total=2.516
  Gen 2: A=-0.906, logp/len=-7.531, kl=-5.812
         PG=-6.812, KL_penalty=-0.582, total=-7.406
  Gen 3: A=+1.225, logp/len=-7.594, kl=-5.875
         PG=9.312, KL_penalty=-0.586, total=8.750
  Gen 4: A=-0.720, logp/len=-7.125, kl=-5.625
         PG=-5.125, KL_penalty=-0.562, total=-5.688
  üìä Prompt averages: PG=0.125, KL=-0.590

üî¢ CORRECTED Loss Breakdown:
  Policy Gradient Loss:    0.146
  KL Loss (with gradients):   -0.551
  Total Loss:          =  -0.404
  KL penalty ratio: 79.5%

üîÑ Performing corrected gradient update (step 27)...
  Total gradient norm: 23.85
  ‚úÖ Gradient update applied with proper KL regularization

==================== PHASE 9: POST-UPDATE PERFORMANCE (step 27) ====================
  Prompt 1: avg reward = 0.155
  Prompt 2: avg reward = 0.387
  Prompt 3: avg reward = 0.630
  Prompt 4: avg reward = 0.505
üìä POST-UPDATE (step 27) Performance:
  Average reward: 0.4193
  Positive ratio: 100.0%

==================== PHASE 10: ANALYSIS (step 27) ====================
üîç GRPO Step Impact (step 27):
  Post-update performance: 0.4193
  Step performance change: -0.0136

======================================================================
üß≠ SEQUENTIAL STEP 28/50
======================================================================
üßæ Overfit mode: reusing initial batch (no dataset slice advance)

ü§ñ Generating completions for GRPO training (step 28)...

üéØ Prompt 1: P: 2k4r/ppp1q1p1/nbb1N3/4PP2/3p2P1/PB5P/1PPK3R/3RQ3 b - - 3 ...
    Gen 1: Train_LP= -512.0, Ref_LP=  -55.0
           Text:                                M: a6c5 e7d7 c8b8 h8e8 b6a5  ...
           Normalized: M: a6c5 e7d7 c8b8 h8e8 b6a5 E: -3.27 -3.64 -3.39 -3.43 -3.45...
    Gen 2: Train_LP= -572.0, Ref_LP=  -55.8
           Text:                                M: d4d3 a6c5 e7d7 c6d7 c6b5  ...
           Normalized: M: d4d3 a6c5 e7d7 c6d7 c6b5 E: -4.07 -3.97 -4.36 -4.24 -4.14...
    Gen 3: Train_LP= -724.0, Ref_LP=  -56.5
           Text:                                M: a6c5 d4d3 e7d7 e7h4 c8b8  ...
           Normalized: M: a6c5 d4d3 e7d7 e7h4 c8b8 E: -4.24 -3.74 -4.12 -4.11 -4.21...
    Gen 4: Train_LP= -428.0, Ref_LP=  -35.5
           Text:                                M: c6b5 d4d3 e7d7 c6f3 a6c5  ...
           Normalized: M: c6b5 d4d3 e7d7 c6f3 a6c5 E: -3.7 -3.78 -3.8 -3.87 -3.48 B...

üéØ Prompt 2: P: 2N5/2k5/3p1p2/3P1BbP/4Pp2/1n6/4K3/8 w - - 9 49...
    Gen 1: Train_LP= -576.0, Ref_LP=  -15.4
           Text:                                             M: e2f2 e2f1 e2d...
           Normalized: M: e2f2 e2f1 e2d1 c8e7 e2f3 E: 0.0 0.0 0.0 0.0 0.0 B: e2f3...
    Gen 2: Train_LP= -664.0, Ref_LP=  -78.5
           Text:                                             M: c8a7 e2f3 e2f...
           Normalized: M: c8a7 e2f3 e2f2 c8e7 e2d3 E: 0.12 0.17 0.16 0.17 0.16 B: e...
    Gen 3: Train_LP= -736.0, Ref_LP=  -14.8
           Text:                                             M: e2d1 c8a7 e2f...
           Normalized: M: e2d1 c8a7 e2f2 c8e7 e2f3 E: 0.0 0.0 0.0 0.0 0.0 B: c8a7...
    Gen 4: Train_LP= -624.0, Ref_LP=  -13.6
           Text:                                             M: e2f2 e2f1 e2f...
           Normalized: M: e2f2 e2f1 e2f3 c8e7 c8a7 E: 0.0 0.0 0.0 0.0 0.0 B: e2f2...

üéØ Prompt 3: A: 8/2k2b2/R1p4r/3p1p2/P7/2P2B1P/5PPK/4R3 w - - 1 31+a6a5+f7...
    Gen 1: Train_LP= -294.0, Ref_LP=   -0.0
           Text: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
           Normalized: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
    Gen 2: Train_LP= -262.0, Ref_LP=   -0.0
           Text: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
           Normalized: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
    Gen 3: Train_LP= -274.0, Ref_LP=   -0.0
           Text: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
           Normalized: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
    Gen 4: Train_LP= -260.0, Ref_LP=   -0.0
           Text: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
           Normalized: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...

üéØ Prompt 4: P: rnbqkbnr/pp1ppppp/2p5/8/4P3/2N5/PPPP1PPP/R1BQKBNR b KQkq ...
    Gen 1: Train_LP= -580.0, Ref_LP=  -25.0
           Text:                             M: d7d6 d7d5 e7e5 c6c5 g7g6     ...
           Normalized: M: d7d6 d7d5 e7e5 c6c5 g7g6 E: -0.7 -0.59 -0.67 -0.77 -0.75 ...
    Gen 2: Train_LP= -436.0, Ref_LP=  -36.0
           Text:                             M: d7d6 d7d5 g7g6 e7e5 d8c7     ...
           Normalized: M: d7d6 d7d5 g7g6 e7e5 d8c7 E: -0.54 -0.46 -0.83 -0.48 -0.73...
    Gen 3: Train_LP= -438.0, Ref_LP=  -34.2
           Text:                             M: d7d5 d7d6 e7e5 e7e6 g7g6     ...
           Normalized: M: d7d5 d7d6 e7e5 e7e6 g7g6 E: -0.38 -0.57 -0.56 -0.77 -0.73...
    Gen 4: Train_LP= -584.0, Ref_LP=  -36.0
           Text:                             M: d7d6 e7e5 d7d5 g7g6 a7a6     ...
           Normalized: M: d7d6 e7e5 d7d5 g7g6 a7a6 E: -0.68 -0.67 -0.44 -0.62 -0.82...

==================== PHASE 4: REWARD CALCULATION (step 28) ====================

üéØ Scoring Prompt 1 completions:
  Gen 1: Reward= 0.179 | M: a6c5 e7d7 c8b8 h8e8 b6a5 E: -3.27 -3....
  Gen 2: Reward= 0.282 | M: d4d3 a6c5 e7d7 c6d7 c6b5 E: -4.07 -3....
  Gen 3: Reward= 0.613 | M: a6c5 d4d3 e7d7 e7h4 c8b8 E: -4.24 -3....
  Gen 4: Reward= 0.541 | M: c6b5 d4d3 e7d7 c6f3 a6c5 E: -3.7 -3.7...
  üìä Average reward: 0.403

üéØ Scoring Prompt 2 completions:
  Gen 1: Reward=-0.021 | M: e2f2 e2f1 e2d1 c8e7 e2f3 E: 0.0 0.0 0...
  Gen 2: Reward= 0.099 | M: c8a7 e2f3 e2f2 c8e7 e2d3 E: 0.12 0.17...
  Gen 3: Reward= 0.340 | M: e2d1 c8a7 e2f2 c8e7 e2f3 E: 0.0 0.0 0...
  Gen 4: Reward= 0.329 | M: e2f2 e2f1 e2f3 c8e7 c8a7 E: 0.0 0.0 0...
  üìä Average reward: 0.186

üéØ Scoring Prompt 3 completions:
  Gen 1: Reward= 0.630 | 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b...
  Gen 2: Reward= 0.630 | 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b...
  Gen 3: Reward= 0.630 | 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b...
  Gen 4: Reward= 0.630 | 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b...
  üìä Average reward: 0.630

üéØ Scoring Prompt 4 completions:
  Gen 1: Reward= 0.678 | M: d7d6 d7d5 e7e5 c6c5 g7g6 E: -0.7 -0.5...
  Gen 2: Reward= 0.719 | M: d7d6 d7d5 g7g6 e7e5 d8c7 E: -0.54 -0....
  Gen 3: Reward= 0.402 | M: d7d5 d7d6 e7e5 e7e6 g7g6 E: -0.38 -0....
  Gen 4: Reward= 0.361 | M: d7d6 e7e5 d7d5 g7g6 a7a6 E: -0.68 -0....
  üìä Average reward: 0.540

==================== PHASE 5: TRL EXACT ADVANTAGE CALCULATION (step 28) ====================
üî¢ TRL Advantage Calculation (exact formula):
  Total rewards shape: torch.Size([16])
  Rewards: [ 0.17857143  0.2818      0.61257143  0.54057143 -0.02142857  0.09857143
  0.34        0.32857143  0.63        0.63        0.63        0.63
  0.6776      0.71897143  0.40217143  0.36097143]

==================== PHASE 6-8: CORRECTED GRPO LOSS CALCULATION (step 28) ====================
üîÑ Computing GRPO loss with proper token-level KL...

üéØ Prompt 1 - Corrected GRPO Loss:
  Gen 1: A=-1.089, logp/len=-7.031, kl=-5.344
         PG=-7.656, KL_penalty=-0.535, total=-8.188
  Gen 2: A=-0.589, logp/len=-7.875, kl=-5.938
         PG=-4.625, KL_penalty=-0.594, total=-5.219
  Gen 3: A=+1.013, logp/len=-7.562, kl=-5.875
         PG=7.656, KL_penalty=-0.586, total=7.062
  Gen 4: A=+0.664, logp/len=-6.594, kl=-4.906
         PG=4.375, KL_penalty=-0.490, total=3.891
  üìä Prompt averages: PG=-0.055, KL=-0.551

üéØ Prompt 2 - Corrected GRPO Loss:
  Gen 1: A=-1.170, logp/len=-7.469, kl=-5.781
         PG=-8.750, KL_penalty=-0.578, total=-9.312
  Gen 2: A=-0.494, logp/len=-5.688, kl=-3.641
         PG=-2.812, KL_penalty=-0.363, total=-3.172
  Gen 3: A=+0.864, logp/len=-7.562, kl=-5.812
         PG=6.531, KL_penalty=-0.582, total=5.938
  Gen 4: A=+0.800, logp/len=-6.531, kl=-5.031
         PG=5.219, KL_penalty=-0.504, total=4.719
  üìä Prompt averages: PG=0.047, KL=-0.508

üéØ Prompt 3 - Corrected GRPO Loss:
  Gen 1: A=+0.000, logp/len=-5.594, kl=-5.250
         PG=0.000, KL_penalty=-0.523, total=-0.523
  Gen 2: A=+0.000, logp/len=-5.188, kl=-4.812
         PG=0.000, KL_penalty=-0.480, total=-0.480
  Gen 3: A=+0.000, logp/len=-5.719, kl=-5.375
         PG=0.000, KL_penalty=-0.539, total=-0.539
  Gen 4: A=+0.000, logp/len=-5.281, kl=-4.938
         PG=0.000, KL_penalty=-0.494, total=-0.494
  üìä Prompt averages: PG=0.000, KL=-0.508

üéØ Prompt 4 - Corrected GRPO Loss:
  Gen 1: A=+0.747, logp/len=-6.562, kl=-4.938
         PG=4.906, KL_penalty=-0.494, total=4.406
  Gen 2: A=+0.971, logp/len=-7.906, kl=-6.344
         PG=7.688, KL_penalty=-0.633, total=7.062
  Gen 3: A=-0.747, logp/len=-7.406, kl=-5.781
         PG=-5.531, KL_penalty=-0.578, total=-6.125
  Gen 4: A=-0.970, logp/len=-7.906, kl=-6.312
         PG=-7.688, KL_penalty=-0.633, total=-8.312
  üìä Prompt averages: PG=-0.148, KL=-0.586

üî¢ CORRECTED Loss Breakdown:
  Policy Gradient Loss:   -0.039
  KL Loss (with gradients):   -0.539
  Total Loss:          =  -0.578
  KL penalty ratio: 93.5%

üîÑ Performing corrected gradient update (step 28)...
  Total gradient norm: 19.54
  ‚úÖ Gradient update applied with proper KL regularization

==================== PHASE 9: POST-UPDATE PERFORMANCE (step 28) ====================
  Prompt 1: avg reward = 0.155
  Prompt 2: avg reward = 0.387
  Prompt 3: avg reward = 0.630
  Prompt 4: avg reward = 0.559
üìä POST-UPDATE (step 28) Performance:
  Average reward: 0.4329
  Positive ratio: 100.0%

==================== PHASE 10: ANALYSIS (step 28) ====================
üîç GRPO Step Impact (step 28):
  Post-update performance: 0.4329
  Step performance change: +0.0136

======================================================================
üß≠ SEQUENTIAL STEP 29/50
======================================================================
üßæ Overfit mode: reusing initial batch (no dataset slice advance)

ü§ñ Generating completions for GRPO training (step 29)...

üéØ Prompt 1: P: 2k4r/ppp1q1p1/nbb1N3/4PP2/3p2P1/PB5P/1PPK3R/3RQ3 b - - 3 ...
    Gen 1: Train_LP= -516.0, Ref_LP=  -35.0
           Text:                                M: a6c5 e7h4 e7d7 d4d3 c6d7  ...
           Normalized: M: a6c5 e7h4 e7d7 d4d3 c6d7 E: -3.92 -4.37 -4.3 -3.66 -4.24 ...
    Gen 2: Train_LP= -616.0, Ref_LP=  -43.5
           Text:                                M: a6c5 c8b8 e7d7 h8e8 d4d3  ...
           Normalized: M: a6c5 c8b8 e7d7 h8e8 d4d3 E: -3.37 -3.75 -3.77 -3.51 -3.67...
    Gen 3: Train_LP= -608.0, Ref_LP=  -45.8
           Text:                                M: d4d3 h8h4 c8b8 a6c5 c6f3  ...
           Normalized: M: d4d3 h8h4 c8b8 a6c5 c6f3 E: -4.04 -4.23 -4.07 -3.95 -3.83...
    Gen 4: Train_LP= -472.0, Ref_LP=  -35.2
           Text:                                M: d4d3 h8h3 b6a5 a6c5 e7h4  ...
           Normalized: M: d4d3 h8h3 b6a5 a6c5 e7h4 E: -3.93 -3.97 -3.84 -3.79 -3.8 ...

üéØ Prompt 2: P: 2N5/2k5/3p1p2/3P1BbP/4Pp2/1n6/4K3/8 w - - 9 49...
    Gen 1: Train_LP= -612.0, Ref_LP=  -15.2
           Text:                                             M: f5g6 c8e7 e2f...
           Normalized: M: f5g6 c8e7 e2f3 e2f2 c8a7 E: 0.0 0.0 0.0 0.0 0.0 B: c8a7...
    Gen 2: Train_LP= -528.0, Ref_LP=  -47.0
           Text:                                             M: f5g6 e2f2 f5g...
           Normalized: M: f5g6 e2f2 f5g4 e2f3 e2f1 E: 0.2 0.2 0.2 0.21 0.18 B: e2f3...
    Gen 3: Train_LP= -796.0, Ref_LP=  -59.0
           Text:                                             M: c8e7 e2f2 e2d...
           Normalized: M: c8e7 e2f2 e2d1 e2f3 e2f1 E: 0.37 0.3 0.3 0.35 0.29 B: c8e...
    Gen 4: Train_LP= -296.0, Ref_LP=  -14.3
           Text:                                             M: e2d3 c8a7 c8e...
           Normalized: M: e2d3 c8a7 c8e7 e2f3 e2f2 E: 0.0 0.0 0.0 0.0 0.0 B: e2f3...

üéØ Prompt 3: A: 8/2k2b2/R1p4r/3p1p2/P7/2P2B1P/5PPK/4R3 w - - 1 31+a6a5+f7...
    Gen 1: Train_LP= -332.0, Ref_LP=   -0.0
           Text: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
           Normalized: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
    Gen 2: Train_LP= -272.0, Ref_LP=   -0.0
           Text: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
           Normalized: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
    Gen 3: Train_LP= -278.0, Ref_LP=   -0.0
           Text: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
           Normalized: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
    Gen 4: Train_LP= -278.0, Ref_LP=   -0.0
           Text: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
           Normalized: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...

üéØ Prompt 4: P: rnbqkbnr/pp1ppppp/2p5/8/4P3/2N5/PPPP1PPP/R1BQKBNR b KQkq ...
    Gen 1: Train_LP= -500.0, Ref_LP=  -45.8
           Text:                             M: d7d5 d7d6 e7e5 e7e6 g7g6     ...
           Normalized: M: d7d5 d7d6 e7e5 e7e6 g7g6 E: -0.34 -0.55 -0.51 -0.81 -0.73...
    Gen 2: Train_LP= -700.0, Ref_LP=  -47.0
           Text:                             M: d7d5 d7d6 e7e5 g7g6 a7a6     ...
           Normalized: M: d7d5 d7d6 e7e5 g7g6 a7a6 E: -0.48 -0.76 -0.57 -0.71 -0.76...
    Gen 3: Train_LP= -596.0, Ref_LP=  -35.8
           Text:                             M: d7d6 e7e5 d7d5 d8c7 g7g6     ...
           Normalized: M: d7d6 e7e5 d7d5 d8c7 g7g6 E: -0.58 -0.58 -0.56 -0.85 -0.7 ...
    Gen 4: Train_LP= -434.0, Ref_LP=  -24.9
           Text:                             M: d7d5 g7g6 d7d6 e7e5 e7e6     ...
           Normalized: M: d7d5 g7g6 d7d6 e7e5 e7e6 E: -0.37 -0.9 -0.76 -0.4 -0.96 B...

==================== PHASE 4: REWARD CALCULATION (step 29) ====================

üéØ Scoring Prompt 1 completions:
  Gen 1: Reward= 0.208 | M: a6c5 e7h4 e7d7 d4d3 c6d7 E: -3.92 -4....
  Gen 2: Reward= 0.227 | M: a6c5 c8b8 e7d7 h8e8 d4d3 E: -3.37 -3....
  Gen 3: Reward= 0.329 | M: d4d3 h8h4 c8b8 a6c5 c6f3 E: -4.04 -4....
  Gen 4: Reward= 0.179 | M: d4d3 h8h3 b6a5 a6c5 e7h4 E: -3.93 -3....
  üìä Average reward: 0.236

üéØ Scoring Prompt 2 completions:
  Gen 1: Reward= 0.229 | M: f5g6 c8e7 e2f3 e2f2 c8a7 E: 0.0 0.0 0...
  Gen 2: Reward=-0.097 | M: f5g6 e2f2 f5g4 e2f3 e2f1 E: 0.2 0.2 0...
  Gen 3: Reward= 0.519 | M: c8e7 e2f2 e2d1 e2f3 e2f1 E: 0.37 0.3 ...
  Gen 4: Reward= 0.019 | M: e2d3 c8a7 c8e7 e2f3 e2f2 E: 0.0 0.0 0...
  üìä Average reward: 0.167

üéØ Scoring Prompt 3 completions:
  Gen 1: Reward= 0.630 | 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b...
  Gen 2: Reward= 0.630 | 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b...
  Gen 3: Reward= 0.630 | 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b...
  Gen 4: Reward= 0.630 | 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b...
  üìä Average reward: 0.630

üéØ Scoring Prompt 4 completions:
  Gen 1: Reward= 0.400 | M: d7d5 d7d6 e7e5 e7e6 g7g6 E: -0.34 -0....
  Gen 2: Reward= 0.391 | M: d7d5 d7d6 e7e5 g7g6 a7a6 E: -0.48 -0....
  Gen 3: Reward= 0.367 | M: d7d6 e7e5 d7d5 d8c7 g7g6 E: -0.58 -0....
  Gen 4: Reward= 0.400 | M: d7d5 g7g6 d7d6 e7e5 e7e6 E: -0.37 -0....
  üìä Average reward: 0.390

==================== PHASE 5: TRL EXACT ADVANTAGE CALCULATION (step 29) ====================
üî¢ TRL Advantage Calculation (exact formula):
  Total rewards shape: torch.Size([16])
  Rewards: [ 0.2078      0.22737143  0.3288      0.17857143  0.22857143 -0.09666667
  0.51857143  0.01857143  0.63        0.63        0.63        0.63
  0.40017143  0.39137143  0.36697143  0.39977143]

==================== PHASE 6-8: CORRECTED GRPO LOSS CALCULATION (step 29) ====================
üîÑ Computing GRPO loss with proper token-level KL...

üéØ Prompt 1 - Corrected GRPO Loss:
  Gen 1: A=-0.426, logp/len=-6.562, kl=-4.750
         PG=-2.797, KL_penalty=-0.475, total=-3.266
  Gen 2: A=-0.127, logp/len=-5.406, kl=-3.734
         PG=-0.684, KL_penalty=-0.373, total=-1.055
  Gen 3: A=+1.427, logp/len=-6.312, kl=-4.594
         PG=9.000, KL_penalty=-0.459, total=8.562
  Gen 4: A=-0.874, logp/len=-8.625, kl=-6.812
         PG=-7.531, KL_penalty=-0.680, total=-8.188
  üìä Prompt averages: PG=-0.508, KL=-0.496

üéØ Prompt 2 - Corrected GRPO Loss:
  Gen 1: A=+0.227, logp/len=-6.844, kl=-5.188
         PG=1.555, KL_penalty=-0.520, total=1.031
  Gen 2: A=-0.977, logp/len=-6.562, kl=-4.562
         PG=-6.406, KL_penalty=-0.457, total=-6.875
  Gen 3: A=+1.300, logp/len=-6.344, kl=-4.344
         PG=8.250, KL_penalty=-0.434, total=7.812
  Gen 4: A=-0.550, logp/len=-6.969, kl=-5.125
         PG=-3.828, KL_penalty=-0.512, total=-4.344
  üìä Prompt averages: PG=-0.105, KL=-0.480

üéØ Prompt 3 - Corrected GRPO Loss:
  Gen 1: A=+0.000, logp/len=-5.312, kl=-4.938
         PG=0.000, KL_penalty=-0.494, total=-0.494
  Gen 2: A=+0.000, logp/len=-5.625, kl=-5.281
         PG=0.000, KL_penalty=-0.527, total=-0.527
  Gen 3: A=+0.000, logp/len=-5.156, kl=-4.781
         PG=0.000, KL_penalty=-0.479, total=-0.479
  Gen 4: A=+0.000, logp/len=-5.906, kl=-5.562
         PG=0.000, KL_penalty=-0.555, total=-0.555
  üìä Prompt averages: PG=0.000, KL=-0.516

üéØ Prompt 4 - Corrected GRPO Loss:
  Gen 1: A=+0.679, logp/len=-6.750, kl=-5.156
         PG=4.594, KL_penalty=-0.516, total=4.062
  Gen 2: A=+0.115, logp/len=-7.969, kl=-6.406
         PG=0.918, KL_penalty=-0.641, total=0.277
  Gen 3: A=-1.448, logp/len=-7.625, kl=-6.156
         PG=-11.062, KL_penalty=-0.617, total=-11.688
  Gen 4: A=+0.654, logp/len=-6.531, kl=-4.656
         PG=4.281, KL_penalty=-0.465, total=3.812
  üìä Prompt averages: PG=-0.320, KL=-0.559

üî¢ CORRECTED Loss Breakdown:
  Policy Gradient Loss:   -0.233
  KL Loss (with gradients):   -0.512
  Total Loss:          =  -0.746
  KL penalty ratio: 69.0%

üîÑ Performing corrected gradient update (step 29)...
  Total gradient norm: 21.14
  ‚úÖ Gradient update applied with proper KL regularization

==================== PHASE 9: POST-UPDATE PERFORMANCE (step 29) ====================
  Prompt 1: avg reward = 0.152
  Prompt 2: avg reward = 0.387
  Prompt 3: avg reward = 0.630
  Prompt 4: avg reward = 0.505
üìä POST-UPDATE (step 29) Performance:
  Average reward: 0.4185
  Positive ratio: 100.0%

==================== PHASE 10: ANALYSIS (step 29) ====================
üîç GRPO Step Impact (step 29):
  Post-update performance: 0.4185
  Step performance change: -0.0144

======================================================================
üß≠ SEQUENTIAL STEP 30/50
======================================================================
üßæ Overfit mode: reusing initial batch (no dataset slice advance)

ü§ñ Generating completions for GRPO training (step 30)...

üéØ Prompt 1: P: 2k4r/ppp1q1p1/nbb1N3/4PP2/3p2P1/PB5P/1PPK3R/3RQ3 b - - 3 ...
    Gen 1: Train_LP= -580.0, Ref_LP=  -44.8
           Text:                                M: d4d3 e7d7 a6c5 e7e8 c6f3  ...
           Normalized: M: d4d3 e7d7 a6c5 e7e8 c6f3 E: -3.73 -4.12 -3.83 -3.99 -3.89...
    Gen 2: Train_LP= -552.0, Ref_LP=  -33.2
           Text:                                M: d4d3 e7d7 h8e8 a6c5 e7e8  ...
           Normalized: M: d4d3 e7d7 h8e8 a6c5 e7e8 E: -3.6 -3.72 -3.87 -3.64 -3.79 ...
    Gen 3: Train_LP= -572.0, Ref_LP=  -34.8
           Text:                                M: h8e8 c8b8 c6f3 a6c5 c6b5  ...
           Normalized: M: h8e8 c8b8 c6f3 a6c5 c6b5 E: -3.8 -3.79 -3.67 -3.72 -3.83 ...
    Gen 4: Train_LP= -378.0, Ref_LP=  -43.0
           Text:                                M: a6c5 h8e8 d4d3 e7d7 c8b8  ...
           Normalized: M: a6c5 h8e8 d4d3 e7d7 c8b8 E: -3.44 -3.58 -3.56 -3.57 -3.66...

üéØ Prompt 2: P: 2N5/2k5/3p1p2/3P1BbP/4Pp2/1n6/4K3/8 w - - 9 49...
    Gen 1: Train_LP= -422.0, Ref_LP=  -17.9
           Text:                                             M: f5g6 c8e7 f5h...
           Normalized: M: f5g6 c8e7 f5h7 e2d3 e2f3 E: 0.0 0.0 0.0 0.0 0.0 B: e2d3...
    Gen 2: Train_LP= -600.0, Ref_LP=  -67.0
           Text:                                             M: e2f1 e2f3 e2e...
           Normalized: M: e2f1 e2f3 e2e1 c8e7 e2f2 E: 0.19 0.2 0.18 0.18 0.19 B: e2...
    Gen 3: Train_LP= -632.0, Ref_LP=  -14.6
           Text:                                             M: e2d3 e2f2 c8e...
           Normalized: M: e2d3 e2f2 c8e7 e2f3 e2f1 E: 0.0 0.0 0.0 0.0 0.0 B: e2f3...
    Gen 4: Train_LP= -314.0, Ref_LP=  -78.5
           Text:                                             M: e2f3 c8a7 e2f...
           Normalized: M: e2f3 c8a7 e2f2 c8e7 e2f1 E: 0.31 0.29 0.28 0.31 0.28 B: c...

üéØ Prompt 3: A: 8/2k2b2/R1p4r/3p1p2/P7/2P2B1P/5PPK/4R3 w - - 1 31+a6a5+f7...
    Gen 1: Train_LP= -270.0, Ref_LP=   -0.0
           Text: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
           Normalized: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
    Gen 2: Train_LP= -274.0, Ref_LP=   -0.0
           Text: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
           Normalized: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
    Gen 3: Train_LP= -290.0, Ref_LP=   -0.0
           Text: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
           Normalized: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
    Gen 4: Train_LP= -294.0, Ref_LP=   -0.0
           Text: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
           Normalized: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...

üéØ Prompt 4: P: rnbqkbnr/pp1ppppp/2p5/8/4P3/2N5/PPPP1PPP/R1BQKBNR b KQkq ...
    Gen 1: Train_LP= -416.0, Ref_LP=  -24.1
           Text:                             M: d7d5 d7d6 g7g6 e7e5 e7e6     ...
           Normalized: M: d7d5 d7d6 g7g6 e7e5 e7e6 E: -0.31 -0.7 -0.65 -0.62 -0.64 ...
    Gen 2: Train_LP= -516.0, Ref_LP=  -23.5
           Text:                             M: d7d5 e7e5 g7g6 d7d6 e7e6     ...
           Normalized: M: d7d5 e7e5 g7g6 d7d6 e7e6 E: -0.46 -0.59 -0.63 -0.63 -0.9 ...
    Gen 3: Train_LP= -612.0, Ref_LP=  -36.5
           Text:                             M: d7d6 d7d5 e7e5 g7g6 d8c7     ...
           Normalized: M: d7d6 d7d5 e7e5 g7g6 d8c7 E: -0.61 -0.56 -0.64 -0.74 -0.83...
    Gen 4: Train_LP= -636.0, Ref_LP=  -24.4
           Text:                             M: d7d5 d7d6 e7e6 e7e5 g7g6     ...
           Normalized: M: d7d5 d7d6 e7e6 e7e5 g7g6 E: -0.42 -0.6 -0.83 -0.43 -0.71 ...

==================== PHASE 4: REWARD CALCULATION (step 30) ====================

üéØ Scoring Prompt 1 completions:
  Gen 1: Reward= 0.276 | M: d4d3 e7d7 a6c5 e7e8 c6f3 E: -3.73 -4....
  Gen 2: Reward= 0.166 | M: d4d3 e7d7 h8e8 a6c5 e7e8 E: -3.6 -3.7...
  Gen 3: Reward= 0.308 | M: h8e8 c8b8 c6f3 a6c5 c6b5 E: -3.8 -3.7...
  Gen 4: Reward= 0.367 | M: a6c5 h8e8 d4d3 e7d7 c8b8 E: -3.44 -3....
  üìä Average reward: 0.279

üéØ Scoring Prompt 2 completions:
  Gen 1: Reward=-0.117 | M: f5g6 c8e7 f5h7 e2d3 e2f3 E: 0.0 0.0 0...
  Gen 2: Reward=-0.035 | M: e2f1 e2f3 e2e1 c8e7 e2f2 E: 0.19 0.2 ...
  Gen 3: Reward=-0.055 | M: e2d3 e2f2 c8e7 e2f3 e2f1 E: 0.0 0.0 0...
  Gen 4: Reward= 0.469 | M: e2f3 c8a7 e2f2 c8e7 e2f1 E: 0.31 0.29...
  üìä Average reward: 0.065

üéØ Scoring Prompt 3 completions:
  Gen 1: Reward= 0.630 | 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b...
  Gen 2: Reward= 0.630 | 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b...
  Gen 3: Reward= 0.630 | 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b...
  Gen 4: Reward= 0.630 | 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b...
  üìä Average reward: 0.630

üéØ Scoring Prompt 4 completions:
  Gen 1: Reward= 0.533 | M: d7d5 d7d6 g7g6 e7e5 e7e6 E: -0.31 -0....
  Gen 2: Reward= 0.393 | M: d7d5 e7e5 g7g6 d7d6 e7e6 E: -0.46 -0....
  Gen 3: Reward= 0.366 | M: d7d6 d7d5 e7e5 g7g6 d8c7 E: -0.61 -0....
  Gen 4: Reward= 0.397 | M: d7d5 d7d6 e7e6 e7e5 g7g6 E: -0.42 -0....
  üìä Average reward: 0.422

==================== PHASE 5: TRL EXACT ADVANTAGE CALCULATION (step 30) ====================
üî¢ TRL Advantage Calculation (exact formula):
  Total rewards shape: torch.Size([16])
  Rewards: [ 0.27577143  0.1658      0.30777143  0.36697143 -0.11666667 -0.035
 -0.055       0.46857143  0.63        0.63        0.63        0.63
  0.53337143  0.39257143  0.36577143  0.39657143]

==================== PHASE 6-8: CORRECTED GRPO LOSS CALCULATION (step 30) ====================
üîÑ Computing GRPO loss with proper token-level KL...

üéØ Prompt 1 - Corrected GRPO Loss:
  Gen 1: A=-0.039, logp/len=-6.719, kl=-4.969
         PG=-0.264, KL_penalty=-0.496, total=-0.758
  Gen 2: A=-1.341, logp/len=-7.156, kl=-5.312
         PG=-9.625, KL_penalty=-0.531, total=-10.125
  Gen 3: A=+0.340, logp/len=-6.719, kl=-4.812
         PG=2.281, KL_penalty=-0.480, total=1.797
  Gen 4: A=+1.041, logp/len=-6.594, kl=-4.906
         PG=6.875, KL_penalty=-0.490, total=6.375
  üìä Prompt averages: PG=-0.180, KL=-0.500

üéØ Prompt 2 - Corrected GRPO Loss:
  Gen 1: A=-0.672, logp/len=-6.844, kl=-5.031
         PG=-4.594, KL_penalty=-0.504, total=-5.094
  Gen 2: A=-0.371, logp/len=-7.062, kl=-5.125
         PG=-2.625, KL_penalty=-0.512, total=-3.141
  Gen 3: A=-0.445, logp/len=-7.469, kl=-5.656
         PG=-3.328, KL_penalty=-0.566, total=-3.891
  Gen 4: A=+1.488, logp/len=-6.125, kl=-4.188
         PG=9.125, KL_penalty=-0.418, total=8.688
  üìä Prompt averages: PG=-0.359, KL=-0.500

üéØ Prompt 3 - Corrected GRPO Loss:
  Gen 1: A=+0.000, logp/len=-5.031, kl=-4.656
         PG=0.000, KL_penalty=-0.465, total=-0.465
  Gen 2: A=+0.000, logp/len=-6.750, kl=-6.406
         PG=0.000, KL_penalty=-0.641, total=-0.641
  Gen 3: A=+0.000, logp/len=-5.281, kl=-4.938
         PG=0.000, KL_penalty=-0.494, total=-0.494
  Gen 4: A=+0.000, logp/len=-5.438, kl=-5.062
         PG=0.000, KL_penalty=-0.508, total=-0.508
  üìä Prompt averages: PG=0.000, KL=-0.527

üéØ Prompt 4 - Corrected GRPO Loss:
  Gen 1: A=+1.475, logp/len=-8.062, kl=-6.312
         PG=11.875, KL_penalty=-0.633, total=11.250
  Gen 2: A=-0.391, logp/len=-7.406, kl=-5.750
         PG=-2.891, KL_penalty=-0.574, total=-3.469
  Gen 3: A=-0.746, logp/len=-7.969, kl=-6.500
         PG=-5.938, KL_penalty=-0.648, total=-6.594
  Gen 4: A=-0.338, logp/len=-6.875, kl=-5.219
         PG=-2.328, KL_penalty=-0.523, total=-2.844
  üìä Prompt averages: PG=0.184, KL=-0.594

üî¢ CORRECTED Loss Breakdown:
  Policy Gradient Loss:   -0.089
  KL Loss (with gradients):   -0.531
  Total Loss:          =  -0.621
  KL penalty ratio: 85.5%

üîÑ Performing corrected gradient update (step 30)...
  Total gradient norm: 16.27
  ‚úÖ Gradient update applied with proper KL regularization

==================== PHASE 9: POST-UPDATE PERFORMANCE (step 30) ====================
  Prompt 1: avg reward = 0.155
  Prompt 2: avg reward = 0.387
  Prompt 3: avg reward = 0.630
  Prompt 4: avg reward = 0.559
üìä POST-UPDATE (step 30) Performance:
  Average reward: 0.4329
  Positive ratio: 100.0%

==================== PHASE 10: ANALYSIS (step 30) ====================
üîç GRPO Step Impact (step 30):
  Post-update performance: 0.4329
  Step performance change: +0.0144

======================================================================
üß≠ SEQUENTIAL STEP 31/50
======================================================================
üßæ Overfit mode: reusing initial batch (no dataset slice advance)

ü§ñ Generating completions for GRPO training (step 31)...

üéØ Prompt 1: P: 2k4r/ppp1q1p1/nbb1N3/4PP2/3p2P1/PB5P/1PPK3R/3RQ3 b - - 3 ...
    Gen 1: Train_LP= -552.0, Ref_LP=  -45.5
           Text:                                M: d4d3 e7d7 a6c5 e7e8 h8h3  ...
           Normalized: M: d4d3 e7d7 a6c5 e7e8 h8h3 E: -4.28 -4.26 -3.94 -4.19 -4.27...
    Gen 2: Train_LP= -348.0, Ref_LP=  -45.0
           Text:                                M: d4d3 c6d7 a6c5 h8e8 b6a5  ...
           Normalized: M: d4d3 c6d7 a6c5 h8e8 b6a5 E: -3.79 -4.18 -3.74 -4.17 -4.08...
    Gen 3: Train_LP= -668.0, Ref_LP=  -43.0
           Text:                                M: a6c5 h8e8 c8b8 b6a5 d4d3  ...
           Normalized: M: a6c5 h8e8 c8b8 b6a5 d4d3 E: -3.17 -3.46 -3.53 -3.43 -3.18...
    Gen 4: Train_LP= -418.0, Ref_LP=  -37.0
           Text:                                M: c6b5 a6c5 d4d3 h8h3 e7e8  ...
           Normalized: M: c6b5 a6c5 d4d3 h8h3 e7e8 E: -4.07 -3.8 -3.69 -3.96 -3.78 ...

üéØ Prompt 2: P: 2N5/2k5/3p1p2/3P1BbP/4Pp2/1n6/4K3/8 w - - 9 49...
    Gen 1: Train_LP= -520.0, Ref_LP=  -78.5
           Text:                                             M: e2f3 e2f2 e2e...
           Normalized: M: e2f3 e2f2 e2e1 c8e7 e2f1 E: 0.41 0.43 0.41 0.41 0.41 B: e...
    Gen 2: Train_LP= -544.0, Ref_LP=  -81.5
           Text:                                             M: e2f2 e2d3 c8e...
           Normalized: M: e2f2 e2d3 c8e7 e2f3 f5g4 E: 0.21 0.26 0.28 0.27 0.26 B: c...
    Gen 3: Train_LP= -596.0, Ref_LP=  -76.5
           Text:                                             M: e2f3 e2f2 c8e...
           Normalized: M: e2f3 e2f2 c8e7 c8a7 e2f1 E: 0.25 0.25 0.24 0.25 0.25 B: c...
    Gen 4: Train_LP= -362.0, Ref_LP=  -15.8
           Text:                                             M: e2f3 f5g6 c8a...
           Normalized: M: e2f3 f5g6 c8a7 c8e7 f5e6 E: 0.0 0.0 0.0 0.0 0.0 B: c8e7...

üéØ Prompt 3: A: 8/2k2b2/R1p4r/3p1p2/P7/2P2B1P/5PPK/4R3 w - - 1 31+a6a5+f7...
    Gen 1: Train_LP= -320.0, Ref_LP=   -0.0
           Text: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
           Normalized: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
    Gen 2: Train_LP= -239.0, Ref_LP=   -0.0
           Text: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
           Normalized: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
    Gen 3: Train_LP= -316.0, Ref_LP=   -0.0
           Text: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
           Normalized: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
    Gen 4: Train_LP= -286.0, Ref_LP=   -0.0
           Text: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
           Normalized: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...

üéØ Prompt 4: P: rnbqkbnr/pp1ppppp/2p5/8/4P3/2N5/PPPP1PPP/R1BQKBNR b KQkq ...
    Gen 1: Train_LP= -564.0, Ref_LP=  -24.5
           Text:                             M: e7e5 d7d6 e7e6 d7d5 c6c5     ...
           Normalized: M: e7e5 d7d6 e7e6 d7d5 c6c5 E: -0.54 -0.72 -0.73 -0.45 -0.74...
    Gen 2: Train_LP= -636.0, Ref_LP=  -26.2
           Text:                             M: d7d5 d7d6 e7e5 g7g6 a7a6     ...
           Normalized: M: d7d5 d7d6 e7e5 g7g6 a7a6 E: -0.47 -0.61 -0.67 -0.79 -0.76...
    Gen 3: Train_LP= -584.0, Ref_LP=  -24.0
           Text:                             M: d7d6 g7g6 d7d5 e7e5 e7e6     ...
           Normalized: M: d7d6 g7g6 d7d5 e7e5 e7e6 E: -0.64 -0.74 -0.37 -0.63 -0.66...
    Gen 4: Train_LP= -516.0, Ref_LP=  -23.0
           Text:                             M: d7d6 d7d5 e7e5 g7g6 e7e6     ...
           Normalized: M: d7d6 d7d5 e7e5 g7g6 e7e6 E: -0.59 -0.51 -0.56 -0.76 -0.74...

==================== PHASE 4: REWARD CALCULATION (step 31) ====================

üéØ Scoring Prompt 1 completions:
  Gen 1: Reward= 0.195 | M: d4d3 e7d7 a6c5 e7e8 h8h3 E: -4.28 -4....
  Gen 2: Reward= 0.222 | M: d4d3 c6d7 a6c5 h8e8 b6a5 E: -3.79 -4....
  Gen 3: Reward= 0.402 | M: a6c5 h8e8 c8b8 b6a5 d4d3 E: -3.17 -3....
  Gen 4: Reward= 0.145 | M: c6b5 a6c5 d4d3 h8h3 e7e8 E: -4.07 -3....
  üìä Average reward: 0.241

üéØ Scoring Prompt 2 completions:
  Gen 1: Reward= 0.335 | M: e2f3 e2f2 e2e1 c8e7 e2f1 E: 0.41 0.43...
  Gen 2: Reward= 0.415 | M: e2f2 e2d3 c8e7 e2f3 f5g4 E: 0.21 0.26...
  Gen 3: Reward= 0.289 | M: e2f3 e2f2 c8e7 c8a7 e2f1 E: 0.25 0.25...
  Gen 4: Reward= 0.385 | M: e2f3 f5g6 c8a7 c8e7 f5e6 E: 0.0 0.0 0...
  üìä Average reward: 0.356

üéØ Scoring Prompt 3 completions:
  Gen 1: Reward= 0.630 | 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b...
  Gen 2: Reward= 0.630 | 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b...
  Gen 3: Reward= 0.630 | 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b...
  Gen 4: Reward= 0.630 | 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b...
  üìä Average reward: 0.630

üéØ Scoring Prompt 4 completions:
  Gen 1: Reward= 0.615 | M: e7e5 d7d6 e7e6 d7d5 c6c5 E: -0.54 -0....
  Gen 2: Reward= 0.746 | M: d7d5 d7d6 e7e5 g7g6 a7a6 E: -0.47 -0....
  Gen 3: Reward= 0.732 | M: d7d6 g7g6 d7d5 e7e5 e7e6 E: -0.64 -0....
  Gen 4: Reward= 0.721 | M: d7d6 d7d5 e7e5 g7g6 e7e6 E: -0.59 -0....
  üìä Average reward: 0.704

==================== PHASE 5: TRL EXACT ADVANTAGE CALCULATION (step 31) ====================
üî¢ TRL Advantage Calculation (exact formula):
  Total rewards shape: torch.Size([16])
  Rewards: [0.1954     0.22177143 0.402      0.145      0.335      0.415
 0.28857143 0.385      0.63       0.63       0.63       0.63
 0.61537143 0.74617143 0.73217143 0.72097143]

==================== PHASE 6-8: CORRECTED GRPO LOSS CALCULATION (step 31) ====================
üîÑ Computing GRPO loss with proper token-level KL...

üéØ Prompt 1 - Corrected GRPO Loss:
  Gen 1: A=-0.408, logp/len=-7.031, kl=-5.250
         PG=-2.859, KL_penalty=-0.523, total=-3.375
  Gen 2: A=-0.172, logp/len=-6.438, kl=-4.688
         PG=-1.109, KL_penalty=-0.469, total=-1.578
  Gen 3: A=+1.438, logp/len=-6.625, kl=-4.938
         PG=9.500, KL_penalty=-0.494, total=9.000
  Gen 4: A=-0.858, logp/len=-7.562, kl=-5.719
         PG=-6.500, KL_penalty=-0.570, total=-7.062
  üìä Prompt averages: PG=-0.242, KL=-0.516

üéØ Prompt 2 - Corrected GRPO Loss:
  Gen 1: A=-0.375, logp/len=-7.906, kl=-6.000
         PG=-2.969, KL_penalty=-0.602, total=-3.562
  Gen 2: A=+1.061, logp/len=-5.969, kl=-4.094
         PG=6.344, KL_penalty=-0.410, total=5.938
  Gen 3: A=-1.209, logp/len=-6.406, kl=-4.500
         PG=-7.750, KL_penalty=-0.449, total=-8.188
  Gen 4: A=+0.523, logp/len=-7.969, kl=-6.375
         PG=4.156, KL_penalty=-0.637, total=3.516
  üìä Prompt averages: PG=-0.055, KL=-0.527

üéØ Prompt 3 - Corrected GRPO Loss:
  Gen 1: A=+0.000, logp/len=-5.531, kl=-5.156
         PG=0.000, KL_penalty=-0.516, total=-0.516
  Gen 2: A=+0.000, logp/len=-5.438, kl=-5.062
         PG=0.000, KL_penalty=-0.508, total=-0.508
  Gen 3: A=+0.000, logp/len=-6.719, kl=-6.344
         PG=0.000, KL_penalty=-0.633, total=-0.633
  Gen 4: A=+0.000, logp/len=-6.281, kl=-5.906
         PG=0.000, KL_penalty=-0.590, total=-0.590
  üìä Prompt averages: PG=0.000, KL=-0.562

üéØ Prompt 4 - Corrected GRPO Loss:
  Gen 1: A=-1.478, logp/len=-7.688, kl=-5.906
         PG=-11.375, KL_penalty=-0.590, total=-11.938
  Gen 2: A=+0.711, logp/len=-9.062, kl=-7.469
         PG=6.438, KL_penalty=-0.746, total=5.688
  Gen 3: A=+0.477, logp/len=-7.469, kl=-5.688
         PG=3.562, KL_penalty=-0.570, total=3.000
  Gen 4: A=+0.289, logp/len=-7.125, kl=-5.562
         PG=2.062, KL_penalty=-0.555, total=1.508
  üìä Prompt averages: PG=0.172, KL=-0.617

üî¢ CORRECTED Loss Breakdown:
  Policy Gradient Loss:   -0.031
  KL Loss (with gradients):   -0.555
  Total Loss:          =  -0.586
  KL penalty ratio: 94.5%

üîÑ Performing corrected gradient update (step 31)...
  Total gradient norm: 22.04
  ‚úÖ Gradient update applied with proper KL regularization

==================== PHASE 9: POST-UPDATE PERFORMANCE (step 31) ====================
  Prompt 1: avg reward = 0.155
  Prompt 2: avg reward = 0.387
  Prompt 3: avg reward = 0.630
  Prompt 4: avg reward = 0.559
üìä POST-UPDATE (step 31) Performance:
  Average reward: 0.4329
  Positive ratio: 100.0%

==================== PHASE 10: ANALYSIS (step 31) ====================
üîç GRPO Step Impact (step 31):
  Post-update performance: 0.4329
  Step performance change: +0.0000

======================================================================
üß≠ SEQUENTIAL STEP 32/50
======================================================================
üßæ Overfit mode: reusing initial batch (no dataset slice advance)

ü§ñ Generating completions for GRPO training (step 32)...

üéØ Prompt 1: P: 2k4r/ppp1q1p1/nbb1N3/4PP2/3p2P1/PB5P/1PPK3R/3RQ3 b - - 3 ...
    Gen 1: Train_LP= -498.0, Ref_LP=  -33.0
           Text:                                M: d4d3 a6c5 c6f3 e7d7 c8b8  ...
           Normalized: M: d4d3 a6c5 c6f3 e7d7 c8b8 E: -3.44 -3.39 -3.68 -3.58 -3.61...
    Gen 2: Train_LP= -390.0, Ref_LP=  -35.0
           Text:                                M: a6c5 c8b8 e7h4 c6d7 h8e8  ...
           Normalized: M: a6c5 c8b8 e7h4 c6d7 h8e8 E: -4.23 -4.49 -4.31 -4.51 -4.44...
    Gen 3: Train_LP= -700.0, Ref_LP=  -33.2
           Text:                                M: e7d7 c8b8 h8e8 a6c5 d4d3  ...
           Normalized: M: e7d7 c8b8 h8e8 a6c5 d4d3 E: -3.94 -3.61 -3.65 -3.71 -3.82...
    Gen 4: Train_LP= -478.0, Ref_LP=  -34.0
           Text:                                M: h8h4 a6c5 c8b8 e7d7 d4d3  ...
           Normalized: M: h8h4 a6c5 c8b8 e7d7 d4d3 E: -4.25 -3.99 -4.04 -4.05 -3.95...

üéØ Prompt 2: P: 2N5/2k5/3p1p2/3P1BbP/4Pp2/1n6/4K3/8 w - - 9 49...
    Gen 1: Train_LP= -348.0, Ref_LP=  -81.5
           Text:                                             M: e2f1 c8e7 e2f...
           Normalized: M: e2f1 c8e7 e2f2 e2d3 e2f3 E: 0.42 0.41 0.37 0.41 0.45 B: e...
    Gen 2: Train_LP= -520.0, Ref_LP=  -14.0
           Text:                                             M: e2f2 e2d3 c8a...
           Normalized: M: e2f2 e2d3 c8a7 c8e7 e2f3 E: 0.0 0.0 0.0 0.0 0.0 B: c8e7...
    Gen 3: Train_LP= -612.0, Ref_LP=  -77.5
           Text:                                             M: e2f1 e2f2 c8a...
           Normalized: M: e2f1 e2f2 c8a7 e2f3 c8e7 E: 0.12 0.12 0.16 0.11 0.11 B: c...
    Gen 4: Train_LP= -568.0, Ref_LP=  -82.5
           Text:                                             M: e2d3 e2f2 c8e...
           Normalized: M: e2d3 e2f2 c8e7 e2f3 e2f1 E: 0.31 0.39 0.28 0.39 0.38 B: e...

üéØ Prompt 3: A: 8/2k2b2/R1p4r/3p1p2/P7/2P2B1P/5PPK/4R3 w - - 1 31+a6a5+f7...
    Gen 1: Train_LP= -320.0, Ref_LP=   -0.0
           Text: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
           Normalized: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
    Gen 2: Train_LP= -290.0, Ref_LP=   -0.0
           Text: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
           Normalized: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
    Gen 3: Train_LP= -224.0, Ref_LP=   -0.0
           Text: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
           Normalized: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
    Gen 4: Train_LP= -276.0, Ref_LP=   -0.0
           Text: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
           Normalized: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...

üéØ Prompt 4: P: rnbqkbnr/pp1ppppp/2p5/8/4P3/2N5/PPPP1PPP/R1BQKBNR b KQkq ...
    Gen 1: Train_LP= -536.0, Ref_LP=  -45.8
           Text:                             M: d7d5 d7d6 e7e5 g7g6 c6c5     ...
           Normalized: M: d7d5 d7d6 e7e5 g7g6 c6c5 E: -0.42 -0.64 -0.44 -0.65 -0.73...
    Gen 2: Train_LP= -700.0, Ref_LP=  -34.5
           Text:                             M: d7d6 e7e5 d7d5 g7g6 c6c5     ...
           Normalized: M: d7d6 e7e5 d7d5 g7g6 c6c5 E: -0.67 -0.57 -0.43 -0.73 -0.7 ...
    Gen 3: Train_LP= -496.0, Ref_LP=  -34.2
           Text:                             M: e7e5 d7d6 e7e6 d7d5 g7g6     ...
           Normalized: M: e7e5 d7d6 e7e6 d7d5 g7g6 E: -0.51 -0.6 -0.78 -0.37 -0.76 ...
    Gen 4: Train_LP= -476.0, Ref_LP=  -25.1
           Text:                             M: d7d6 d7d5 d8c7 e7e5 g7g6     ...
           Normalized: M: d7d6 d7d5 d8c7 e7e5 g7g6 E: -0.65 -0.34 -0.7 -0.52 -0.7 B...

==================== PHASE 4: REWARD CALCULATION (step 32) ====================

üéØ Scoring Prompt 1 completions:
  Gen 1: Reward= 0.612 | M: d4d3 a6c5 c6f3 e7d7 c8b8 E: -3.44 -3....
  Gen 2: Reward= 0.216 | M: a6c5 c8b8 e7h4 c6d7 h8e8 E: -4.23 -4....
  Gen 3: Reward= 0.251 | M: e7d7 c8b8 h8e8 a6c5 d4d3 E: -3.94 -3....
  Gen 4: Reward= 0.253 | M: h8h4 a6c5 c8b8 e7d7 d4d3 E: -4.25 -3....
  üìä Average reward: 0.333

üéØ Scoring Prompt 2 completions:
  Gen 1: Reward=-0.035 | M: e2f1 c8e7 e2f2 e2d3 e2f3 E: 0.42 0.41...
  Gen 2: Reward= 0.479 | M: e2f2 e2d3 c8a7 c8e7 e2f3 E: 0.0 0.0 0...
  Gen 3: Reward= 0.299 | M: e2f1 e2f2 c8a7 e2f3 c8e7 E: 0.12 0.12...
  Gen 4: Reward=-0.015 | M: e2d3 e2f2 c8e7 e2f3 e2f1 E: 0.31 0.39...
  üìä Average reward: 0.182

üéØ Scoring Prompt 3 completions:
  Gen 1: Reward= 0.630 | 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b...
  Gen 2: Reward= 0.630 | 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b...
  Gen 3: Reward= 0.630 | 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b...
  Gen 4: Reward= 0.630 | 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b...
  üìä Average reward: 0.630

üéØ Scoring Prompt 4 completions:
  Gen 1: Reward= 0.713 | M: d7d5 d7d6 e7e5 g7g6 c6c5 E: -0.42 -0....
  Gen 2: Reward= 0.790 | M: d7d6 e7e5 d7d5 g7g6 c6c5 E: -0.67 -0....
  Gen 3: Reward= 0.371 | M: e7e5 d7d6 e7e6 d7d5 g7g6 E: -0.51 -0....
  Gen 4: Reward= 0.364 | M: d7d6 d7d5 d8c7 e7e5 g7g6 E: -0.65 -0....
  üìä Average reward: 0.560

==================== PHASE 5: TRL EXACT ADVANTAGE CALCULATION (step 32) ====================
üî¢ TRL Advantage Calculation (exact formula):
  Total rewards shape: torch.Size([16])
  Rewards: [ 0.6116      0.2158      0.25137143  0.25257143 -0.035       0.47857143
  0.29857143 -0.015       0.63        0.63        0.63        0.63
  0.7132      0.7904      0.37097143  0.36417143]

==================== PHASE 6-8: CORRECTED GRPO LOSS CALCULATION (step 32) ====================
üîÑ Computing GRPO loss with proper token-level KL...

üéØ Prompt 1 - Corrected GRPO Loss:
  Gen 1: A=+1.494, logp/len=-6.406, kl=-4.719
         PG=9.562, KL_penalty=-0.473, total=9.062
  Gen 2: A=-0.627, logp/len=-7.625, kl=-5.906
         PG=-4.781, KL_penalty=-0.590, total=-5.375
  Gen 3: A=-0.437, logp/len=-7.375, kl=-5.688
         PG=-3.219, KL_penalty=-0.570, total=-3.781
  Gen 4: A=-0.430, logp/len=-6.906, kl=-5.188
         PG=-2.969, KL_penalty=-0.520, total=-3.484
  üìä Prompt averages: PG=-0.352, KL=-0.539

üéØ Prompt 2 - Corrected GRPO Loss:
  Gen 1: A=-0.867, logp/len=-6.281, kl=-4.188
         PG=-5.438, KL_penalty=-0.418, total=-5.844
  Gen 2: A=+1.187, logp/len=-7.500, kl=-5.812
         PG=8.875, KL_penalty=-0.582, total=8.312
  Gen 3: A=+0.467, logp/len=-7.125, kl=-5.250
         PG=3.328, KL_penalty=-0.523, total=2.812
  Gen 4: A=-0.787, logp/len=-6.844, kl=-4.719
         PG=-5.375, KL_penalty=-0.473, total=-5.844
  üìä Prompt averages: PG=0.344, KL=-0.500

üéØ Prompt 3 - Corrected GRPO Loss:
  Gen 1: A=+0.000, logp/len=-5.406, kl=-5.031
         PG=0.000, KL_penalty=-0.504, total=-0.504
  Gen 2: A=+0.000, logp/len=-6.125, kl=-5.750
         PG=0.000, KL_penalty=-0.574, total=-0.574
  Gen 3: A=+0.000, logp/len=-5.000, kl=-4.625
         PG=0.000, KL_penalty=-0.463, total=-0.463
  Gen 4: A=+0.000, logp/len=-6.000, kl=-5.625
         PG=0.000, KL_penalty=-0.562, total=-0.562
  üìä Prompt averages: PG=0.000, KL=-0.523

üéØ Prompt 4 - Corrected GRPO Loss:
  Gen 1: A=+0.685, logp/len=-6.438, kl=-4.812
         PG=4.406, KL_penalty=-0.480, total=3.922
  Gen 2: A=+1.030, logp/len=-6.562, kl=-4.938
         PG=6.750, KL_penalty=-0.494, total=6.250
  Gen 3: A=-0.842, logp/len=-8.812, kl=-7.219
         PG=-7.406, KL_penalty=-0.723, total=-8.125
  Gen 4: A=-0.873, logp/len=-8.062, kl=-6.375
         PG=-7.031, KL_penalty=-0.637, total=-7.656
  üìä Prompt averages: PG=-0.828, KL=-0.586

üî¢ CORRECTED Loss Breakdown:
  Policy Gradient Loss:   -0.209
  KL Loss (with gradients):   -0.539
  Total Loss:          =  -0.750
  KL penalty ratio: 72.0%

üîÑ Performing corrected gradient update (step 32)...
  Total gradient norm: 20.70
  ‚úÖ Gradient update applied with proper KL regularization

==================== PHASE 9: POST-UPDATE PERFORMANCE (step 32) ====================
  Prompt 1: avg reward = 0.155
  Prompt 2: avg reward = 0.387
  Prompt 3: avg reward = 0.630
  Prompt 4: avg reward = 0.559
üìä POST-UPDATE (step 32) Performance:
  Average reward: 0.4329
  Positive ratio: 100.0%

==================== PHASE 10: ANALYSIS (step 32) ====================
üîç GRPO Step Impact (step 32):
  Post-update performance: 0.4329
  Step performance change: +0.0000

======================================================================
üß≠ SEQUENTIAL STEP 33/50
======================================================================
üßæ Overfit mode: reusing initial batch (no dataset slice advance)

ü§ñ Generating completions for GRPO training (step 33)...

üéØ Prompt 1: P: 2k4r/ppp1q1p1/nbb1N3/4PP2/3p2P1/PB5P/1PPK3R/3RQ3 b - - 3 ...
    Gen 1: Train_LP= -472.0, Ref_LP=  -44.8
           Text:                                M: e7d7 a6c5 c6d7 h8e8 d4d3  ...
           Normalized: M: e7d7 a6c5 c6d7 h8e8 d4d3 E: -4.09 -3.94 -4.15 -4.05 -4.21...
    Gen 2: Train_LP= -552.0, Ref_LP=  -34.0
           Text:                                M: a6c5 c8b8 h8h6 e7h4 d4d3  ...
           Normalized: M: a6c5 c8b8 h8h6 e7h4 d4d3 E: -3.51 -3.94 -4.02 -4.1 -3.94 ...
    Gen 3: Train_LP= -470.0, Ref_LP=  -46.0
           Text:                                M: a6c5 e7d7 c6d7 h8e8 e7e8  ...
           Normalized: M: a6c5 e7d7 c6d7 h8e8 e7e8 E: -3.76 -4.17 -3.82 -3.86 -4.09...
    Gen 4: Train_LP= -380.0, Ref_LP=  -32.2
           Text:                                M: d4d3 a6c5 e7d7 b6a5 c8b8  ...
           Normalized: M: d4d3 a6c5 e7d7 b6a5 c8b8 E: -3.65 -3.4 -3.85 -3.71 -3.72 ...

üéØ Prompt 2: P: 2N5/2k5/3p1p2/3P1BbP/4Pp2/1n6/4K3/8 w - - 9 49...
    Gen 1: Train_LP= -812.0, Ref_LP=  -14.6
           Text:                                             M: f5g6 e2f2 e2f...
           Normalized: M: f5g6 e2f2 e2f3 c8e7 c8a7 E: 0.0 0.0 0.0 0.0 0.0 B: e2f3...
    Gen 2: Train_LP= -568.0, Ref_LP=  -17.2
           Text:                                             M: f5h7 f5g4 e2f...
           Normalized: M: f5h7 f5g4 e2f3 f5g6 c8e7 E: 0.0 0.0 0.0 0.0 0.0 B: e2f3...
    Gen 3: Train_LP= -640.0, Ref_LP=  -27.5
           Text:                                             M: e2f2 e2f3 c8e...
           Normalized: M: e2f2 e2f3 c8e7 e2f1 c8a7 E: 0.02 0.0 0.0 0.0 0.0 B: e2f2...
    Gen 4: Train_LP= -394.0, Ref_LP=  -15.4
           Text:                                             M: f5g4 e2f2 c8e...
           Normalized: M: f5g4 e2f2 c8e7 e2f1 e2f3 E: 0.0 0.0 0.0 0.0 0.0 B: e2f2...

üéØ Prompt 3: A: 8/2k2b2/R1p4r/3p1p2/P7/2P2B1P/5PPK/4R3 w - - 1 31+a6a5+f7...
    Gen 1: Train_LP= -304.0, Ref_LP=   -0.0
           Text: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
           Normalized: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
    Gen 2: Train_LP= -306.0, Ref_LP=   -0.0
           Text: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
           Normalized: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
    Gen 3: Train_LP= -302.0, Ref_LP=   -0.0
           Text: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
           Normalized: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
    Gen 4: Train_LP= -376.0, Ref_LP=   -0.0
           Text: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
           Normalized: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...

üéØ Prompt 4: P: rnbqkbnr/pp1ppppp/2p5/8/4P3/2N5/PPPP1PPP/R1BQKBNR b KQkq ...
    Gen 1: Train_LP= -454.0, Ref_LP=  -23.8
           Text:                             M: d7d5 e7e5 d7d6 g7g6 c6c5     ...
           Normalized: M: d7d5 e7e5 d7d6 g7g6 c6c5 E: -0.45 -0.45 -0.68 -0.66 -0.78...
    Gen 2: Train_LP= -544.0, Ref_LP=  -24.2
           Text:                             M: d7d5 d7d6 g7g6 e7e5 e7e6     ...
           Normalized: M: d7d5 d7d6 g7g6 e7e5 e7e6 E: -0.38 -0.61 -0.76 -0.48 -0.76...
    Gen 3: Train_LP= -504.0, Ref_LP=  -24.0
           Text:                             M: d7d6 d7d5 e7e5 g7g6 e7e6     ...
           Normalized: M: d7d6 d7d5 e7e5 g7g6 e7e6 E: -0.71 -0.38 -0.49 -0.79 -0.74...
    Gen 4: Train_LP= -584.0, Ref_LP=  -24.6
           Text:                             M: d7d6 d7d5 e7e5 g7g6 c6c5     ...
           Normalized: M: d7d6 d7d5 e7e5 g7g6 c6c5 E: -0.67 -0.53 -0.58 -0.64 -0.71...

==================== PHASE 4: REWARD CALCULATION (step 33) ====================

üéØ Scoring Prompt 1 completions:
  Gen 1: Reward= 0.288 | M: e7d7 a6c5 c6d7 h8e8 d4d3 E: -4.09 -3....
  Gen 2: Reward= 0.273 | M: a6c5 c8b8 h8h6 e7h4 d4d3 E: -3.51 -3....
  Gen 3: Reward= 0.125 | M: a6c5 e7d7 c6d7 h8e8 e7e8 E: -3.76 -4....
  Gen 4: Reward= 0.320 | M: d4d3 a6c5 e7d7 b6a5 c8b8 E: -3.65 -3....
  üìä Average reward: 0.251

üéØ Scoring Prompt 2 completions:
  Gen 1: Reward=-0.001 | M: f5g6 e2f2 e2f3 c8e7 c8a7 E: 0.0 0.0 0...
  Gen 2: Reward=-0.117 | M: f5h7 f5g4 e2f3 f5g6 c8e7 E: 0.0 0.0 0...
  Gen 3: Reward= 0.329 | M: e2f2 e2f3 c8e7 e2f1 c8a7 E: 0.02 0.0 ...
  Gen 4: Reward= 0.295 | M: f5g4 e2f2 c8e7 e2f1 e2f3 E: 0.0 0.0 0...
  üìä Average reward: 0.126

üéØ Scoring Prompt 3 completions:
  Gen 1: Reward= 0.630 | 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b...
  Gen 2: Reward= 0.630 | 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b...
  Gen 3: Reward= 0.630 | 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b...
  Gen 4: Reward= 0.630 | 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b...
  üìä Average reward: 0.630

üéØ Scoring Prompt 4 completions:
  Gen 1: Reward= 0.711 | M: d7d5 e7e5 d7d6 g7g6 c6c5 E: -0.45 -0....
  Gen 2: Reward= 0.749 | M: d7d5 d7d6 g7g6 e7e5 e7e6 E: -0.38 -0....
  Gen 3: Reward= 0.715 | M: d7d6 d7d5 e7e5 g7g6 e7e6 E: -0.71 -0....
  Gen 4: Reward= 0.683 | M: d7d6 d7d5 e7e5 g7g6 c6c5 E: -0.67 -0....
  üìä Average reward: 0.714

==================== PHASE 5: TRL EXACT ADVANTAGE CALCULATION (step 33) ====================
üî¢ TRL Advantage Calculation (exact formula):
  Total rewards shape: torch.Size([16])
  Rewards: [ 0.2882      0.27257143  0.12493333  0.3196     -0.00142857 -0.11666667
  0.32857143  0.295       0.63        0.63        0.63        0.63
  0.7108      0.74897143  0.71497143  0.6828    ]

==================== PHASE 6-8: CORRECTED GRPO LOSS CALCULATION (step 33) ====================
üîÑ Computing GRPO loss with proper token-level KL...

üéØ Prompt 1 - Corrected GRPO Loss:
  Gen 1: A=+0.426, logp/len=-6.594, kl=-4.812
         PG=2.812, KL_penalty=-0.480, total=2.328
  Gen 2: A=+0.246, logp/len=-6.906, kl=-5.219
         PG=1.695, KL_penalty=-0.523, total=1.172
  Gen 3: A=-1.461, logp/len=-6.406, kl=-4.438
         PG=-9.375, KL_penalty=-0.443, total=-9.812
  Gen 4: A=+0.789, logp/len=-7.406, kl=-5.719
         PG=5.844, KL_penalty=-0.570, total=5.281
  üìä Prompt averages: PG=0.242, KL=-0.504

üéØ Prompt 2 - Corrected GRPO Loss:
  Gen 1: A=-0.582, logp/len=-6.562, kl=-5.000
         PG=-3.812, KL_penalty=-0.500, total=-4.312
  Gen 2: A=-1.107, logp/len=-7.688, kl=-6.062
         PG=-8.500, KL_penalty=-0.605, total=-9.125
  Gen 3: A=+0.921, logp/len=-5.281, kl=-3.641
         PG=4.875, KL_penalty=-0.363, total=4.500
  Gen 4: A=+0.768, logp/len=-6.875, kl=-5.062
         PG=5.281, KL_penalty=-0.508, total=4.781
  üìä Prompt averages: PG=-0.539, KL=-0.494

üéØ Prompt 3 - Corrected GRPO Loss:
  Gen 1: A=+0.000, logp/len=-6.250, kl=-5.875
         PG=0.000, KL_penalty=-0.586, total=-0.586
  Gen 2: A=+0.000, logp/len=-5.531, kl=-5.156
         PG=0.000, KL_penalty=-0.516, total=-0.516
  Gen 3: A=+0.000, logp/len=-6.094, kl=-5.719
         PG=0.000, KL_penalty=-0.570, total=-0.570
  Gen 4: A=+0.000, logp/len=-5.031, kl=-4.688
         PG=0.000, KL_penalty=-0.469, total=-0.469
  üìä Prompt averages: PG=0.000, KL=-0.535

üéØ Prompt 4 - Corrected GRPO Loss:
  Gen 1: A=-0.132, logp/len=-8.188, kl=-6.531
         PG=-1.086, KL_penalty=-0.652, total=-1.734
  Gen 2: A=+1.275, logp/len=-7.156, kl=-5.406
         PG=9.125, KL_penalty=-0.539, total=8.562
  Gen 3: A=+0.022, logp/len=-7.156, kl=-5.562
         PG=0.154, KL_penalty=-0.555, total=-0.400
  Gen 4: A=-1.165, logp/len=-7.406, kl=-5.812
         PG=-8.625, KL_penalty=-0.582, total=-9.188
  üìä Prompt averages: PG=-0.109, KL=-0.582

üî¢ CORRECTED Loss Breakdown:
  Policy Gradient Loss:   -0.102
  KL Loss (with gradients):   -0.527
  Total Loss:          =  -0.629
  KL penalty ratio: 84.0%

üîÑ Performing corrected gradient update (step 33)...
  Total gradient norm: 15.81
  ‚úÖ Gradient update applied with proper KL regularization

==================== PHASE 9: POST-UPDATE PERFORMANCE (step 33) ====================
  Prompt 1: avg reward = 0.155
  Prompt 2: avg reward = 0.387
  Prompt 3: avg reward = 0.630
  Prompt 4: avg reward = 0.559
üìä POST-UPDATE (step 33) Performance:
  Average reward: 0.4329
  Positive ratio: 100.0%

==================== PHASE 10: ANALYSIS (step 33) ====================
üîç GRPO Step Impact (step 33):
  Post-update performance: 0.4329
  Step performance change: +0.0000

======================================================================
üß≠ SEQUENTIAL STEP 34/50
======================================================================
üßæ Overfit mode: reusing initial batch (no dataset slice advance)

ü§ñ Generating completions for GRPO training (step 34)...

üéØ Prompt 1: P: 2k4r/ppp1q1p1/nbb1N3/4PP2/3p2P1/PB5P/1PPK3R/3RQ3 b - - 3 ...
    Gen 1: Train_LP= -628.0, Ref_LP=  -45.0
           Text:                                M: a6c5 d4d3 e7e8 c6d7 h8e8  ...
           Normalized: M: a6c5 d4d3 e7e8 c6d7 h8e8 E: -3.48 -3.54 -3.8 -3.56 -3.61 ...
    Gen 2: Train_LP= -608.0, Ref_LP=  -57.5
           Text:                                M: b6a5 h8h4 a6c5 c6f3 d4d3  ...
           Normalized: M: b6a5 h8h4 a6c5 c6f3 d4d3 E: -4.01 -3.71 -3.46 -3.93 -3.67...
    Gen 3: Train_LP= -416.0, Ref_LP=  -45.2
           Text:                                M: c8b8 a6c5 h8h4 h8e8 d4d3  ...
           Normalized: M: c8b8 a6c5 h8h4 h8e8 d4d3 E: -3.9 -3.56 -4.05 -3.85 -3.86 ...
    Gen 4: Train_LP= -478.0, Ref_LP=  -35.5
           Text:                                M: a6c5 e7d7 e7h4 c6d7 c8b8  ...
           Normalized: M: a6c5 e7d7 e7h4 c6d7 c8b8 E: -4.24 -4.4 -4.28 -4.5 -4.21 B...

üéØ Prompt 2: P: 2N5/2k5/3p1p2/3P1BbP/4Pp2/1n6/4K3/8 w - - 9 49...
    Gen 1: Train_LP= -724.0, Ref_LP=  -78.0
           Text:                                             M: e2f3 c8a7 e2f...
           Normalized: M: e2f3 c8a7 e2f2 c8e7 e2f1 E: 0.16 0.13 0.12 0.14 0.11 B: e...
    Gen 2: Train_LP= -416.0, Ref_LP=  -15.2
           Text:                                             M: e2d3 e2f3 e2f...
           Normalized: M: e2d3 e2f3 e2f2 c8e7 e2f1 E: 0.0 0.0 0.0 0.0 0.0 B: e2f2...
    Gen 3: Train_LP= -584.0, Ref_LP=  -32.5
           Text:                                             M: e2f3 e2f2 f5g...
           Normalized: M: e2f3 e2f2 f5g6 c8e7 e2d3 E: 0.17 0.0 0.0 0.0 0.0 B: e2f3...
    Gen 4: Train_LP= -588.0, Ref_LP=  -81.0
           Text:                                             M: e2f3 f5g6 c8a...
           Normalized: M: e2f3 f5g6 c8a7 e2d3 c8e7 E: 0.18 0.15 0.14 0.13 0.13 B: e...

üéØ Prompt 3: A: 8/2k2b2/R1p4r/3p1p2/P7/2P2B1P/5PPK/4R3 w - - 1 31+a6a5+f7...
    Gen 1: Train_LP= -248.0, Ref_LP=   -0.0
           Text: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
           Normalized: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
    Gen 2: Train_LP= -292.0, Ref_LP=   -0.0
           Text: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
           Normalized: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
    Gen 3: Train_LP= -298.0, Ref_LP=   -0.0
           Text: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
           Normalized: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
    Gen 4: Train_LP= -314.0, Ref_LP=   -0.0
           Text: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
           Normalized: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...

üéØ Prompt 4: P: rnbqkbnr/pp1ppppp/2p5/8/4P3/2N5/PPPP1PPP/R1BQKBNR b KQkq ...
    Gen 1: Train_LP= -600.0, Ref_LP=  -24.4
           Text:                             M: d7d6 g7g6 e7e5 d7d5 c6c5     ...
           Normalized: M: d7d6 g7g6 e7e5 d7d5 c6c5 E: -0.63 -0.69 -0.59 -0.46 -0.64...
    Gen 2: Train_LP= -652.0, Ref_LP=  -23.6
           Text:                             M: d7d6 e7e5 g7g6 d7d5 e7e6     ...
           Normalized: M: d7d6 e7e5 g7g6 d7d5 e7e6 E: -0.62 -0.51 -0.64 -0.43 -0.81...
    Gen 3: Train_LP= -540.0, Ref_LP=  -25.9
           Text:                             M: e7e6 e7e5 d7d6 d7d5 c6c5     ...
           Normalized: M: e7e6 e7e5 d7d6 d7d5 c6c5 E: -0.77 -0.74 -0.57 -0.38 -0.79...
    Gen 4: Train_LP= -608.0, Ref_LP=  -24.2
           Text:                             M: d7d5 d7d6 e7e5 e7e6 g7g6     ...
           Normalized: M: d7d5 d7d6 e7e5 e7e6 g7g6 E: -0.46 -0.64 -0.58 -0.82 -0.64...

==================== PHASE 4: REWARD CALCULATION (step 34) ====================

üéØ Scoring Prompt 1 completions:
  Gen 1: Reward= 0.125 | M: a6c5 d4d3 e7e8 c6d7 h8e8 E: -3.48 -3....
  Gen 2: Reward= 0.730 | M: b6a5 h8h4 a6c5 c6f3 d4d3 E: -4.01 -3....
  Gen 3: Reward= 0.227 | M: c8b8 a6c5 h8h4 h8e8 d4d3 E: -3.9 -3.5...
  Gen 4: Reward= 0.218 | M: a6c5 e7d7 e7h4 c6d7 c8b8 E: -4.24 -4....
  üìä Average reward: 0.325

üéØ Scoring Prompt 2 completions:
  Gen 1: Reward= 0.019 | M: e2f3 c8a7 e2f2 c8e7 e2f1 E: 0.16 0.13...
  Gen 2: Reward= 0.275 | M: e2d3 e2f3 e2f2 c8e7 e2f1 E: 0.0 0.0 0...
  Gen 3: Reward=-0.055 | M: e2f3 e2f2 f5g6 c8e7 e2d3 E: 0.17 0.0 ...
  Gen 4: Reward=-0.025 | M: e2f3 f5g6 c8a7 e2d3 c8e7 E: 0.18 0.15...
  üìä Average reward: 0.053

üéØ Scoring Prompt 3 completions:
  Gen 1: Reward= 0.630 | 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b...
  Gen 2: Reward= 0.630 | 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b...
  Gen 3: Reward= 0.630 | 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b...
  Gen 4: Reward= 0.630 | 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b...
  üìä Average reward: 0.630

üéØ Scoring Prompt 4 completions:
  Gen 1: Reward= 0.810 | M: d7d6 g7g6 e7e5 d7d5 c6c5 E: -0.63 -0....
  Gen 2: Reward= 0.367 | M: d7d6 e7e5 g7g6 d7d5 e7e6 E: -0.62 -0....
  Gen 3: Reward= 0.612 | M: e7e6 e7e5 d7d6 d7d5 c6c5 E: -0.77 -0....
  Gen 4: Reward= 0.396 | M: d7d5 d7d6 e7e5 e7e6 g7g6 E: -0.46 -0....
  üìä Average reward: 0.546

==================== PHASE 5: TRL EXACT ADVANTAGE CALCULATION (step 34) ====================
üî¢ TRL Advantage Calculation (exact formula):
  Total rewards shape: torch.Size([16])
  Rewards: [ 0.125       0.73        0.22737143  0.2182      0.01857143  0.275
 -0.055      -0.025       0.63        0.63        0.63        0.63
  0.81        0.36737143  0.61177143  0.39617143]

==================== PHASE 6-8: CORRECTED GRPO LOSS CALCULATION (step 34) ====================
üîÑ Computing GRPO loss with proper token-level KL...

üéØ Prompt 1 - Corrected GRPO Loss:
  Gen 1: A=-0.731, logp/len=-6.312, kl=-4.500
         PG=-4.625, KL_penalty=-0.449, total=-5.062
  Gen 2: A=+1.478, logp/len=-5.562, kl=-3.797
         PG=8.250, KL_penalty=-0.379, total=7.875
  Gen 3: A=-0.357, logp/len=-7.125, kl=-5.375
         PG=-2.547, KL_penalty=-0.539, total=-3.094
  Gen 4: A=-0.391, logp/len=-6.125, kl=-4.375
         PG=-2.391, KL_penalty=-0.438, total=-2.828
  üìä Prompt averages: PG=-0.328, KL=-0.451

üéØ Prompt 2 - Corrected GRPO Loss:
  Gen 1: A=-0.231, logp/len=-7.031, kl=-5.094
         PG=-1.625, KL_penalty=-0.508, total=-2.125
  Gen 2: A=+1.470, logp/len=-6.500, kl=-4.750
         PG=9.562, KL_penalty=-0.475, total=9.062
  Gen 3: A=-0.719, logp/len=-6.188, kl=-4.219
         PG=-4.438, KL_penalty=-0.422, total=-4.875
  Gen 4: A=-0.520, logp/len=-7.812, kl=-5.906
         PG=-4.062, KL_penalty=-0.590, total=-4.656
  üìä Prompt averages: PG=-0.141, KL=-0.500

üéØ Prompt 3 - Corrected GRPO Loss:
  Gen 1: A=+0.000, logp/len=-6.031, kl=-5.688
         PG=0.000, KL_penalty=-0.570, total=-0.570
  Gen 2: A=+0.000, logp/len=-4.875, kl=-4.500
         PG=0.000, KL_penalty=-0.449, total=-0.449
  Gen 3: A=+0.000, logp/len=-6.281, kl=-5.906
         PG=0.000, KL_penalty=-0.590, total=-0.590
  Gen 4: A=+0.000, logp/len=-6.125, kl=-5.750
         PG=0.000, KL_penalty=-0.574, total=-0.574
  üìä Prompt averages: PG=0.000, KL=-0.547

üéØ Prompt 4 - Corrected GRPO Loss:
  Gen 1: A=+1.275, logp/len=-6.688, kl=-4.969
         PG=8.500, KL_penalty=-0.496, total=8.000
  Gen 2: A=-0.865, logp/len=-6.875, kl=-5.219
         PG=-5.938, KL_penalty=-0.523, total=-6.469
  Gen 3: A=+0.316, logp/len=-5.844, kl=-3.984
         PG=1.852, KL_penalty=-0.398, total=1.453
  Gen 4: A=-0.726, logp/len=-7.406, kl=-5.781
         PG=-5.375, KL_penalty=-0.578, total=-5.938
  üìä Prompt averages: PG=-0.242, KL=-0.498

üî¢ CORRECTED Loss Breakdown:
  Policy Gradient Loss:   -0.178
  KL Loss (with gradients):   -0.500
  Total Loss:          =  -0.680
  KL penalty ratio: 73.5%

üîÑ Performing corrected gradient update (step 34)...
  Total gradient norm: 18.49
  ‚úÖ Gradient update applied with proper KL regularization

==================== PHASE 9: POST-UPDATE PERFORMANCE (step 34) ====================
  Prompt 1: avg reward = 0.155
  Prompt 2: avg reward = 0.387
  Prompt 3: avg reward = 0.630
  Prompt 4: avg reward = 0.559
üìä POST-UPDATE (step 34) Performance:
  Average reward: 0.4329
  Positive ratio: 100.0%

==================== PHASE 10: ANALYSIS (step 34) ====================
üîç GRPO Step Impact (step 34):
  Post-update performance: 0.4329
  Step performance change: +0.0000

======================================================================
üß≠ SEQUENTIAL STEP 35/50
======================================================================
üßæ Overfit mode: reusing initial batch (no dataset slice advance)

ü§ñ Generating completions for GRPO training (step 35)...

üéØ Prompt 1: P: 2k4r/ppp1q1p1/nbb1N3/4PP2/3p2P1/PB5P/1PPK3R/3RQ3 b - - 3 ...
    Gen 1: Train_LP= -872.0, Ref_LP=  -35.8
           Text:                                M: h8h4 a6c5 e7h4 c6f3 d4d3  ...
           Normalized: M: h8h4 a6c5 e7h4 c6f3 d4d3 E: -3.6 -3.6 -4.04 -3.86 -3.88 B...
    Gen 2: Train_LP= -496.0, Ref_LP=  -54.0
           Text:                                M: a6c5 c6d7 h8e8 c8b8 d4d3  ...
           Normalized: M: a6c5 c6d7 h8e8 c8b8 d4d3 E: -3.33 -3.76 -3.77 -3.76 -3.55...
    Gen 3: Train_LP= -600.0, Ref_LP=  -45.0
           Text:                                M: a6c5 c6f3 h8e8 b6a5 d4d3  ...
           Normalized: M: a6c5 c6f3 h8e8 b6a5 d4d3 E: -3.2 -3.52 -3.47 -3.52 -3.47 ...
    Gen 4: Train_LP= -628.0, Ref_LP=  -54.0
           Text:                                M: a6c5 c8b8 d4d3 e7d7 h8e8  ...
           Normalized: M: a6c5 c8b8 d4d3 e7d7 h8e8 E: -3.46 -3.45 -3.36 -3.64 -3.49...

üéØ Prompt 2: P: 2N5/2k5/3p1p2/3P1BbP/4Pp2/1n6/4K3/8 w - - 9 49...
    Gen 1: Train_LP= -396.0, Ref_LP=  -14.9
           Text:                                             M: e2d3 e2f2 e2f...
           Normalized: M: e2d3 e2f2 e2f1 c8e7 e2f3 E: 0.0 0.0 0.0 0.0 0.0 B: e2f1...
    Gen 2: Train_LP= -552.0, Ref_LP=  -69.0
           Text:                                             M: e2f3 f5g4 f5e...
           Normalized: M: e2f3 f5g4 f5e6 c8e7 e2f2 E: 0.31 0.31 0.31 0.31 0.3 B: f5...
    Gen 3: Train_LP= -458.0, Ref_LP=  -65.5
           Text:                                             M: e2f1 e2f2 c8e...
           Normalized: M: e2f1 e2f2 c8e7 c8a7 e2e1 E: 0.08 0.08 0.1 0.09 0.08 B: c8...
    Gen 4: Train_LP= -564.0, Ref_LP=  -13.8
           Text:                                             M: e2f2 c8a7 e2f...
           Normalized: M: e2f2 c8a7 e2f3 e2f1 c8e7 E: 0.0 0.0 0.0 0.0 0.0 B: e2f2...

üéØ Prompt 3: A: 8/2k2b2/R1p4r/3p1p2/P7/2P2B1P/5PPK/4R3 w - - 1 31+a6a5+f7...
    Gen 1: Train_LP= -304.0, Ref_LP=   -0.0
           Text: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
           Normalized: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
    Gen 2: Train_LP= -264.0, Ref_LP=   -0.0
           Text: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
           Normalized: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
    Gen 3: Train_LP= -280.0, Ref_LP=   -0.0
           Text: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
           Normalized: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
    Gen 4: Train_LP= -358.0, Ref_LP=   -0.0
           Text: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
           Normalized: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...

üéØ Prompt 4: P: rnbqkbnr/pp1ppppp/2p5/8/4P3/2N5/PPPP1PPP/R1BQKBNR b KQkq ...
    Gen 1: Train_LP= -548.0, Ref_LP=  -23.8
           Text:                             M: d7d6 g7g6 d7d5 e7e5 e7e6     ...
           Normalized: M: d7d6 g7g6 d7d5 e7e5 e7e6 E: -0.61 -0.7 -0.49 -0.62 -0.66 ...
    Gen 2: Train_LP= -378.0, Ref_LP=  -23.8
           Text:                             M: e7e6 e7e5 d7d6 d7d5 g7g6     ...
           Normalized: M: e7e6 e7e5 d7d6 d7d5 g7g6 E: -0.8 -0.62 -0.71 -0.51 -0.76 ...
    Gen 3: Train_LP= -494.0, Ref_LP=  -36.8
           Text:                             M: d7d6 d7d5 e7e5 c6c5 g7g6     ...
           Normalized: M: d7d6 d7d5 e7e5 c6c5 g7g6 E: -0.61 -0.52 -0.45 -0.59 -0.68...
    Gen 4: Train_LP= -400.0, Ref_LP=  -35.2
           Text:                             M: d7d5 e7e5 g7g6 d7d6 e7e6     ...
           Normalized: M: d7d5 e7e5 g7g6 d7d6 e7e6 E: -0.35 -0.46 -0.73 -0.66 -0.68...

==================== PHASE 4: REWARD CALCULATION (step 35) ====================

üéØ Scoring Prompt 1 completions:
  Gen 1: Reward= 0.213 | M: h8h4 a6c5 e7h4 c6f3 d4d3 E: -3.6 -3.6...
  Gen 2: Reward= 0.215 | M: a6c5 c6d7 h8e8 c8b8 d4d3 E: -3.33 -3....
  Gen 3: Reward= 0.540 | M: a6c5 c6f3 h8e8 b6a5 d4d3 E: -3.2 -3.5...
  Gen 4: Reward= 0.201 | M: a6c5 c8b8 d4d3 e7d7 h8e8 E: -3.46 -3....
  üìä Average reward: 0.292

üéØ Scoring Prompt 2 completions:
  Gen 1: Reward=-0.055 | M: e2d3 e2f2 e2f1 c8e7 e2f3 E: 0.0 0.0 0...
  Gen 2: Reward=-0.075 | M: e2f3 f5g4 f5e6 c8e7 e2f2 E: 0.31 0.31...
  Gen 3: Reward= 0.741 | M: e2f1 e2f2 c8e7 c8a7 e2e1 E: 0.08 0.08...
  Gen 4: Reward= 0.329 | M: e2f2 c8a7 e2f3 e2f1 c8e7 E: 0.0 0.0 0...
  üìä Average reward: 0.235

üéØ Scoring Prompt 3 completions:
  Gen 1: Reward= 0.630 | 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b...
  Gen 2: Reward= 0.630 | 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b...
  Gen 3: Reward= 0.630 | 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b...
  Gen 4: Reward= 0.630 | 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b...
  üìä Average reward: 0.630

üéØ Scoring Prompt 4 completions:
  Gen 1: Reward= 0.733 | M: d7d6 g7g6 d7d5 e7e5 e7e6 E: -0.61 -0....
  Gen 2: Reward= 0.365 | M: e7e6 e7e5 d7d6 d7d5 g7g6 E: -0.8 -0.6...
  Gen 3: Reward= 0.784 | M: d7d6 d7d5 e7e5 c6c5 g7g6 E: -0.61 -0....
  Gen 4: Reward= 0.741 | M: d7d5 e7e5 g7g6 d7d6 e7e6 E: -0.35 -0....
  üìä Average reward: 0.656

==================== PHASE 5: TRL EXACT ADVANTAGE CALCULATION (step 35) ====================
üî¢ TRL Advantage Calculation (exact formula):
  Total rewards shape: torch.Size([16])
  Rewards: [ 0.21297143  0.21497143  0.5396      0.20057143 -0.055      -0.075
  0.74057143  0.32857143  0.63        0.63        0.63        0.63
  0.73297143  0.36537143  0.7836      0.74057143]

==================== PHASE 6-8: CORRECTED GRPO LOSS CALCULATION (step 35) ====================
üîÑ Computing GRPO loss with proper token-level KL...

üéØ Prompt 1 - Corrected GRPO Loss:
  Gen 1: A=-0.479, logp/len=-7.188, kl=-5.438
         PG=-3.438, KL_penalty=-0.543, total=-3.984
  Gen 2: A=-0.467, logp/len=-6.875, kl=-5.188
         PG=-3.203, KL_penalty=-0.520, total=-3.719
  Gen 3: A=+1.499, logp/len=-6.750, kl=-5.031
         PG=10.125, KL_penalty=-0.504, total=9.625
  Gen 4: A=-0.554, logp/len=-7.406, kl=-5.719
         PG=-4.094, KL_penalty=-0.570, total=-4.656
  üìä Prompt averages: PG=-0.148, KL=-0.531

üéØ Prompt 2 - Corrected GRPO Loss:
  Gen 1: A=-0.753, logp/len=-8.312, kl=-6.625
         PG=-6.250, KL_penalty=-0.664, total=-6.906
  Gen 2: A=-0.805, logp/len=-7.188, kl=-5.219
         PG=-5.781, KL_penalty=-0.523, total=-6.312
  Gen 3: A=+1.314, logp/len=-5.344, kl=-3.422
         PG=7.031, KL_penalty=-0.342, total=6.688
  Gen 4: A=+0.244, logp/len=-8.688, kl=-7.125
         PG=2.109, KL_penalty=-0.711, total=1.398
  üìä Prompt averages: PG=-0.715, KL=-0.562

üéØ Prompt 3 - Corrected GRPO Loss:
  Gen 1: A=+0.000, logp/len=-5.469, kl=-5.125
         PG=0.000, KL_penalty=-0.512, total=-0.512
  Gen 2: A=+0.000, logp/len=-5.156, kl=-4.781
         PG=0.000, KL_penalty=-0.479, total=-0.479
  Gen 3: A=+0.000, logp/len=-5.281, kl=-4.906
         PG=0.000, KL_penalty=-0.490, total=-0.490
  Gen 4: A=+0.000, logp/len=-5.312, kl=-4.938
         PG=0.000, KL_penalty=-0.494, total=-0.494
  üìä Prompt averages: PG=0.000, KL=-0.494

üéØ Prompt 4 - Corrected GRPO Loss:
  Gen 1: A=+0.397, logp/len=-7.375, kl=-5.625
         PG=2.922, KL_penalty=-0.562, total=2.359
  Gen 2: A=-1.490, logp/len=-6.906, kl=-5.312
         PG=-10.312, KL_penalty=-0.531, total=-10.875
  Gen 3: A=+0.657, logp/len=-7.125, kl=-5.438
         PG=4.688, KL_penalty=-0.543, total=4.156
  Gen 4: A=+0.436, logp/len=-6.312, kl=-4.625
         PG=2.750, KL_penalty=-0.463, total=2.281
  üìä Prompt averages: PG=0.016, KL=-0.527

üî¢ CORRECTED Loss Breakdown:
  Policy Gradient Loss:   -0.212
  KL Loss (with gradients):   -0.527
  Total Loss:          =  -0.738
  KL penalty ratio: 71.5%

üîÑ Performing corrected gradient update (step 35)...
  Total gradient norm: 19.95
  ‚úÖ Gradient update applied with proper KL regularization

==================== PHASE 9: POST-UPDATE PERFORMANCE (step 35) ====================
  Prompt 1: avg reward = 0.155
  Prompt 2: avg reward = 0.212
  Prompt 3: avg reward = 0.630
  Prompt 4: avg reward = 0.505
üìä POST-UPDATE (step 35) Performance:
  Average reward: 0.3756
  Positive ratio: 87.5%

==================== PHASE 10: ANALYSIS (step 35) ====================
üîç GRPO Step Impact (step 35):
  Post-update performance: 0.3756
  Step performance change: -0.0574

======================================================================
üß≠ SEQUENTIAL STEP 36/50
======================================================================
üßæ Overfit mode: reusing initial batch (no dataset slice advance)

ü§ñ Generating completions for GRPO training (step 36)...

üéØ Prompt 1: P: 2k4r/ppp1q1p1/nbb1N3/4PP2/3p2P1/PB5P/1PPK3R/3RQ3 b - - 3 ...
    Gen 1: Train_LP= -540.0, Ref_LP=  -45.8
           Text:                                M: a6c5 h8h4 d4d3 e7d7 b6a5  ...
           Normalized: M: a6c5 h8h4 d4d3 e7d7 b6a5 E: -3.24 -3.53 -3.16 -3.44 -3.58...
    Gen 2: Train_LP= -506.0, Ref_LP=  -31.8
           Text:                                M: d4d3 b6a5 a6c5 c8b8 h8e8  ...
           Normalized: M: d4d3 b6a5 a6c5 c8b8 h8e8 E: -3.66 -3.66 -3.61 -3.7 -3.89 ...
    Gen 3: Train_LP= -408.0, Ref_LP=  -44.5
           Text:                                M: c8b8 a6c5 h8e8 c6f3 d4d3  ...
           Normalized: M: c8b8 a6c5 h8e8 c6f3 d4d3 E: -3.72 -3.42 -3.56 -3.57 -3.73...
    Gen 4: Train_LP= -498.0, Ref_LP=  -45.2
           Text:                                M: d4d3 a6c5 e7d7 e7h4 c6f3  ...
           Normalized: M: d4d3 a6c5 e7d7 e7h4 c6f3 E: -4.16 -3.88 -4.34 -4.24 -3.89...

üéØ Prompt 2: P: 2N5/2k5/3p1p2/3P1BbP/4Pp2/1n6/4K3/8 w - - 9 49...
    Gen 1: Train_LP= -536.0, Ref_LP=  -34.8
           Text:                                             M: e2f3 e2f2 c8e...
           Normalized: M: e2f3 e2f2 c8e7 e2e1 e2f1 E: 0.45 0.45 0.45 0.45 0.45 B: e...
    Gen 2: Train_LP= -372.0, Ref_LP=  -24.5
           Text:                                             M: e2f2 c8a7 e2f...
           Normalized: M: e2f2 c8a7 e2f1 c8e7 e2f3 E: 0.22 0.21 0.2 0.26 0.19 B: c8...
    Gen 3: Train_LP= -540.0, Ref_LP=  -27.8
           Text:                                             M: e2d3 c8e7 e2f...
           Normalized: M: e2d3 c8e7 e2f2 e2f3 f5g6 E: 0.41 0.46 0.4 0.43 0.42 B: c8...
    Gen 4: Train_LP= -318.0, Ref_LP=  -36.5
           Text:                                             M: e2f2 e2d3 e2f...
           Normalized: M: e2f2 e2d3 e2f3 c8e7 e2f1 E: 0.17 0.16 0.17 0.18 0.15 B: e...

üéØ Prompt 3: A: 8/2k2b2/R1p4r/3p1p2/P7/2P2B1P/5PPK/4R3 w - - 1 31+a6a5+f7...
    Gen 1: Train_LP= -302.0, Ref_LP=   -0.0
           Text: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
           Normalized: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
    Gen 2: Train_LP= -272.0, Ref_LP=   -0.0
           Text: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
           Normalized: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
    Gen 3: Train_LP= -314.0, Ref_LP=   -0.0
           Text: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
           Normalized: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
    Gen 4: Train_LP= -268.0, Ref_LP=   -0.0
           Text: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
           Normalized: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...

üéØ Prompt 4: P: rnbqkbnr/pp1ppppp/2p5/8/4P3/2N5/PPPP1PPP/R1BQKBNR b KQkq ...
    Gen 1: Train_LP= -560.0, Ref_LP=  -46.0
           Text:                             M: e7e6 e7e5 d7d6 d7d5 g7g6     ...
           Normalized: M: e7e6 e7e5 d7d6 d7d5 g7g6 E: -0.88 -0.57 -0.57 -0.49 -0.65...
    Gen 2: Train_LP= -498.0, Ref_LP=  -24.2
           Text:                             M: e7e5 d7d6 d7d5 g7g6 c6c5     ...
           Normalized: M: e7e5 d7d6 d7d5 g7g6 c6c5 E: -0.5 -0.68 -0.45 -0.79 -0.7 B...
    Gen 3: Train_LP= -350.0, Ref_LP=  -46.0
           Text:                             M: d7d5 d7d6 d8c7 e7e5 g7g6     ...
           Normalized: M: d7d5 d7d6 d8c7 e7e5 g7g6 E: -0.53 -0.73 -0.82 -0.58 -0.76...
    Gen 4: Train_LP= -604.0, Ref_LP=  -45.2
           Text:                             M: d7d6 d7d5 e7e5 e7e6 g7g6     ...
           Normalized: M: d7d6 d7d5 e7e5 e7e6 g7g6 E: -0.63 -0.39 -0.62 -0.92 -0.84...

==================== PHASE 4: REWARD CALCULATION (step 36) ====================

üéØ Scoring Prompt 1 completions:
  Gen 1: Reward= 0.629 | M: a6c5 h8h4 d4d3 e7d7 b6a5 E: -3.24 -3....
  Gen 2: Reward= 0.304 | M: d4d3 b6a5 a6c5 c8b8 h8e8 E: -3.66 -3....
  Gen 3: Reward= 0.408 | M: c8b8 a6c5 h8e8 c6f3 d4d3 E: -3.72 -3....
  Gen 4: Reward= 0.322 | M: d4d3 a6c5 e7d7 e7h4 c6f3 E: -4.16 -3....
  üìä Average reward: 0.415

üéØ Scoring Prompt 2 completions:
  Gen 1: Reward=-0.015 | M: e2f3 e2f2 c8e7 e2e1 e2f1 E: 0.45 0.45...
  Gen 2: Reward= 0.469 | M: e2f2 c8a7 e2f1 c8e7 e2f3 E: 0.22 0.21...
  Gen 3: Reward= 0.415 | M: e2d3 c8e7 e2f2 e2f3 f5g6 E: 0.41 0.46...
  Gen 4: Reward=-0.035 | M: e2f2 e2d3 e2f3 c8e7 e2f1 E: 0.17 0.16...
  üìä Average reward: 0.208

üéØ Scoring Prompt 3 completions:
  Gen 1: Reward= 0.630 | 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b...
  Gen 2: Reward= 0.630 | 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b...
  Gen 3: Reward= 0.630 | 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b...
  Gen 4: Reward= 0.630 | 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b...
  üìä Average reward: 0.630

üéØ Scoring Prompt 4 completions:
  Gen 1: Reward= 0.366 | M: e7e6 e7e5 d7d6 d7d5 g7g6 E: -0.88 -0....
  Gen 2: Reward= 0.790 | M: e7e5 d7d6 d7d5 g7g6 c6c5 E: -0.5 -0.6...
  Gen 3: Reward= 0.393 | M: d7d5 d7d6 d8c7 e7e5 g7g6 E: -0.53 -0....
  Gen 4: Reward= 0.363 | M: d7d6 d7d5 e7e5 e7e6 g7g6 E: -0.63 -0....
  üìä Average reward: 0.478

==================== PHASE 5: TRL EXACT ADVANTAGE CALCULATION (step 36) ====================
üî¢ TRL Advantage Calculation (exact formula):
  Total rewards shape: torch.Size([16])
  Rewards: [ 0.62857143  0.3036      0.4076      0.32177143 -0.015       0.46857143
  0.415      -0.035       0.63        0.63        0.63        0.63
  0.36617143  0.7904      0.39297143  0.36297143]

==================== PHASE 6-8: CORRECTED GRPO LOSS CALCULATION (step 36) ====================
üîÑ Computing GRPO loss with proper token-level KL...

üéØ Prompt 1 - Corrected GRPO Loss:
  Gen 1: A=+1.429, logp/len=-6.750, kl=-5.000
         PG=9.625, KL_penalty=-0.500, total=9.125
  Gen 2: A=-0.749, logp/len=-6.438, kl=-4.812
         PG=-4.812, KL_penalty=-0.480, total=-5.281
  Gen 3: A=-0.052, logp/len=-7.188, kl=-5.438
         PG=-0.375, KL_penalty=-0.543, total=-0.918
  Gen 4: A=-0.628, logp/len=-5.562, kl=-3.766
         PG=-3.484, KL_penalty=-0.377, total=-3.859
  üìä Prompt averages: PG=0.238, KL=-0.475

üéØ Prompt 2 - Corrected GRPO Loss:
  Gen 1: A=-0.826, logp/len=-7.719, kl=-5.844
         PG=-6.375, KL_penalty=-0.586, total=-6.969
  Gen 2: A=+0.962, logp/len=-7.375, kl=-5.406
         PG=7.094, KL_penalty=-0.539, total=6.562
  Gen 3: A=+0.764, logp/len=-7.281, kl=-5.375
         PG=5.562, KL_penalty=-0.539, total=5.031
  Gen 4: A=-0.900, logp/len=-7.188, kl=-5.250
         PG=-6.469, KL_penalty=-0.523, total=-7.000
  üìä Prompt averages: PG=-0.047, KL=-0.547

üéØ Prompt 3 - Corrected GRPO Loss:
  Gen 1: A=+0.000, logp/len=-6.375, kl=-6.000
         PG=0.000, KL_penalty=-0.602, total=-0.602
  Gen 2: A=+0.000, logp/len=-6.406, kl=-6.031
         PG=0.000, KL_penalty=-0.602, total=-0.602
  Gen 3: A=+0.000, logp/len=-5.031, kl=-4.656
         PG=0.000, KL_penalty=-0.465, total=-0.465
  Gen 4: A=+0.000, logp/len=-6.812, kl=-6.406
         PG=0.000, KL_penalty=-0.641, total=-0.641
  üìä Prompt averages: PG=0.000, KL=-0.578

üéØ Prompt 4 - Corrected GRPO Loss:
  Gen 1: A=-0.537, logp/len=-7.281, kl=-5.656
         PG=-3.906, KL_penalty=-0.566, total=-4.469
  Gen 2: A=+1.497, logp/len=-6.750, kl=-5.094
         PG=10.125, KL_penalty=-0.508, total=9.625
  Gen 3: A=-0.408, logp/len=-7.625, kl=-5.969
         PG=-3.109, KL_penalty=-0.598, total=-3.703
  Gen 4: A=-0.552, logp/len=-9.125, kl=-7.531
         PG=-5.031, KL_penalty=-0.754, total=-5.781
  üìä Prompt averages: PG=-0.480, KL=-0.605

üî¢ CORRECTED Loss Breakdown:
  Policy Gradient Loss:   -0.072
  KL Loss (with gradients):   -0.551
  Total Loss:          =  -0.625
  KL penalty ratio: 88.5%

üîÑ Performing corrected gradient update (step 36)...
  Total gradient norm: 18.96
  ‚úÖ Gradient update applied with proper KL regularization

==================== PHASE 9: POST-UPDATE PERFORMANCE (step 36) ====================
  Prompt 1: avg reward = 0.155
  Prompt 2: avg reward = 0.387
  Prompt 3: avg reward = 0.630
  Prompt 4: avg reward = 0.397
üìä POST-UPDATE (step 36) Performance:
  Average reward: 0.3922
  Positive ratio: 100.0%

==================== PHASE 10: ANALYSIS (step 36) ====================
üîç GRPO Step Impact (step 36):
  Post-update performance: 0.3922
  Step performance change: +0.0166

======================================================================
üß≠ SEQUENTIAL STEP 37/50
======================================================================
üßæ Overfit mode: reusing initial batch (no dataset slice advance)

ü§ñ Generating completions for GRPO training (step 37)...

üéØ Prompt 1: P: 2k4r/ppp1q1p1/nbb1N3/4PP2/3p2P1/PB5P/1PPK3R/3RQ3 b - - 3 ...
    Gen 1: Train_LP= -652.0, Ref_LP=  -45.0
           Text:                                M: e7e8 a6c5 b6a5 e7d7 c6f3  ...
           Normalized: M: e7e8 a6c5 b6a5 e7d7 c6f3 E: -3.68 -3.68 -3.69 -3.69 -3.78...
    Gen 2: Train_LP= -612.0, Ref_LP=  -35.8
           Text:                                M: c6d7 h8h4 a6c5 d4d3 h8e8  ...
           Normalized: M: c6d7 h8h4 a6c5 d4d3 h8e8 E: -3.89 -3.71 -3.39 -3.7 -3.82 ...
    Gen 3: Train_LP= -644.0, Ref_LP=  -45.0
           Text:                                M: a6c5 d4d3 e7d7 c6f3 b6a5  ...
           Normalized: M: a6c5 d4d3 e7d7 c6f3 b6a5 E: -3.39 -3.55 -3.77 -3.72 -3.58...
    Gen 4: Train_LP= -604.0, Ref_LP=  -46.0
           Text:                                M: h8h4 e7e8 c8b8 d4d3 a6c5  ...
           Normalized: M: h8h4 e7e8 c8b8 d4d3 a6c5 E: -3.86 -3.83 -3.59 -3.83 -3.42...

üéØ Prompt 2: P: 2N5/2k5/3p1p2/3P1BbP/4Pp2/1n6/4K3/8 w - - 9 49...
    Gen 1: Train_LP= -512.0, Ref_LP=  -78.0
           Text:                                             M: e2f2 c8e7 e2f...
           Normalized: M: e2f2 c8e7 e2f1 e2f3 e2e1 E: 0.22 0.24 0.23 0.24 0.23 B: e...
    Gen 2: Train_LP= -426.0, Ref_LP=  -78.5
           Text:                                             M: e2f2 c8a7 e2f...
           Normalized: M: e2f2 c8a7 e2f3 e2f1 c8e7 E: 0.09 0.14 0.17 0.09 0.16 B: e...
    Gen 3: Train_LP= -490.0, Ref_LP=  -16.6
           Text:                                             M: c8a7 e2d1 e2f...
           Normalized: M: c8a7 e2d1 e2f3 c8e7 e2f1 E: 0.0 0.0 0.0 0.0 0.0 B: e2f1...
    Gen 4: Train_LP= -422.0, Ref_LP=  -13.8
           Text:                                             M: e2f2 e2f1 c8a...
           Normalized: M: e2f2 e2f1 c8a7 c8e7 e2f3 E: 0.0 0.0 0.0 0.0 0.0 B: c8a7...

üéØ Prompt 3: A: 8/2k2b2/R1p4r/3p1p2/P7/2P2B1P/5PPK/4R3 w - - 1 31+a6a5+f7...
    Gen 1: Train_LP= -314.0, Ref_LP=   -0.0
           Text: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
           Normalized: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
    Gen 2: Train_LP= -320.0, Ref_LP=   -0.0
           Text: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
           Normalized: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
    Gen 3: Train_LP= -332.0, Ref_LP=   -0.0
           Text: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
           Normalized: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
    Gen 4: Train_LP= -304.0, Ref_LP=   -0.0
           Text: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
           Normalized: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...

üéØ Prompt 4: P: rnbqkbnr/pp1ppppp/2p5/8/4P3/2N5/PPPP1PPP/R1BQKBNR b KQkq ...
    Gen 1: Train_LP= -396.0, Ref_LP=  -34.8
           Text:                             M: e7e5 g7g6 d7d6 d7d5 c6c5     ...
           Normalized: M: e7e5 g7g6 d7d6 d7d5 c6c5 E: -0.54 -0.74 -0.53 -0.51 -0.73...
    Gen 2: Train_LP= -434.0, Ref_LP=  -35.5
           Text:                             M: d7d5 g7g6 d7d6 e7e5 d8c7     ...
           Normalized: M: d7d5 g7g6 d7d6 e7e5 d8c7 E: -0.41 -0.76 -0.59 -0.53 -0.84...
    Gen 3: Train_LP= -486.0, Ref_LP=  -34.2
           Text:                             M: d7d5 d7d6 e7e5 g7g6 e7e6     ...
           Normalized: M: d7d5 d7d6 e7e5 g7g6 e7e6 E: -0.36 -0.68 -0.56 -0.73 -0.78...
    Gen 4: Train_LP= -608.0, Ref_LP=  -24.5
           Text:                             M: e7e5 d7d5 d7d6 c6c5 g7g6     ...
           Normalized: M: e7e5 d7d5 d7d6 c6c5 g7g6 E: -0.62 -0.32 -0.59 -0.7 -0.65 ...

==================== PHASE 4: REWARD CALCULATION (step 37) ====================

üéØ Scoring Prompt 1 completions:
  Gen 1: Reward= 0.490 | M: e7e8 a6c5 b6a5 e7d7 c6f3 E: -3.68 -3....
  Gen 2: Reward= 0.125 | M: c6d7 h8h4 a6c5 d4d3 h8e8 E: -3.89 -3....
  Gen 3: Reward= 0.321 | M: a6c5 d4d3 e7d7 c6f3 b6a5 E: -3.39 -3....
  Gen 4: Reward= 0.179 | M: h8h4 e7e8 c8b8 d4d3 a6c5 E: -3.86 -3....
  üìä Average reward: 0.279

üéØ Scoring Prompt 2 completions:
  Gen 1: Reward=-0.035 | M: e2f2 c8e7 e2f1 e2f3 e2e1 E: 0.22 0.24...
  Gen 2: Reward= 0.019 | M: e2f2 c8a7 e2f3 e2f1 c8e7 E: 0.09 0.14...
  Gen 3: Reward=-0.021 | M: c8a7 e2d1 e2f3 c8e7 e2f1 E: 0.0 0.0 0...
  Gen 4: Reward= 0.239 | M: e2f2 e2f1 c8a7 c8e7 e2f3 E: 0.0 0.0 0...
  üìä Average reward: 0.050

üéØ Scoring Prompt 3 completions:
  Gen 1: Reward= 0.630 | 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b...
  Gen 2: Reward= 0.630 | 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b...
  Gen 3: Reward= 0.630 | 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b...
  Gen 4: Reward= 0.630 | 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b...
  üìä Average reward: 0.630

üéØ Scoring Prompt 4 completions:
  Gen 1: Reward= 0.812 | M: e7e5 g7g6 d7d6 d7d5 c6c5 E: -0.54 -0....
  Gen 2: Reward= 0.423 | M: d7d5 g7g6 d7d6 e7e5 d8c7 E: -0.41 -0....
  Gen 3: Reward= 0.397 | M: d7d5 d7d6 e7e5 g7g6 e7e6 E: -0.36 -0....
  Gen 4: Reward= 0.684 | M: e7e5 d7d5 d7d6 c6c5 g7g6 E: -0.62 -0....
  üìä Average reward: 0.579

==================== PHASE 5: TRL EXACT ADVANTAGE CALCULATION (step 37) ====================
üî¢ TRL Advantage Calculation (exact formula):
  Total rewards shape: torch.Size([16])
  Rewards: [ 0.48977143  0.125       0.3208      0.17857143 -0.035       0.01857143
 -0.02142857  0.23857143  0.63        0.63        0.63        0.63
  0.8116      0.42297143  0.39737143  0.684     ]

==================== PHASE 6-8: CORRECTED GRPO LOSS CALCULATION (step 37) ====================
üîÑ Computing GRPO loss with proper token-level KL...

üéØ Prompt 1 - Corrected GRPO Loss:
  Gen 1: A=+1.294, logp/len=-6.156, kl=-4.438
         PG=7.969, KL_penalty=-0.443, total=7.531
  Gen 2: A=-0.940, logp/len=-6.281, kl=-4.531
         PG=-5.906, KL_penalty=-0.453, total=-6.375
  Gen 3: A=+0.259, logp/len=-6.625, kl=-4.938
         PG=1.719, KL_penalty=-0.494, total=1.227
  Gen 4: A=-0.612, logp/len=-6.625, kl=-4.875
         PG=-4.062, KL_penalty=-0.488, total=-4.562
  üìä Prompt averages: PG=-0.070, KL=-0.469

üéØ Prompt 2 - Corrected GRPO Loss:
  Gen 1: A=-0.667, logp/len=-7.812, kl=-5.906
         PG=-5.219, KL_penalty=-0.590, total=-5.812
  Gen 2: A=-0.248, logp/len=-5.906, kl=-4.125
         PG=-1.461, KL_penalty=-0.412, total=-1.875
  Gen 3: A=-0.561, logp/len=-6.844, kl=-5.094
         PG=-3.844, KL_penalty=-0.508, total=-4.344
  Gen 4: A=+1.476, logp/len=-8.062, kl=-6.438
         PG=11.875, KL_penalty=-0.645, total=11.250
  üìä Prompt averages: PG=0.344, KL=-0.539

üéØ Prompt 3 - Corrected GRPO Loss:
  Gen 1: A=+0.000, logp/len=-5.438, kl=-5.094
         PG=0.000, KL_penalty=-0.508, total=-0.508
  Gen 2: A=+0.000, logp/len=-6.312, kl=-5.969
         PG=0.000, KL_penalty=-0.598, total=-0.598
  Gen 3: A=+0.000, logp/len=-6.281, kl=-5.906
         PG=0.000, KL_penalty=-0.590, total=-0.590
  Gen 4: A=+0.000, logp/len=-6.125, kl=-5.750
         PG=0.000, KL_penalty=-0.574, total=-0.574
  üìä Prompt averages: PG=0.000, KL=-0.570

üéØ Prompt 4 - Corrected GRPO Loss:
  Gen 1: A=+1.151, logp/len=-7.438, kl=-5.656
         PG=8.562, KL_penalty=-0.566, total=8.000
  Gen 2: A=-0.772, logp/len=-6.406, kl=-4.719
         PG=-4.938, KL_penalty=-0.473, total=-5.406
  Gen 3: A=-0.899, logp/len=-7.969, kl=-6.312
         PG=-7.156, KL_penalty=-0.633, total=-7.781
  Gen 4: A=+0.520, logp/len=-8.250, kl=-6.562
         PG=4.281, KL_penalty=-0.656, total=3.625
  üìä Prompt averages: PG=0.188, KL=-0.582

üî¢ CORRECTED Loss Breakdown:
  Policy Gradient Loss:    0.115
  KL Loss (with gradients):   -0.539
  Total Loss:          =  -0.424
  KL penalty ratio: 82.0%

üîÑ Performing corrected gradient update (step 37)...
  Total gradient norm: 31.16
  ‚úÖ Gradient update applied with proper KL regularization

==================== PHASE 9: POST-UPDATE PERFORMANCE (step 37) ====================
  Prompt 1: avg reward = 0.152
  Prompt 2: avg reward = 0.387
  Prompt 3: avg reward = 0.630
  Prompt 4: avg reward = 0.505
üìä POST-UPDATE (step 37) Performance:
  Average reward: 0.4185
  Positive ratio: 100.0%

==================== PHASE 10: ANALYSIS (step 37) ====================
üîç GRPO Step Impact (step 37):
  Post-update performance: 0.4185
  Step performance change: +0.0263

======================================================================
üß≠ SEQUENTIAL STEP 38/50
======================================================================
üßæ Overfit mode: reusing initial batch (no dataset slice advance)

ü§ñ Generating completions for GRPO training (step 38)...

üéØ Prompt 1: P: 2k4r/ppp1q1p1/nbb1N3/4PP2/3p2P1/PB5P/1PPK3R/3RQ3 b - - 3 ...
    Gen 1: Train_LP= -266.0, Ref_LP=  -43.8
           Text:                                M: h8e8 d4d3 c6d7 a6c5 c8b8  ...
           Normalized: M: h8e8 d4d3 c6d7 a6c5 c8b8 E: -3.97 -3.94 -3.88 -3.88 -3.94...
    Gen 2: Train_LP= -504.0, Ref_LP=  -35.5
           Text:                                M: c6f3 d4d3 a6c5 e7e8 h8e8  ...
           Normalized: M: c6f3 d4d3 a6c5 e7e8 h8e8 E: -3.92 -3.78 -3.88 -4.2 -4.19 ...
    Gen 3: Train_LP= -422.0, Ref_LP=  -34.8
           Text:                                M: a6c5 b6a5 e7h4 c6f3 c8b8  ...
           Normalized: M: a6c5 b6a5 e7h4 c6f3 c8b8 E: -3.7 -3.93 -4.16 -3.98 -4.07 ...
    Gen 4: Train_LP= -528.0, Ref_LP=  -37.5
           Text:                                M: h8h4 a6c5 e7d7 c6b5 h8e8  ...
           Normalized: M: h8h4 a6c5 e7d7 c6b5 h8e8 E: -4.03 -3.5 -4.19 -3.98 -4.19 ...

üéØ Prompt 2: P: 2N5/2k5/3p1p2/3P1BbP/4Pp2/1n6/4K3/8 w - - 9 49...
    Gen 1: Train_LP= -422.0, Ref_LP=  -15.0
           Text:                                             M: e2f2 e2f1 e2d...
           Normalized: M: e2f2 e2f1 e2d3 c8e7 e2f3 E: 0.0 0.0 0.0 0.0 0.0 B: e2f3...
    Gen 2: Train_LP= -752.0, Ref_LP=  -78.0
           Text:                                             M: f5g6 e2f2 e2f...
           Normalized: M: f5g6 e2f2 e2f3 c8e7 c8a7 E: 0.25 0.25 0.24 0.27 0.24 B: c...
    Gen 3: Train_LP= -564.0, Ref_LP=  -15.4
           Text:                                             M: e2f2 c8e7 f5g...
           Normalized: M: e2f2 c8e7 f5g4 e2f3 c8a7 E: 0.0 0.0 0.0 0.0 0.0 B: e2f2...
    Gen 4: Train_LP= -444.0, Ref_LP=  -16.4
           Text:                                             M: e2d3 e2f2 c8e...
           Normalized: M: e2d3 e2f2 c8e7 e2f3 f5g4 E: 0.0 0.0 0.0 0.0 0.0 B: e2d3...

üéØ Prompt 3: A: 8/2k2b2/R1p4r/3p1p2/P7/2P2B1P/5PPK/4R3 w - - 1 31+a6a5+f7...
    Gen 1: Train_LP= -338.0, Ref_LP=   -0.0
           Text: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
           Normalized: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
    Gen 2: Train_LP= -256.0, Ref_LP=   -0.0
           Text: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
           Normalized: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
    Gen 3: Train_LP= -332.0, Ref_LP=   -0.0
           Text: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
           Normalized: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
    Gen 4: Train_LP= -318.0, Ref_LP=   -0.0
           Text: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
           Normalized: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...

üéØ Prompt 4: P: rnbqkbnr/pp1ppppp/2p5/8/4P3/2N5/PPPP1PPP/R1BQKBNR b KQkq ...
    Gen 1: Train_LP= -490.0, Ref_LP=  -35.0
           Text:                             M: d7d6 e7e5 d7d5 g7g6 d8c7     ...
           Normalized: M: d7d6 e7e5 d7d5 g7g6 d8c7 E: -0.58 -0.51 -0.37 -0.7 -0.82 ...
    Gen 2: Train_LP= -442.0, Ref_LP=  -45.0
           Text:                             M: e7e5 d7d5 d7d6 g7g6 e7e6     ...
           Normalized: M: e7e5 d7d5 d7d6 g7g6 e7e6 E: -0.62 -0.48 -0.73 -0.67 -0.82...
    Gen 3: Train_LP= -540.0, Ref_LP=  -26.0
           Text:                             M: e7e5 d7d6 d7d5 g7g6 d8c7     ...
           Normalized: M: e7e5 d7d6 d7d5 g7g6 d8c7 E: -0.52 -0.6 -0.34 -0.66 -0.8 B...
    Gen 4: Train_LP= -700.0, Ref_LP=  -45.2
           Text:                             M: d7d5 e7e5 d7d6 e7e6 g7g6     ...
           Normalized: M: d7d5 e7e5 d7d6 e7e6 g7g6 E: -0.49 -0.63 -0.68 -0.67 -0.71...

==================== PHASE 4: REWARD CALCULATION (step 38) ====================

üéØ Scoring Prompt 1 completions:
  Gen 1: Reward= 0.257 | M: h8e8 d4d3 c6d7 a6c5 c8b8 E: -3.97 -3....
  Gen 2: Reward= 0.278 | M: c6f3 d4d3 a6c5 e7e8 h8e8 E: -3.92 -3....
  Gen 3: Reward= 0.312 | M: a6c5 b6a5 e7h4 c6f3 c8b8 E: -3.7 -3.9...
  Gen 4: Reward= 0.142 | M: h8h4 a6c5 e7d7 c6b5 h8e8 E: -4.03 -3....
  üìä Average reward: 0.247

üéØ Scoring Prompt 2 completions:
  Gen 1: Reward=-0.075 | M: e2f2 e2f1 e2d3 c8e7 e2f3 E: 0.0 0.0 0...
  Gen 2: Reward= 0.489 | M: f5g6 e2f2 e2f3 c8e7 c8a7 E: 0.25 0.25...
  Gen 3: Reward= 0.329 | M: e2f2 c8e7 f5g4 e2f3 c8a7 E: 0.0 0.0 0...
  Gen 4: Reward=-0.055 | M: e2d3 e2f2 c8e7 e2f3 f5g4 E: 0.0 0.0 0...
  üìä Average reward: 0.172

üéØ Scoring Prompt 3 completions:
  Gen 1: Reward= 0.630 | 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b...
  Gen 2: Reward= 0.630 | 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b...
  Gen 3: Reward= 0.630 | 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b...
  Gen 4: Reward= 0.630 | 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b...
  üìä Average reward: 0.630

üéØ Scoring Prompt 4 completions:
  Gen 1: Reward= 0.368 | M: d7d6 e7e5 d7d5 g7g6 d8c7 E: -0.58 -0....
  Gen 2: Reward= 0.361 | M: e7e5 d7d5 d7d6 g7g6 e7e6 E: -0.62 -0....
  Gen 3: Reward= 0.364 | M: e7e5 d7d6 d7d5 g7g6 d8c7 E: -0.52 -0....
  Gen 4: Reward= 0.741 | M: d7d5 e7e5 d7d6 e7e6 g7g6 E: -0.49 -0....
  üìä Average reward: 0.458

==================== PHASE 5: TRL EXACT ADVANTAGE CALCULATION (step 38) ====================
üî¢ TRL Advantage Calculation (exact formula):
  Total rewards shape: torch.Size([16])
  Rewards: [ 0.25657143  0.27817143  0.3124      0.14213333 -0.075       0.48857143
  0.32857143 -0.055       0.63        0.63        0.63        0.63
  0.36777143  0.36137143  0.36377143  0.74057143]

==================== PHASE 6-8: CORRECTED GRPO LOSS CALCULATION (step 38) ====================
üîÑ Computing GRPO loss with proper token-level KL...

üéØ Prompt 1 - Corrected GRPO Loss:
  Gen 1: A=+0.125, logp/len=-7.281, kl=-5.531
         PG=0.914, KL_penalty=-0.555, total=0.359
  Gen 2: A=+0.418, logp/len=-7.125, kl=-5.406
         PG=2.984, KL_penalty=-0.539, total=2.438
  Gen 3: A=+0.882, logp/len=-7.594, kl=-5.906
         PG=6.688, KL_penalty=-0.590, total=6.094
  Gen 4: A=-1.425, logp/len=-5.562, kl=-3.750
         PG=-7.938, KL_penalty=-0.375, total=-8.312
  üìä Prompt averages: PG=0.672, KL=-0.516

üéØ Prompt 2 - Corrected GRPO Loss:
  Gen 1: A=-0.878, logp/len=-7.250, kl=-5.531
         PG=-6.375, KL_penalty=-0.555, total=-6.938
  Gen 2: A=+1.126, logp/len=-7.500, kl=-5.688
         PG=8.438, KL_penalty=-0.570, total=7.875
  Gen 3: A=+0.558, logp/len=-6.969, kl=-5.375
         PG=3.891, KL_penalty=-0.539, total=3.344
  Gen 4: A=-0.806, logp/len=-7.781, kl=-6.188
         PG=-6.281, KL_penalty=-0.617, total=-6.906
  üìä Prompt averages: PG=-0.086, KL=-0.570

üéØ Prompt 3 - Corrected GRPO Loss:
  Gen 1: A=+0.000, logp/len=-5.719, kl=-5.344
         PG=0.000, KL_penalty=-0.535, total=-0.535
  Gen 2: A=+0.000, logp/len=-5.312, kl=-4.969
         PG=0.000, KL_penalty=-0.496, total=-0.496
  Gen 3: A=+0.000, logp/len=-6.469, kl=-6.125
         PG=0.000, KL_penalty=-0.613, total=-0.613
  Gen 4: A=+0.000, logp/len=-6.906, kl=-6.531
         PG=0.000, KL_penalty=-0.652, total=-0.652
  üìä Prompt averages: PG=0.000, KL=-0.574

üéØ Prompt 4 - Corrected GRPO Loss:
  Gen 1: A=-0.482, logp/len=-7.438, kl=-6.000
         PG=-3.578, KL_penalty=-0.602, total=-4.188
  Gen 2: A=-0.516, logp/len=-6.281, kl=-4.688
         PG=-3.234, KL_penalty=-0.469, total=-3.703
  Gen 3: A=-0.503, logp/len=-7.125, kl=-5.562
         PG=-3.578, KL_penalty=-0.555, total=-4.125
  Gen 4: A=+1.500, logp/len=-7.188, kl=-5.594
         PG=10.750, KL_penalty=-0.559, total=10.188
  üìä Prompt averages: PG=0.094, KL=-0.547

üî¢ CORRECTED Loss Breakdown:
  Policy Gradient Loss:    0.170
  KL Loss (with gradients):   -0.551
  Total Loss:          =  -0.381
  KL penalty ratio: 76.5%

üîÑ Performing corrected gradient update (step 38)...
  Total gradient norm: 19.91
  ‚úÖ Gradient update applied with proper KL regularization

==================== PHASE 9: POST-UPDATE PERFORMANCE (step 38) ====================
  Prompt 1: avg reward = 0.155
  Prompt 2: avg reward = 0.387
  Prompt 3: avg reward = 0.630
  Prompt 4: avg reward = 0.559
üìä POST-UPDATE (step 38) Performance:
  Average reward: 0.4329
  Positive ratio: 100.0%

==================== PHASE 10: ANALYSIS (step 38) ====================
üîç GRPO Step Impact (step 38):
  Post-update performance: 0.4329
  Step performance change: +0.0144

======================================================================
üß≠ SEQUENTIAL STEP 39/50
======================================================================
üßæ Overfit mode: reusing initial batch (no dataset slice advance)

ü§ñ Generating completions for GRPO training (step 39)...

üéØ Prompt 1: P: 2k4r/ppp1q1p1/nbb1N3/4PP2/3p2P1/PB5P/1PPK3R/3RQ3 b - - 3 ...
    Gen 1: Train_LP= -442.0, Ref_LP=  -44.8
           Text:                                M: a6c5 d4d3 h8e8 e7d7 c6f3  ...
           Normalized: M: a6c5 d4d3 h8e8 e7d7 c6f3 E: -3.65 -3.53 -3.64 -3.68 -3.75...
    Gen 2: Train_LP= -548.0, Ref_LP=  -35.5
           Text:                                M: c6d7 c6b5 e7d7 h8e8 a6c5  ...
           Normalized: M: c6d7 c6b5 e7d7 h8e8 a6c5 E: -3.67 -3.58 -3.8 -3.58 -3.59 ...
    Gen 3: Train_LP= -528.0, Ref_LP=  -44.8
           Text:                                M: a6c5 e7e8 d4d3 c6d7 c8b8  ...
           Normalized: M: a6c5 e7e8 d4d3 c6d7 c8b8 E: -3.68 -4.05 -3.75 -4.05 -3.78...
    Gen 4: Train_LP= -378.0, Ref_LP=  -45.0
           Text:                                M: a6c5 c6d7 c8b8 d4d3 e7h4  ...
           Normalized: M: a6c5 c6d7 c8b8 d4d3 e7h4 E: -3.74 -3.81 -4.02 -3.98 -4.02...

üéØ Prompt 2: P: 2N5/2k5/3p1p2/3P1BbP/4Pp2/1n6/4K3/8 w - - 9 49...
    Gen 1: Train_LP= -636.0, Ref_LP=  -15.2
           Text:                                             M: e2d1 c8a7 e2f...
           Normalized: M: e2d1 c8a7 e2f2 e2f3 c8e7 E: 0.0 0.0 0.0 0.0 0.0 B: e2f3...
    Gen 2: Train_LP= -724.0, Ref_LP=  -71.0
           Text:                                             M: e2f2 c8a7 e2f...
           Normalized: M: e2f2 c8a7 e2f3 e2f1 c8e7 E: 0.51 0.57 0.58 0.45 0.6 B: c8...
    Gen 3: Train_LP= -430.0, Ref_LP=  -81.5
           Text:                                             M: e2f3 e2f1 c8a...
           Normalized: M: e2f3 e2f1 c8a7 e2e1 c8e7 E: 0.31 0.26 0.32 0.26 0.34 B: c...
    Gen 4: Train_LP= -548.0, Ref_LP=  -80.0
           Text:                                             M: f5g6 e2f3 c8e...
           Normalized: M: f5g6 e2f3 c8e7 c8a7 e2f2 E: 0.44 0.48 0.48 0.48 0.48 B: e...

üéØ Prompt 3: A: 8/2k2b2/R1p4r/3p1p2/P7/2P2B1P/5PPK/4R3 w - - 1 31+a6a5+f7...
    Gen 1: Train_LP= -302.0, Ref_LP=   -0.0
           Text: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
           Normalized: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
    Gen 2: Train_LP= -247.0, Ref_LP=   -0.0
           Text: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
           Normalized: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
    Gen 3: Train_LP= -255.0, Ref_LP=   -0.0
           Text: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
           Normalized: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
    Gen 4: Train_LP= -292.0, Ref_LP=   -0.0
           Text: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
           Normalized: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...

üéØ Prompt 4: P: rnbqkbnr/pp1ppppp/2p5/8/4P3/2N5/PPPP1PPP/R1BQKBNR b KQkq ...
    Gen 1: Train_LP= -426.0, Ref_LP=  -25.6
           Text:                             M: d7d5 d7d6 e7e5 g7g6 d8c7     ...
           Normalized: M: d7d5 d7d6 e7e5 g7g6 d8c7 E: -0.49 -0.76 -0.5 -0.8 -0.77 B...
    Gen 2: Train_LP= -450.0, Ref_LP=  -35.5
           Text:                             M: d7d6 d7d5 e7e5 c6c5 e7e6     ...
           Normalized: M: d7d6 d7d5 e7e5 c6c5 e7e6 E: -0.57 -0.5 -0.54 -0.76 -0.84 ...
    Gen 3: Train_LP= -628.0, Ref_LP=  -45.5
           Text:                             M: e7e5 d7d5 d7d6 e7e6 g7g6     ...
           Normalized: M: e7e5 d7d5 d7d6 e7e6 g7g6 E: -0.68 -0.36 -0.63 -0.76 -0.71...
    Gen 4: Train_LP= -664.0, Ref_LP=  -45.5
           Text:                             M: e7e5 d7d6 g7g6 d7d5 e7e6     ...
           Normalized: M: e7e5 d7d6 g7g6 d7d5 e7e6 E: -0.53 -0.62 -0.61 -0.45 -0.63...

==================== PHASE 4: REWARD CALCULATION (step 39) ====================

üéØ Scoring Prompt 1 completions:
  Gen 1: Reward= 0.506 | M: a6c5 d4d3 h8e8 e7d7 c6f3 E: -3.65 -3....
  Gen 2: Reward= 0.109 | M: c6d7 c6b5 e7d7 h8e8 a6c5 E: -3.67 -3....
  Gen 3: Reward= 0.251 | M: a6c5 e7e8 d4d3 c6d7 c8b8 E: -3.68 -4....
  Gen 4: Reward= 0.249 | M: a6c5 c6d7 c8b8 d4d3 e7h4 E: -3.74 -3....
  üìä Average reward: 0.279

üéØ Scoring Prompt 2 completions:
  Gen 1: Reward= 0.090 | M: e2d1 c8a7 e2f2 e2f3 c8e7 E: 0.0 0.0 0...
  Gen 2: Reward= 0.469 | M: e2f2 c8a7 e2f3 e2f1 c8e7 E: 0.51 0.57...
  Gen 3: Reward= 0.425 | M: e2f3 e2f1 c8a7 e2e1 c8e7 E: 0.31 0.26...
  Gen 4: Reward= 0.019 | M: f5g6 e2f3 c8e7 c8a7 e2f2 E: 0.44 0.48...
  üìä Average reward: 0.251

üéØ Scoring Prompt 3 completions:
  Gen 1: Reward= 0.630 | 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b...
  Gen 2: Reward= 0.630 | 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b...
  Gen 3: Reward= 0.630 | 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b...
  Gen 4: Reward= 0.630 | 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b...
  üìä Average reward: 0.630

üéØ Scoring Prompt 4 completions:
  Gen 1: Reward= 0.741 | M: d7d5 d7d6 e7e5 g7g6 d8c7 E: -0.49 -0....
  Gen 2: Reward= 0.369 | M: d7d6 d7d5 e7e5 c6c5 e7e6 E: -0.57 -0....
  Gen 3: Reward= 0.363 | M: e7e5 d7d5 d7d6 e7e6 g7g6 E: -0.68 -0....
  Gen 4: Reward= 0.361 | M: e7e5 d7d6 g7g6 d7d5 e7e6 E: -0.53 -0....
  üìä Average reward: 0.459

==================== PHASE 5: TRL EXACT ADVANTAGE CALCULATION (step 39) ====================
üî¢ TRL Advantage Calculation (exact formula):
  Total rewards shape: torch.Size([16])
  Rewards: [0.50577143 0.10893333 0.25137143 0.24897143 0.09       0.46857143
 0.425      0.01857143 0.63       0.63       0.63       0.63
 0.74137143 0.36897143 0.36297143 0.36137143]

==================== PHASE 6-8: CORRECTED GRPO LOSS CALCULATION (step 39) ====================
üîÑ Computing GRPO loss with proper token-level KL...

üéØ Prompt 1 - Corrected GRPO Loss:
  Gen 1: A=+1.373, logp/len=-7.375, kl=-5.625
         PG=10.125, KL_penalty=-0.562, total=9.562
  Gen 2: A=-1.027, logp/len=-6.875, kl=-5.125
         PG=-7.062, KL_penalty=-0.512, total=-7.562
  Gen 3: A=-0.166, logp/len=-6.250, kl=-4.500
         PG=-1.039, KL_penalty=-0.449, total=-1.484
  Gen 4: A=-0.180, logp/len=-7.438, kl=-5.594
         PG=-1.344, KL_penalty=-0.559, total=-1.906
  üìä Prompt averages: PG=0.172, KL=-0.523

üéØ Prompt 2 - Corrected GRPO Loss:
  Gen 1: A=-0.701, logp/len=-6.406, kl=-4.812
         PG=-4.500, KL_penalty=-0.480, total=-4.969
  Gen 2: A=+0.951, logp/len=-5.594, kl=-3.734
         PG=5.312, KL_penalty=-0.373, total=4.938
  Gen 3: A=+0.761, logp/len=-6.562, kl=-4.750
         PG=5.000, KL_penalty=-0.475, total=4.531
  Gen 4: A=-1.012, logp/len=-5.812, kl=-3.781
         PG=-5.875, KL_penalty=-0.379, total=-6.250
  üìä Prompt averages: PG=-0.016, KL=-0.426

üéØ Prompt 3 - Corrected GRPO Loss:
  Gen 1: A=+0.000, logp/len=-4.969, kl=-4.594
         PG=0.000, KL_penalty=-0.459, total=-0.459
  Gen 2: A=+0.000, logp/len=-6.250, kl=-5.906
         PG=0.000, KL_penalty=-0.590, total=-0.590
  Gen 3: A=+0.000, logp/len=-4.281, kl=-3.906
         PG=0.000, KL_penalty=-0.391, total=-0.391
  Gen 4: A=+0.000, logp/len=-5.562, kl=-5.188
         PG=0.000, KL_penalty=-0.520, total=-0.520
  üìä Prompt averages: PG=0.000, KL=-0.488

üéØ Prompt 4 - Corrected GRPO Loss:
  Gen 1: A=+1.500, logp/len=-9.250, kl=-7.719
         PG=13.875, KL_penalty=-0.773, total=13.125
  Gen 2: A=-0.476, logp/len=-6.406, kl=-4.656
         PG=-3.047, KL_penalty=-0.465, total=-3.516
  Gen 3: A=-0.508, logp/len=-10.125, kl=-8.438
         PG=-5.125, KL_penalty=-0.844, total=-5.969
  Gen 4: A=-0.516, logp/len=-6.250, kl=-4.625
         PG=-3.219, KL_penalty=-0.463, total=-3.688
  üìä Prompt averages: PG=0.617, KL=-0.637

üî¢ CORRECTED Loss Breakdown:
  Policy Gradient Loss:    0.193
  KL Loss (with gradients):   -0.520
  Total Loss:          =  -0.326
  KL penalty ratio: 73.0%

üîÑ Performing corrected gradient update (step 39)...
  Total gradient norm: 33.21
  ‚úÖ Gradient update applied with proper KL regularization

==================== PHASE 9: POST-UPDATE PERFORMANCE (step 39) ====================
  Prompt 1: avg reward = 0.152
  Prompt 2: avg reward = 0.387
  Prompt 3: avg reward = 0.630
  Prompt 4: avg reward = 0.559
üìä POST-UPDATE (step 39) Performance:
  Average reward: 0.4321
  Positive ratio: 100.0%

==================== PHASE 10: ANALYSIS (step 39) ====================
üîç GRPO Step Impact (step 39):
  Post-update performance: 0.4321
  Step performance change: -0.0009

======================================================================
üß≠ SEQUENTIAL STEP 40/50
======================================================================
üßæ Overfit mode: reusing initial batch (no dataset slice advance)

ü§ñ Generating completions for GRPO training (step 40)...

üéØ Prompt 1: P: 2k4r/ppp1q1p1/nbb1N3/4PP2/3p2P1/PB5P/1PPK3R/3RQ3 b - - 3 ...
    Gen 1: Train_LP= -462.0, Ref_LP=  -55.2
           Text:                                M: b6a5 d4d3 e7d7 a6c5 c6f3  ...
           Normalized: M: b6a5 d4d3 e7d7 a6c5 c6f3 E: -3.73 -3.78 -3.92 -3.67 -3.93...
    Gen 2: Train_LP= -492.0, Ref_LP=  -33.5
           Text:                                M: a6c5 d4d3 b6a5 g7g6 h8e8  ...
           Normalized: M: a6c5 d4d3 b6a5 g7g6 h8e8 E: -3.38 -3.56 -3.4 -3.4 -3.67 B...
    Gen 3: Train_LP= -302.0, Ref_LP=  -55.2
           Text:                                M: a6c5 e7e8 c8b8 h8h6 e7d7  ...
           Normalized: M: a6c5 e7e8 c8b8 h8h6 e7d7 E: -3.76 -4.04 -4.01 -4.07 -4.08...
    Gen 4: Train_LP= -484.0, Ref_LP=  -58.0
           Text:                                M: a6c5 c6d7 b6a5 c8b8 c6f3  ...
           Normalized: M: a6c5 c6d7 b6a5 c8b8 c6f3 E: -2.63 -3.03 -3.26 -3.09 -3.04...

üéØ Prompt 2: P: 2N5/2k5/3p1p2/3P1BbP/4Pp2/1n6/4K3/8 w - - 9 49...
    Gen 1: Train_LP= -672.0, Ref_LP=  -25.9
           Text:                                             M: e2f2 c8e7 e2f...
           Normalized: M: e2f2 c8e7 e2f3 f5g6 c8a7 E: 0.23 0.19 0.24 0.23 0.24 B: c...
    Gen 2: Train_LP= -536.0, Ref_LP=  -24.0
           Text:                                             M: e2f1 e2d3 c8e...
           Normalized: M: e2f1 e2d3 c8e7 e2f2 e2f3 E: 0.16 0.15 0.17 0.16 0.17 B: c...
    Gen 3: Train_LP= -474.0, Ref_LP=  -25.0
           Text:                                             M: e2f3 e2d3 e2f...
           Normalized: M: e2f3 e2d3 e2f2 c8e7 c8a7 E: 0.25 0.21 0.24 0.26 0.21 B: c...
    Gen 4: Train_LP= -692.0, Ref_LP=  -22.5
           Text:                                             M: e2f2 e2f1 e2f...
           Normalized: M: e2f2 e2f1 e2f3 c8e7 c8a7 E: 0.29 0.29 0.28 0.28 0.28 B: e...

üéØ Prompt 3: A: 8/2k2b2/R1p4r/3p1p2/P7/2P2B1P/5PPK/4R3 w - - 1 31+a6a5+f7...
    Gen 1: Train_LP= -270.0, Ref_LP=   -0.0
           Text: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
           Normalized: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
    Gen 2: Train_LP= -270.0, Ref_LP=   -0.0
           Text: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
           Normalized: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
    Gen 3: Train_LP= -346.0, Ref_LP=   -0.0
           Text: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
           Normalized: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
    Gen 4: Train_LP= -304.0, Ref_LP=   -0.0
           Text: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
           Normalized: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...

üéØ Prompt 4: P: rnbqkbnr/pp1ppppp/2p5/8/4P3/2N5/PPPP1PPP/R1BQKBNR b KQkq ...
    Gen 1: Train_LP= -438.0, Ref_LP=  -36.0
           Text:                             M: d7d6 d7d5 e7e5 c6c5 g7g6     ...
           Normalized: M: d7d6 d7d5 e7e5 c6c5 g7g6 E: -0.76 -0.48 -0.62 -0.86 -0.74...
    Gen 2: Train_LP= -608.0, Ref_LP=  -35.0
           Text:                             M: d7d5 d7d6 e7e5 g7g6 d8c7     ...
           Normalized: M: d7d5 d7d6 e7e5 g7g6 d8c7 E: -0.36 -0.68 -0.67 -0.68 -0.74...
    Gen 3: Train_LP= -528.0, Ref_LP=  -35.2
           Text:                             M: d7d6 e7e6 d7d5 g7g6 e7e5     ...
           Normalized: M: d7d6 e7e6 d7d5 g7g6 e7e5 E: -0.64 -0.82 -0.49 -0.71 -0.68...
    Gen 4: Train_LP= -552.0, Ref_LP=  -25.5
           Text:                             M: e7e5 d7d6 d7d5 g7g6 d8c7     ...
           Normalized: M: e7e5 d7d6 d7d5 g7g6 d8c7 E: -0.61 -0.6 -0.48 -0.79 -0.76 ...

==================== PHASE 4: REWARD CALCULATION (step 40) ====================

üéØ Scoring Prompt 1 completions:
  Gen 1: Reward= 0.613 | M: b6a5 d4d3 e7d7 a6c5 c6f3 E: -3.73 -3....
  Gen 2: Reward= 0.179 | M: a6c5 d4d3 b6a5 g7g6 h8e8 E: -3.38 -3....
  Gen 3: Reward= 0.219 | M: a6c5 e7e8 c8b8 h8h6 e7d7 E: -3.76 -4....
  Gen 4: Reward= 0.700 | M: a6c5 c6d7 b6a5 c8b8 c6f3 E: -2.63 -3....
  üìä Average reward: 0.428

üéØ Scoring Prompt 2 completions:
  Gen 1: Reward= 0.269 | M: e2f2 c8e7 e2f3 f5g6 c8a7 E: 0.23 0.19...
  Gen 2: Reward= 0.415 | M: e2f1 e2d3 c8e7 e2f2 e2f3 E: 0.16 0.15...
  Gen 3: Reward= 0.549 | M: e2f3 e2d3 e2f2 c8e7 c8a7 E: 0.25 0.21...
  Gen 4: Reward= 0.369 | M: e2f2 e2f1 e2f3 c8e7 c8a7 E: 0.29 0.29...
  üìä Average reward: 0.400

üéØ Scoring Prompt 3 completions:
  Gen 1: Reward= 0.630 | 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b...
  Gen 2: Reward= 0.630 | 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b...
  Gen 3: Reward= 0.630 | 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b...
  Gen 4: Reward= 0.630 | 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b...
  üìä Average reward: 0.630

üéØ Scoring Prompt 4 completions:
  Gen 1: Reward= 0.678 | M: d7d6 d7d5 e7e5 c6c5 g7g6 E: -0.76 -0....
  Gen 2: Reward= 0.386 | M: d7d5 d7d6 e7e5 g7g6 d8c7 E: -0.36 -0....
  Gen 3: Reward= 0.364 | M: d7d6 e7e6 d7d5 g7g6 e7e5 E: -0.64 -0....
  Gen 4: Reward= 0.718 | M: e7e5 d7d6 d7d5 g7g6 d8c7 E: -0.61 -0....
  üìä Average reward: 0.536

==================== PHASE 5: TRL EXACT ADVANTAGE CALCULATION (step 40) ====================
üî¢ TRL Advantage Calculation (exact formula):
  Total rewards shape: torch.Size([16])
  Rewards: [0.6128     0.17857143 0.219      0.7        0.26857143 0.415
 0.54857143 0.36857143 0.63       0.63       0.63       0.63
 0.6776     0.38617143 0.36377143 0.71777143]

==================== PHASE 6-8: CORRECTED GRPO LOSS CALCULATION (step 40) ====================
üîÑ Computing GRPO loss with proper token-level KL...

üéØ Prompt 1 - Corrected GRPO Loss:
  Gen 1: A=+0.693, logp/len=-5.375, kl=-3.672
         PG=3.734, KL_penalty=-0.367, total=3.375
  Gen 2: A=-0.932, logp/len=-7.594, kl=-5.938
         PG=-7.094, KL_penalty=-0.594, total=-7.688
  Gen 3: A=-0.781, logp/len=-6.000, kl=-4.156
         PG=-4.688, KL_penalty=-0.416, total=-5.094
  Gen 4: A=+1.020, logp/len=-6.250, kl=-4.438
         PG=6.375, KL_penalty=-0.443, total=5.938
  üìä Prompt averages: PG=-0.422, KL=-0.455

üéØ Prompt 2 - Corrected GRPO Loss:
  Gen 1: A=-1.132, logp/len=-8.562, kl=-6.719
         PG=-9.688, KL_penalty=-0.672, total=-10.375
  Gen 2: A=+0.127, logp/len=-6.844, kl=-4.844
         PG=0.871, KL_penalty=-0.484, total=0.387
  Gen 3: A=+1.276, logp/len=-5.656, kl=-3.859
         PG=7.219, KL_penalty=-0.387, total=6.844
  Gen 4: A=-0.272, logp/len=-7.156, kl=-5.469
         PG=-1.945, KL_penalty=-0.547, total=-2.500
  üìä Prompt averages: PG=-0.883, KL=-0.523

üéØ Prompt 3 - Corrected GRPO Loss:
  Gen 1: A=+0.000, logp/len=-4.375, kl=-4.000
         PG=0.000, KL_penalty=-0.400, total=-0.400
  Gen 2: A=+0.000, logp/len=-5.094, kl=-4.719
         PG=0.000, KL_penalty=-0.473, total=-0.473
  Gen 3: A=+0.000, logp/len=-6.406, kl=-6.062
         PG=0.000, KL_penalty=-0.605, total=-0.605
  Gen 4: A=+0.000, logp/len=-6.375, kl=-5.969
         PG=0.000, KL_penalty=-0.598, total=-0.598
  üìä Prompt averages: PG=0.000, KL=-0.520

üéØ Prompt 4 - Corrected GRPO Loss:
  Gen 1: A=+0.754, logp/len=-7.031, kl=-5.375
         PG=5.312, KL_penalty=-0.539, total=4.781
  Gen 2: A=-0.802, logp/len=-7.469, kl=-6.000
         PG=-6.000, KL_penalty=-0.602, total=-6.594
  Gen 3: A=-0.921, logp/len=-6.188, kl=-4.719
         PG=-5.688, KL_penalty=-0.473, total=-6.156
  Gen 4: A=+0.969, logp/len=-7.312, kl=-5.812
         PG=7.094, KL_penalty=-0.582, total=6.500
  üìä Prompt averages: PG=0.180, KL=-0.547

üî¢ CORRECTED Loss Breakdown:
  Policy Gradient Loss:   -0.281
  KL Loss (with gradients):   -0.512
  Total Loss:          =  -0.793
  KL penalty ratio: 64.5%

üîÑ Performing corrected gradient update (step 40)...
  Total gradient norm: 34.06
  ‚úÖ Gradient update applied with proper KL regularization

==================== PHASE 9: POST-UPDATE PERFORMANCE (step 40) ====================
  Prompt 1: avg reward = 0.155
  Prompt 2: avg reward = 0.387
  Prompt 3: avg reward = 0.630
  Prompt 4: avg reward = 0.559
üìä POST-UPDATE (step 40) Performance:
  Average reward: 0.4329
  Positive ratio: 100.0%

==================== PHASE 10: ANALYSIS (step 40) ====================
üîç GRPO Step Impact (step 40):
  Post-update performance: 0.4329
  Step performance change: +0.0009

======================================================================
üß≠ SEQUENTIAL STEP 41/50
======================================================================
üßæ Overfit mode: reusing initial batch (no dataset slice advance)

ü§ñ Generating completions for GRPO training (step 41)...

üéØ Prompt 1: P: 2k4r/ppp1q1p1/nbb1N3/4PP2/3p2P1/PB5P/1PPK3R/3RQ3 b - - 3 ...
    Gen 1: Train_LP= -516.0, Ref_LP=  -47.2
           Text:                                M: e7h4 a6c5 c6d7 d4d3 e7d7  ...
           Normalized: M: e7h4 a6c5 c6d7 d4d3 e7d7 E: -4.24 -3.66 -3.79 -4.24 -3.85...
    Gen 2: Train_LP= -448.0, Ref_LP=  -32.2
           Text:                                M: a6c5 h8e8 c8b8 d4d3 e7d7  ...
           Normalized: M: a6c5 h8e8 c8b8 d4d3 e7d7 E: -3.49 -3.74 -3.84 -3.7 -3.83 ...
    Gen 3: Train_LP= -366.0, Ref_LP=  -35.5
           Text:                                M: e7h4 c6d7 c6b5 a6c5 d4d3  ...
           Normalized: M: e7h4 c6d7 c6b5 a6c5 d4d3 E: -4.17 -4.24 -4.21 -3.65 -3.9 ...
    Gen 4: Train_LP= -478.0, Ref_LP=  -43.5
           Text:                                M: e7d7 c8b8 a6c5 d4d3 e7e8  ...
           Normalized: M: e7d7 c8b8 a6c5 d4d3 e7e8 E: -3.83 -3.88 -3.85 -3.83 -3.81...

üéØ Prompt 2: P: 2N5/2k5/3p1p2/3P1BbP/4Pp2/1n6/4K3/8 w - - 9 49...
    Gen 1: Train_LP= -828.0, Ref_LP=  -14.4
           Text:                                             M: e2f2 e2f3 e2d...
           Normalized: M: e2f2 e2f3 e2d3 c8e7 c8a7 E: 0.0 0.0 0.0 0.0 0.0 B: c8e7...
    Gen 2: Train_LP= -450.0, Ref_LP=  -80.0
           Text:                                             M: e2d1 e2f2 e2f...
           Normalized: M: e2d1 e2f2 e2f3 c8e7 c8a7 E: 0.22 0.28 0.25 0.28 0.25 B: c...
    Gen 3: Train_LP= -318.0, Ref_LP=  -80.0
           Text:                                             M: e2f3 e2f2 e2e...
           Normalized: M: e2f3 e2f2 e2e1 e2f1 c8e7 E: 0.39 0.35 0.36 0.38 0.38 B: e...
    Gen 4: Train_LP= -656.0, Ref_LP=  -13.6
           Text:                                             M: e2f2 c8a7 e2f...
           Normalized: M: e2f2 c8a7 e2f3 e2f1 c8e7 E: 0.0 0.0 0.0 0.0 0.0 B: c8a7...

üéØ Prompt 3: A: 8/2k2b2/R1p4r/3p1p2/P7/2P2B1P/5PPK/4R3 w - - 1 31+a6a5+f7...
    Gen 1: Train_LP= -296.0, Ref_LP=   -0.0
           Text: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
           Normalized: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
    Gen 2: Train_LP= -324.0, Ref_LP=   -0.0
           Text: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
           Normalized: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
    Gen 3: Train_LP= -270.0, Ref_LP=   -0.0
           Text: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
           Normalized: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
    Gen 4: Train_LP= -260.0, Ref_LP=   -0.0
           Text: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
           Normalized: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...

üéØ Prompt 4: P: rnbqkbnr/pp1ppppp/2p5/8/4P3/2N5/PPPP1PPP/R1BQKBNR b KQkq ...
    Gen 1: Train_LP= -668.0, Ref_LP=  -35.0
           Text:                             M: d7d6 g7g6 e7e5 d7d5 c6c5     ...
           Normalized: M: d7d6 g7g6 e7e5 d7d5 c6c5 E: -0.59 -0.81 -0.62 -0.56 -0.71...
    Gen 2: Train_LP= -354.0, Ref_LP=  -35.8
           Text:                             M: d7d6 d7d5 e7e5 g7g6 c6c5     ...
           Normalized: M: d7d6 d7d5 e7e5 g7g6 c6c5 E: -0.68 -0.32 -0.48 -0.74 -0.74...
    Gen 3: Train_LP= -392.0, Ref_LP=  -24.5
           Text:                             M: d7d6 d7d5 e7e5 g7g6 c6c5     ...
           Normalized: M: d7d6 d7d5 e7e5 g7g6 c6c5 E: -0.7 -0.52 -0.54 -0.67 -0.77 ...
    Gen 4: Train_LP= -512.0, Ref_LP=  -34.8
           Text:                             M: e7e5 d7d5 g7g6 d7d6 e7e6     ...
           Normalized: M: e7e5 d7d5 g7g6 d7d6 e7e6 E: -0.53 -0.48 -0.78 -0.63 -0.93...

==================== PHASE 4: REWARD CALCULATION (step 41) ====================

üéØ Scoring Prompt 1 completions:
  Gen 1: Reward= 0.213 | M: e7h4 a6c5 c6d7 d4d3 e7d7 E: -4.24 -3....
  Gen 2: Reward= 0.399 | M: a6c5 h8e8 c8b8 d4d3 e7d7 E: -3.49 -3....
  Gen 3: Reward= 0.225 | M: e7h4 c6d7 c6b5 a6c5 d4d3 E: -4.17 -4....
  Gen 4: Reward= 0.400 | M: e7d7 c8b8 a6c5 d4d3 e7e8 E: -3.83 -3....
  üìä Average reward: 0.309

üéØ Scoring Prompt 2 completions:
  Gen 1: Reward= 0.469 | M: e2f2 e2f3 e2d3 c8e7 c8a7 E: 0.0 0.0 0...
  Gen 2: Reward= 0.640 | M: e2d1 e2f2 e2f3 c8e7 c8a7 E: 0.22 0.28...
  Gen 3: Reward=-0.015 | M: e2f3 e2f2 e2e1 e2f1 c8e7 E: 0.39 0.35...
  Gen 4: Reward= 0.229 | M: e2f2 c8a7 e2f3 e2f1 c8e7 E: 0.0 0.0 0...
  üìä Average reward: 0.331

üéØ Scoring Prompt 3 completions:
  Gen 1: Reward= 0.630 | 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b...
  Gen 2: Reward= 0.630 | 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b...
  Gen 3: Reward= 0.630 | 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b...
  Gen 4: Reward= 0.630 | 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b...
  üìä Average reward: 0.630

üéØ Scoring Prompt 4 completions:
  Gen 1: Reward= 0.808 | M: d7d6 g7g6 e7e5 d7d5 c6c5 E: -0.59 -0....
  Gen 2: Reward= 0.782 | M: d7d6 d7d5 e7e5 g7g6 c6c5 E: -0.68 -0....
  Gen 3: Reward= 0.682 | M: d7d6 d7d5 e7e5 g7g6 c6c5 E: -0.7 -0.5...
  Gen 4: Reward= 0.367 | M: e7e5 d7d5 g7g6 d7d6 e7e6 E: -0.53 -0....
  üìä Average reward: 0.660

==================== PHASE 5: TRL EXACT ADVANTAGE CALCULATION (step 41) ====================
üî¢ TRL Advantage Calculation (exact formula):
  Total rewards shape: torch.Size([16])
  Rewards: [ 0.213       0.39857143  0.2254      0.39977143  0.46857143  0.64
 -0.015       0.22857143  0.63        0.63        0.63        0.63
  0.8076      0.7824      0.6816      0.36737143]

==================== PHASE 6-8: CORRECTED GRPO LOSS CALCULATION (step 41) ====================
üîÑ Computing GRPO loss with proper token-level KL...

üéØ Prompt 1 - Corrected GRPO Loss:
  Gen 1: A=-0.925, logp/len=-6.844, kl=-4.875
         PG=-6.312, KL_penalty=-0.488, total=-6.812
  Gen 2: A=+0.859, logp/len=-5.844, kl=-4.031
         PG=5.031, KL_penalty=-0.402, total=4.625
  Gen 3: A=-0.805, logp/len=-6.031, kl=-4.219
         PG=-4.844, KL_penalty=-0.422, total=-5.250
  Gen 4: A=+0.871, logp/len=-6.625, kl=-4.906
         PG=5.781, KL_penalty=-0.490, total=5.281
  üìä Prompt averages: PG=-0.086, KL=-0.451

üéØ Prompt 2 - Corrected GRPO Loss:
  Gen 1: A=+0.483, logp/len=-7.156, kl=-5.625
         PG=3.453, KL_penalty=-0.562, total=2.891
  Gen 2: A=+1.084, logp/len=-7.344, kl=-5.562
         PG=7.969, KL_penalty=-0.555, total=7.406
  Gen 3: A=-1.210, logp/len=-7.875, kl=-6.156
         PG=-9.500, KL_penalty=-0.617, total=-10.125
  Gen 4: A=-0.357, logp/len=-6.938, kl=-5.375
         PG=-2.484, KL_penalty=-0.539, total=-3.031
  üìä Prompt averages: PG=-0.137, KL=-0.570

üéØ Prompt 3 - Corrected GRPO Loss:
  Gen 1: A=+0.000, logp/len=-6.469, kl=-6.125
         PG=0.000, KL_penalty=-0.613, total=-0.613
  Gen 2: A=+0.000, logp/len=-4.469, kl=-4.094
         PG=0.000, KL_penalty=-0.410, total=-0.410
  Gen 3: A=+0.000, logp/len=-7.750, kl=-7.406
         PG=0.000, KL_penalty=-0.742, total=-0.742
  Gen 4: A=+0.000, logp/len=-5.906, kl=-5.531
         PG=0.000, KL_penalty=-0.555, total=-0.555
  üìä Prompt averages: PG=0.000, KL=-0.578

üéØ Prompt 4 - Corrected GRPO Loss:
  Gen 1: A=+0.731, logp/len=-6.188, kl=-4.500
         PG=4.531, KL_penalty=-0.449, total=4.094
  Gen 2: A=+0.606, logp/len=-7.250, kl=-5.625
         PG=4.406, KL_penalty=-0.562, total=3.844
  Gen 3: A=+0.108, logp/len=-6.031, kl=-4.406
         PG=0.652, KL_penalty=-0.441, total=0.211
  Gen 4: A=-1.445, logp/len=-6.844, kl=-5.188
         PG=-9.875, KL_penalty=-0.520, total=-10.375
  üìä Prompt averages: PG=-0.078, KL=-0.492

üî¢ CORRECTED Loss Breakdown:
  Policy Gradient Loss:   -0.075
  KL Loss (with gradients):   -0.523
  Total Loss:          =  -0.598
  KL penalty ratio: 87.5%

üîÑ Performing corrected gradient update (step 41)...
  Total gradient norm: 23.18
  ‚úÖ Gradient update applied with proper KL regularization

==================== PHASE 9: POST-UPDATE PERFORMANCE (step 41) ====================
  Prompt 1: avg reward = 0.152
  Prompt 2: avg reward = 0.212
  Prompt 3: avg reward = 0.630
  Prompt 4: avg reward = 0.559
üìä POST-UPDATE (step 41) Performance:
  Average reward: 0.3883
  Positive ratio: 87.5%

==================== PHASE 10: ANALYSIS (step 41) ====================
üîç GRPO Step Impact (step 41):
  Post-update performance: 0.3883
  Step performance change: -0.0446

======================================================================
üß≠ SEQUENTIAL STEP 42/50
======================================================================
üßæ Overfit mode: reusing initial batch (no dataset slice advance)

ü§ñ Generating completions for GRPO training (step 42)...

üéØ Prompt 1: P: 2k4r/ppp1q1p1/nbb1N3/4PP2/3p2P1/PB5P/1PPK3R/3RQ3 b - - 3 ...
    Gen 1: Train_LP= -420.0, Ref_LP=  -34.0
           Text:                                M: d4d3 a6c5 c6f3 b6a5 c8b8  ...
           Normalized: M: d4d3 a6c5 c6f3 b6a5 c8b8 E: -3.47 -2.88 -3.29 -3.42 -3.35...
    Gen 2: Train_LP= -380.0, Ref_LP=  -36.2
           Text:                                M: d4d3 a6c5 c6f3 h8h4 c6d7  ...
           Normalized: M: d4d3 a6c5 c6f3 h8h4 c6d7 E: -3.89 -3.88 -3.84 -3.87 -4.13...
    Gen 3: Train_LP= -636.0, Ref_LP=  -33.5
           Text:                                M: c8b8 a6c5 h8e8 d4d3 c6d7  ...
           Normalized: M: c8b8 a6c5 h8e8 d4d3 c6d7 E: -3.91 -3.64 -4.01 -3.65 -3.98...
    Gen 4: Train_LP= -572.0, Ref_LP=  -36.0
           Text:                                M: c6d7 c6b5 a6c5 d4d3 c6f3  ...
           Normalized: M: c6d7 c6b5 a6c5 d4d3 c6f3 E: -4.32 -4.22 -4.13 -4.16 -4.09...

üéØ Prompt 2: P: 2N5/2k5/3p1p2/3P1BbP/4Pp2/1n6/4K3/8 w - - 9 49...
    Gen 1: Train_LP= -394.0, Ref_LP=  -15.3
           Text:                                             M: e2f3 c8a7 e2f...
           Normalized: M: e2f3 c8a7 e2f1 e2f2 e2e1 E: 0.0 0.0 0.0 0.0 0.0 B: e2f3...
    Gen 2: Train_LP= -434.0, Ref_LP=  -78.0
           Text:                                             M: e2d3 e2f2 c8e...
           Normalized: M: e2d3 e2f2 c8e7 e2f3 e2f1 E: 0.24 0.25 0.25 0.24 0.25 B: e...
    Gen 3: Train_LP= -528.0, Ref_LP=  -17.1
           Text:                                             M: e2d3 f5g4 f5g...
           Normalized: M: e2d3 f5g4 f5g6 c8e7 e2f3 E: 0.0 0.0 0.0 0.0 0.0 B: e2f3...
    Gen 4: Train_LP= -796.0, Ref_LP=  -78.0
           Text:                                             M: e2d3 c8e7 e2f...
           Normalized: M: e2d3 c8e7 e2f3 e2f2 f5g4 E: 0.18 0.18 0.18 0.18 0.18 B: e...

üéØ Prompt 3: A: 8/2k2b2/R1p4r/3p1p2/P7/2P2B1P/5PPK/4R3 w - - 1 31+a6a5+f7...
    Gen 1: Train_LP= -260.0, Ref_LP=   -0.0
           Text: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
           Normalized: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
    Gen 2: Train_LP= -266.0, Ref_LP=   -0.0
           Text: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
           Normalized: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
    Gen 3: Train_LP= -298.0, Ref_LP=   -0.0
           Text: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
           Normalized: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
    Gen 4: Train_LP= -280.0, Ref_LP=   -0.0
           Text: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
           Normalized: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...

üéØ Prompt 4: P: rnbqkbnr/pp1ppppp/2p5/8/4P3/2N5/PPPP1PPP/R1BQKBNR b KQkq ...
    Gen 1: Train_LP= -756.0, Ref_LP=  -45.2
           Text:                             M: e7e6 d7d6 e7e5 d7d5 g7g6     ...
           Normalized: M: e7e6 d7d6 e7e5 d7d5 g7g6 E: -0.77 -0.77 -0.62 -0.43 -0.78...
    Gen 2: Train_LP= -588.0, Ref_LP=  -36.2
           Text:                             M: d7d5 d7d6 e7e5 g7g6 d8c7     ...
           Normalized: M: d7d5 d7d6 e7e5 g7g6 d8c7 E: -0.45 -0.75 -0.5 -0.65 -0.87 ...
    Gen 3: Train_LP= -472.0, Ref_LP=  -24.8
           Text:                             M: d7d5 e7e5 d7d6 g7g6 a7a6     ...
           Normalized: M: d7d5 e7e5 d7d6 g7g6 a7a6 E: -0.42 -0.65 -0.7 -0.7 -0.77 B...
    Gen 4: Train_LP= -448.0, Ref_LP=  -35.0
           Text:                             M: d7d6 e7e5 d7d5 g7g6 e7e6     ...
           Normalized: M: d7d6 e7e5 d7d5 g7g6 e7e6 E: -0.68 -0.5 -0.39 -0.67 -0.77 ...

==================== PHASE 4: REWARD CALCULATION (step 42) ====================

üéØ Scoring Prompt 1 completions:
  Gen 1: Reward= 0.438 | M: d4d3 a6c5 c6f3 b6a5 c8b8 E: -3.47 -2....
  Gen 2: Reward= 0.242 | M: d4d3 a6c5 c6f3 h8h4 c6d7 E: -3.89 -3....
  Gen 3: Reward= 0.276 | M: c8b8 a6c5 h8e8 d4d3 c6d7 E: -3.91 -3....
  Gen 4: Reward= 0.291 | M: c6d7 c6b5 a6c5 d4d3 c6f3 E: -4.32 -4....
  üìä Average reward: 0.312

üéØ Scoring Prompt 2 completions:
  Gen 1: Reward=-0.035 | M: e2f3 c8a7 e2f1 e2f2 e2e1 E: 0.0 0.0 0...
  Gen 2: Reward= 0.335 | M: e2d3 e2f2 c8e7 e2f3 e2f1 E: 0.24 0.25...
  Gen 3: Reward=-0.117 | M: e2d3 f5g4 f5g6 c8e7 e2f3 E: 0.0 0.0 0...
  Gen 4: Reward=-0.035 | M: e2d3 c8e7 e2f3 e2f2 f5g4 E: 0.18 0.18...
  üìä Average reward: 0.037

üéØ Scoring Prompt 3 completions:
  Gen 1: Reward= 0.630 | 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b...
  Gen 2: Reward= 0.630 | 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b...
  Gen 3: Reward= 0.630 | 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b...
  Gen 4: Reward= 0.630 | 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b...
  üìä Average reward: 0.630

üéØ Scoring Prompt 4 completions:
  Gen 1: Reward= 0.716 | M: e7e6 d7d6 e7e5 d7d5 g7g6 E: -0.77 -0....
  Gen 2: Reward= 0.393 | M: d7d5 d7d6 e7e5 g7g6 d8c7 E: -0.45 -0....
  Gen 3: Reward= 0.391 | M: d7d5 e7e5 d7d6 g7g6 a7a6 E: -0.42 -0....
  Gen 4: Reward= 0.364 | M: d7d6 e7e5 d7d5 g7g6 e7e6 E: -0.68 -0....
  üìä Average reward: 0.466

==================== PHASE 5: TRL EXACT ADVANTAGE CALCULATION (step 42) ====================
üî¢ TRL Advantage Calculation (exact formula):
  Total rewards shape: torch.Size([16])
  Rewards: [ 0.4376      0.24217143  0.27577143  0.29097143 -0.035       0.335
 -0.11666667 -0.035       0.63        0.63        0.63        0.63
  0.71577143  0.39297143  0.39137143  0.36417143]

==================== PHASE 6-8: CORRECTED GRPO LOSS CALCULATION (step 42) ====================
üîÑ Computing GRPO loss with proper token-level KL...

üéØ Prompt 1 - Corrected GRPO Loss:
  Gen 1: A=+1.458, logp/len=-7.094, kl=-5.375
         PG=10.312, KL_penalty=-0.539, total=9.750
  Gen 2: A=-0.804, logp/len=-5.875, kl=-3.922
         PG=-4.719, KL_penalty=-0.393, total=-5.125
  Gen 3: A=-0.415, logp/len=-8.062, kl=-6.250
         PG=-3.344, KL_penalty=-0.625, total=-3.969
  Gen 4: A=-0.239, logp/len=-5.875, kl=-4.062
         PG=-1.406, KL_penalty=-0.406, total=-1.812
  üìä Prompt averages: PG=0.211, KL=-0.490

üéØ Prompt 2 - Corrected GRPO Loss:
  Gen 1: A=-0.356, logp/len=-7.500, kl=-5.781
         PG=-2.672, KL_penalty=-0.578, total=-3.250
  Gen 2: A=+1.473, logp/len=-6.344, kl=-4.344
         PG=9.312, KL_penalty=-0.434, total=8.875
  Gen 3: A=-0.760, logp/len=-6.344, kl=-4.656
         PG=-4.812, KL_penalty=-0.465, total=-5.281
  Gen 4: A=-0.356, logp/len=-6.750, kl=-4.938
         PG=-2.406, KL_penalty=-0.494, total=-2.906
  üìä Prompt averages: PG=-0.148, KL=-0.494

üéØ Prompt 3 - Corrected GRPO Loss:
  Gen 1: A=+0.000, logp/len=-5.469, kl=-5.094
         PG=0.000, KL_penalty=-0.508, total=-0.508
  Gen 2: A=+0.000, logp/len=-4.750, kl=-4.406
         PG=0.000, KL_penalty=-0.441, total=-0.441
  Gen 3: A=+0.000, logp/len=-5.031, kl=-4.656
         PG=0.000, KL_penalty=-0.465, total=-0.465
  Gen 4: A=+0.000, logp/len=-6.125, kl=-5.781
         PG=0.000, KL_penalty=-0.578, total=-0.578
  üìä Prompt averages: PG=0.000, KL=-0.498

üéØ Prompt 4 - Corrected GRPO Loss:
  Gen 1: A=+1.495, logp/len=-7.281, kl=-5.688
         PG=10.875, KL_penalty=-0.570, total=10.312
  Gen 2: A=-0.438, logp/len=-6.844, kl=-5.312
         PG=-3.000, KL_penalty=-0.531, total=-3.531
  Gen 3: A=-0.447, logp/len=-7.250, kl=-5.625
         PG=-3.250, KL_penalty=-0.562, total=-3.812
  Gen 4: A=-0.610, logp/len=-7.969, kl=-6.375
         PG=-4.875, KL_penalty=-0.637, total=-5.500
  üìä Prompt averages: PG=-0.062, KL=-0.574

üî¢ CORRECTED Loss Breakdown:
  Policy Gradient Loss:    0.000
  KL Loss (with gradients):   -0.516
  Total Loss:          =  -0.516
  KL penalty ratio: 100.0%

üîÑ Performing corrected gradient update (step 42)...
  Total gradient norm: 24.97
  ‚úÖ Gradient update applied with proper KL regularization

==================== PHASE 9: POST-UPDATE PERFORMANCE (step 42) ====================
  Prompt 1: avg reward = 0.145
  Prompt 2: avg reward = 0.387
  Prompt 3: avg reward = 0.630
  Prompt 4: avg reward = 0.559
üìä POST-UPDATE (step 42) Performance:
  Average reward: 0.4302
  Positive ratio: 100.0%

==================== PHASE 10: ANALYSIS (step 42) ====================
üîç GRPO Step Impact (step 42):
  Post-update performance: 0.4302
  Step performance change: +0.0419

======================================================================
üß≠ SEQUENTIAL STEP 43/50
======================================================================
üßæ Overfit mode: reusing initial batch (no dataset slice advance)

ü§ñ Generating completions for GRPO training (step 43)...

üéØ Prompt 1: P: 2k4r/ppp1q1p1/nbb1N3/4PP2/3p2P1/PB5P/1PPK3R/3RQ3 b - - 3 ...
    Gen 1: Train_LP= -572.0, Ref_LP=  -35.0
           Text:                                M: h8e8 a6c5 c6d7 c8b8 c6b5  ...
           Normalized: M: h8e8 a6c5 c6d7 c8b8 c6b5 E: -3.75 -3.28 -3.8 -3.55 -3.88 ...
    Gen 2: Train_LP= -688.0, Ref_LP=  -33.8
           Text:                                M: a6c5 c8b8 h8e8 e7h4 e7d7  ...
           Normalized: M: a6c5 c8b8 h8e8 e7h4 e7d7 E: -3.89 -4.03 -4.14 -4.14 -4.1 ...
    Gen 3: Train_LP= -620.0, Ref_LP=  -46.0
           Text:                                M: h8h4 d4d3 e7h4 a6c5 c8b8  ...
           Normalized: M: h8h4 d4d3 e7h4 a6c5 c8b8 E: -3.77 -3.59 -4.02 -3.63 -3.66...
    Gen 4: Train_LP= -548.0, Ref_LP=  -44.8
           Text:                                M: e7h4 c8b8 d4d3 a6c5 c6d7  ...
           Normalized: M: e7h4 c8b8 d4d3 a6c5 c6d7 E: -4.17 -4.05 -3.87 -3.86 -4.24...

üéØ Prompt 2: P: 2N5/2k5/3p1p2/3P1BbP/4Pp2/1n6/4K3/8 w - - 9 49...
    Gen 1: Train_LP= -660.0, Ref_LP=  -58.0
           Text:                                             M: e2f1 e2f2 c8e...
           Normalized: M: e2f1 e2f2 c8e7 e2f3 c8a7 E: 0.19 0.26 0.3 0.2 0.22 B: c8e...
    Gen 2: Train_LP= -384.0, Ref_LP=  -13.7
           Text:                                             M: e2f2 e2f1 c8e...
           Normalized: M: e2f2 e2f1 c8e7 c8a7 e2f3 E: 0.0 0.0 0.0 0.0 0.0 B: e2f1...
    Gen 3: Train_LP= -812.0, Ref_LP=  -80.5
           Text:                                             M: e2d3 c8a7 e2f...
           Normalized: M: e2d3 c8a7 e2f3 e2f2 c8e7 E: 0.24 0.13 0.21 0.24 0.24 B: e...
    Gen 4: Train_LP= -422.0, Ref_LP=  -81.5
           Text:                                             M: c8a7 c8e7 e2f...
           Normalized: M: c8a7 c8e7 e2f1 e2f2 e2f3 E: 0.42 0.54 0.38 0.39 0.49 B: c...

üéØ Prompt 3: A: 8/2k2b2/R1p4r/3p1p2/P7/2P2B1P/5PPK/4R3 w - - 1 31+a6a5+f7...
    Gen 1: Train_LP= -312.0, Ref_LP=   -0.0
           Text: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
           Normalized: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
    Gen 2: Train_LP= -260.0, Ref_LP=   -0.0
           Text: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
           Normalized: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
    Gen 3: Train_LP= -296.0, Ref_LP=   -0.0
           Text: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
           Normalized: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
    Gen 4: Train_LP= -362.0, Ref_LP=   -0.0
           Text: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
           Normalized: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...

üéØ Prompt 4: P: rnbqkbnr/pp1ppppp/2p5/8/4P3/2N5/PPPP1PPP/R1BQKBNR b KQkq ...
    Gen 1: Train_LP= -492.0, Ref_LP=  -24.5
           Text:                             M: e7e5 d7d5 d7d6 g7g6 c6c5     ...
           Normalized: M: e7e5 d7d5 d7d6 g7g6 c6c5 E: -0.55 -0.43 -0.77 -0.76 -0.74...
    Gen 2: Train_LP= -528.0, Ref_LP=  -26.1
           Text:                             M: d7d6 d7d5 e7e5 g7g6 a7a5     ...
           Normalized: M: d7d6 d7d5 e7e5 g7g6 a7a5 E: -0.62 -0.41 -0.49 -0.62 -0.67...
    Gen 3: Train_LP= -388.0, Ref_LP=  -24.0
           Text:                             M: d7d6 d7d5 e7e5 g7g6 c6c5     ...
           Normalized: M: d7d6 d7d5 e7e5 g7g6 c6c5 E: -0.58 -0.43 -0.51 -0.63 -0.79...
    Gen 4: Train_LP= -620.0, Ref_LP=  -24.6
           Text:                             M: d7d6 g7g6 e7e5 d7d5 c6c5     ...
           Normalized: M: d7d6 g7g6 e7e5 d7d5 c6c5 E: -0.76 -0.83 -0.54 -0.51 -0.84...

==================== PHASE 4: REWARD CALCULATION (step 43) ====================

üéØ Scoring Prompt 1 completions:
  Gen 1: Reward= 0.201 | M: h8e8 a6c5 c6d7 c8b8 c6b5 E: -3.75 -3....
  Gen 2: Reward= 0.187 | M: a6c5 c8b8 h8e8 e7h4 e7d7 E: -3.89 -4....
  Gen 3: Reward= 0.179 | M: h8h4 d4d3 e7h4 a6c5 c8b8 E: -3.77 -3....
  Gen 4: Reward= 0.249 | M: e7h4 c8b8 d4d3 a6c5 c6d7 E: -4.17 -4....
  üìä Average reward: 0.204

üéØ Scoring Prompt 2 completions:
  Gen 1: Reward= 0.489 | M: e2f1 e2f2 c8e7 e2f3 c8a7 E: 0.19 0.26...
  Gen 2: Reward=-0.021 | M: e2f2 e2f1 c8e7 c8a7 e2f3 E: 0.0 0.0 0...
  Gen 3: Reward= 0.099 | M: e2d3 c8a7 e2f3 e2f2 c8e7 E: 0.24 0.13...
  Gen 4: Reward= 0.469 | M: c8a7 c8e7 e2f1 e2f2 e2f3 E: 0.42 0.54...
  üìä Average reward: 0.259

üéØ Scoring Prompt 3 completions:
  Gen 1: Reward= 0.630 | 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b...
  Gen 2: Reward= 0.630 | 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b...
  Gen 3: Reward= 0.630 | 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b...
  Gen 4: Reward= 0.630 | 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b...
  üìä Average reward: 0.630

üéØ Scoring Prompt 4 completions:
  Gen 1: Reward= 0.587 | M: e7e5 d7d5 d7d6 g7g6 c6c5 E: -0.55 -0....
  Gen 2: Reward= 0.353 | M: d7d6 d7d5 e7e5 g7g6 a7a5 E: -0.62 -0....
  Gen 3: Reward= 0.686 | M: d7d6 d7d5 e7e5 g7g6 c6c5 E: -0.58 -0....
  Gen 4: Reward= 0.699 | M: d7d6 g7g6 e7e5 d7d5 c6c5 E: -0.76 -0....
  üìä Average reward: 0.581

==================== PHASE 5: TRL EXACT ADVANTAGE CALCULATION (step 43) ====================
üî¢ TRL Advantage Calculation (exact formula):
  Total rewards shape: torch.Size([16])
  Rewards: [ 0.2006      0.187       0.17857143  0.24937143  0.48857143 -0.02142857
  0.09857143  0.46857143  0.63        0.63        0.63        0.63
  0.5868      0.35337143  0.6856      0.6992    ]

==================== PHASE 6-8: CORRECTED GRPO LOSS CALCULATION (step 43) ====================
üîÑ Computing GRPO loss with proper token-level KL...

üéØ Prompt 1 - Corrected GRPO Loss:
  Gen 1: A=-0.104, logp/len=-6.625, kl=-4.562
         PG=-0.688, KL_penalty=-0.457, total=-1.141
  Gen 2: A=-0.533, logp/len=-7.719, kl=-5.781
         PG=-4.125, KL_penalty=-0.578, total=-4.688
  Gen 3: A=-0.800, logp/len=-5.031, kl=-3.281
         PG=-4.031, KL_penalty=-0.328, total=-4.375
  Gen 4: A=+1.437, logp/len=-6.438, kl=-4.625
         PG=9.250, KL_penalty=-0.463, total=8.812
  üìä Prompt averages: PG=0.094, KL=-0.455

üéØ Prompt 2 - Corrected GRPO Loss:
  Gen 1: A=+0.889, logp/len=-7.062, kl=-5.156
         PG=6.281, KL_penalty=-0.516, total=5.750
  Gen 2: A=-1.082, logp/len=-6.188, kl=-4.500
         PG=-6.688, KL_penalty=-0.449, total=-7.125
  Gen 3: A=-0.618, logp/len=-6.562, kl=-4.719
         PG=-4.062, KL_penalty=-0.473, total=-4.531
  Gen 4: A=+0.811, logp/len=-6.656, kl=-4.594
         PG=5.406, KL_penalty=-0.459, total=4.938
  üìä Prompt averages: PG=0.234, KL=-0.475

üéØ Prompt 3 - Corrected GRPO Loss:
  Gen 1: A=+0.000, logp/len=-6.625, kl=-6.281
         PG=0.000, KL_penalty=-0.629, total=-0.629
  Gen 2: A=+0.000, logp/len=-6.094, kl=-5.719
         PG=0.000, KL_penalty=-0.570, total=-0.570
  Gen 3: A=+0.000, logp/len=-5.438, kl=-5.094
         PG=0.000, KL_penalty=-0.508, total=-0.508
  Gen 4: A=+0.000, logp/len=-6.625, kl=-6.250
         PG=0.000, KL_penalty=-0.625, total=-0.625
  üìä Prompt averages: PG=0.000, KL=-0.586

üéØ Prompt 4 - Corrected GRPO Loss:
  Gen 1: A=+0.035, logp/len=-7.469, kl=-5.812
         PG=0.260, KL_penalty=-0.582, total=-0.322
  Gen 2: A=-1.425, logp/len=-7.562, kl=-6.062
         PG=-10.750, KL_penalty=-0.605, total=-11.375
  Gen 3: A=+0.652, logp/len=-8.000, kl=-6.438
         PG=5.219, KL_penalty=-0.645, total=4.562
  Gen 4: A=+0.737, logp/len=-7.156, kl=-5.406
         PG=5.281, KL_penalty=-0.539, total=4.750
  üìä Prompt averages: PG=0.000, KL=-0.594

üî¢ CORRECTED Loss Breakdown:
  Policy Gradient Loss:    0.082
  KL Loss (with gradients):   -0.527
  Total Loss:          =  -0.445
  KL penalty ratio: 86.5%

üîÑ Performing corrected gradient update (step 43)...
  Total gradient norm: 15.11
  ‚úÖ Gradient update applied with proper KL regularization

==================== PHASE 9: POST-UPDATE PERFORMANCE (step 43) ====================
  Prompt 1: avg reward = 0.145
  Prompt 2: avg reward = 0.212
  Prompt 3: avg reward = 0.630
  Prompt 4: avg reward = 0.559
üìä POST-UPDATE (step 43) Performance:
  Average reward: 0.3865
  Positive ratio: 87.5%

==================== PHASE 10: ANALYSIS (step 43) ====================
üîç GRPO Step Impact (step 43):
  Post-update performance: 0.3865
  Step performance change: -0.0437

======================================================================
üß≠ SEQUENTIAL STEP 44/50
======================================================================
üßæ Overfit mode: reusing initial batch (no dataset slice advance)

ü§ñ Generating completions for GRPO training (step 44)...

üéØ Prompt 1: P: 2k4r/ppp1q1p1/nbb1N3/4PP2/3p2P1/PB5P/1PPK3R/3RQ3 b - - 3 ...
    Gen 1: Train_LP= -588.0, Ref_LP=  -35.5
           Text:                                M: d4d3 e7e8 a6c5 c6d7 e7f7  ...
           Normalized: M: d4d3 e7e8 a6c5 c6d7 e7f7 E: -3.75 -4.0 -3.51 -3.79 -3.71 ...
    Gen 2: Train_LP= -500.0, Ref_LP=  -44.2
           Text:                                M: a6c5 b6a5 e7h4 e7d7 d4d3  ...
           Normalized: M: a6c5 b6a5 e7h4 e7d7 d4d3 E: -3.89 -3.88 -4.04 -4.16 -3.87...
    Gen 3: Train_LP= -568.0, Ref_LP=  -33.0
           Text:                                M: d4d3 a6c5 e7e8 e7d7 c6d7  ...
           Normalized: M: d4d3 a6c5 e7e8 e7d7 c6d7 E: -4.2 -4.14 -4.23 -4.25 -4.23 ...
    Gen 4: Train_LP= -748.0, Ref_LP=  -44.2
           Text:                                M: a6c5 d4d3 c8b8 h8e8 c6d7  ...
           Normalized: M: a6c5 d4d3 c8b8 h8e8 c6d7 E: -3.19 -3.54 -3.77 -3.59 -3.83...

üéØ Prompt 2: P: 2N5/2k5/3p1p2/3P1BbP/4Pp2/1n6/4K3/8 w - - 9 49...
    Gen 1: Train_LP= -506.0, Ref_LP=  -79.0
           Text:                                             M: e2f3 c8e7 e2f...
           Normalized: M: e2f3 c8e7 e2f2 c8a7 e2d3 E: 0.21 0.24 0.15 0.15 0.16 B: c...
    Gen 2: Train_LP= -616.0, Ref_LP=  -16.5
           Text:                                             M: f5g6 e2f2 c8e...
           Normalized: M: f5g6 e2f2 c8e7 e2f3 f5g4 E: 0.0 0.0 0.0 0.0 0.0 B: f5g6...
    Gen 3: Train_LP= -564.0, Ref_LP=  -50.2
           Text:                                             M: c8e7 e2f3 e2f...
           Normalized: M: c8e7 e2f3 e2f1 e2f2 e2d1 E: 0.4 0.49 0.39 0.4 0.4 B: e2f3...
    Gen 4: Train_LP= -768.0, Ref_LP=  -29.9
           Text:                                             M: f5g6 e2f3 c8e...
           Normalized: M: f5g6 e2f3 c8e7 f5h7 c8a7 E: 0.02 0.0 0.0 0.0 0.0 B: f5g6...

üéØ Prompt 3: A: 8/2k2b2/R1p4r/3p1p2/P7/2P2B1P/5PPK/4R3 w - - 1 31+a6a5+f7...
    Gen 1: Train_LP= -298.0, Ref_LP=   -0.0
           Text: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
           Normalized: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
    Gen 2: Train_LP= -316.0, Ref_LP=   -0.0
           Text: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
           Normalized: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
    Gen 3: Train_LP= -256.0, Ref_LP=   -0.0
           Text: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
           Normalized: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
    Gen 4: Train_LP= -256.0, Ref_LP=   -0.0
           Text: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
           Normalized: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...

üéØ Prompt 4: P: rnbqkbnr/pp1ppppp/2p5/8/4P3/2N5/PPPP1PPP/R1BQKBNR b KQkq ...
    Gen 1: Train_LP= -466.0, Ref_LP=  -45.2
           Text:                             M: d7d5 d7d6 e7e5 g7g6 e7e6     ...
           Normalized: M: d7d5 d7d6 e7e5 g7g6 e7e6 E: -0.45 -0.67 -0.52 -0.76 -0.85...
    Gen 2: Train_LP= -494.0, Ref_LP=  -46.2
           Text:                             M: d7d6 d7d5 e7e5 e7e6 c6c5     ...
           Normalized: M: d7d6 d7d5 e7e5 e7e6 c6c5 E: -0.61 -0.42 -0.56 -0.76 -0.72...
    Gen 3: Train_LP= -604.0, Ref_LP=  -25.0
           Text:                             M: d7d5 d7d6 e7e5 g7g6 d8c7     ...
           Normalized: M: d7d5 d7d6 e7e5 g7g6 d8c7 E: -0.35 -0.58 -0.5 -0.7 -0.75 B...
    Gen 4: Train_LP= -564.0, Ref_LP=  -35.2
           Text:                             M: e7e5 d7d5 d7d6 c6c5 e7e6     ...
           Normalized: M: e7e5 d7d5 d7d6 c6c5 e7e6 E: -0.67 -0.4 -0.65 -0.68 -0.76 ...

==================== PHASE 4: REWARD CALCULATION (step 44) ====================

üéØ Scoring Prompt 1 completions:
  Gen 1: Reward= 0.125 | M: d4d3 e7e8 a6c5 c6d7 e7f7 E: -3.75 -4....
  Gen 2: Reward= 0.205 | M: a6c5 b6a5 e7h4 e7d7 d4d3 E: -3.89 -3....
  Gen 3: Reward= 0.223 | M: d4d3 a6c5 e7e8 e7d7 c6d7 E: -4.2 -4.1...
  Gen 4: Reward= 0.220 | M: a6c5 d4d3 c8b8 h8e8 c6d7 E: -3.19 -3....
  üìä Average reward: 0.193

üéØ Scoring Prompt 2 completions:
  Gen 1: Reward= 0.549 | M: e2f3 c8e7 e2f2 c8a7 e2d3 E: 0.21 0.24...
  Gen 2: Reward=-0.055 | M: f5g6 e2f2 c8e7 e2f3 f5g4 E: 0.0 0.0 0...
  Gen 3: Reward= 0.049 | M: c8e7 e2f3 e2f1 e2f2 e2d1 E: 0.4 0.49 ...
  Gen 4: Reward=-0.075 | M: f5g6 e2f3 c8e7 f5h7 c8a7 E: 0.02 0.0 ...
  üìä Average reward: 0.117

üéØ Scoring Prompt 3 completions:
  Gen 1: Reward= 0.630 | 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b...
  Gen 2: Reward= 0.630 | 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b...
  Gen 3: Reward= 0.630 | 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b...
  Gen 4: Reward= 0.630 | 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b...
  üìä Average reward: 0.630

üéØ Scoring Prompt 4 completions:
  Gen 1: Reward= 0.399 | M: d7d5 d7d6 e7e5 g7g6 e7e6 E: -0.45 -0....
  Gen 2: Reward= 0.371 | M: d7d6 d7d5 e7e5 e7e6 c6c5 E: -0.61 -0....
  Gen 3: Reward= 0.394 | M: d7d5 d7d6 e7e5 g7g6 d8c7 E: -0.35 -0....
  Gen 4: Reward= 0.366 | M: e7e5 d7d5 d7d6 c6c5 e7e6 E: -0.67 -0....
  üìä Average reward: 0.383

==================== PHASE 5: TRL EXACT ADVANTAGE CALCULATION (step 44) ====================
üî¢ TRL Advantage Calculation (exact formula):
  Total rewards shape: torch.Size([16])
  Rewards: [ 0.125       0.20537143  0.2226      0.21977143  0.54857143 -0.055
  0.04857143 -0.075       0.63        0.63        0.63        0.63
  0.39897143  0.37137143  0.39377143  0.36617143]

==================== PHASE 6-8: CORRECTED GRPO LOSS CALCULATION (step 44) ====================
üîÑ Computing GRPO loss with proper token-level KL...

üéØ Prompt 1 - Corrected GRPO Loss:
  Gen 1: A=-1.480, logp/len=-6.531, kl=-4.625
         PG=-9.688, KL_penalty=-0.463, total=-10.125
  Gen 2: A=+0.264, logp/len=-7.156, kl=-5.469
         PG=1.891, KL_penalty=-0.547, total=1.344
  Gen 3: A=+0.638, logp/len=-6.844, kl=-5.000
         PG=4.375, KL_penalty=-0.500, total=3.875
  Gen 4: A=+0.577, logp/len=-6.188, kl=-4.375
         PG=3.562, KL_penalty=-0.438, total=3.125
  üìä Prompt averages: PG=0.031, KL=-0.486

üéØ Prompt 2 - Corrected GRPO Loss:
  Gen 1: A=+1.474, logp/len=-8.812, kl=-6.781
         PG=13.000, KL_penalty=-0.680, total=12.312
  Gen 2: A=-0.586, logp/len=-6.719, kl=-5.031
         PG=-3.938, KL_penalty=-0.504, total=-4.438
  Gen 3: A=-0.233, logp/len=-4.969, kl=-2.875
         PG=-1.156, KL_penalty=-0.287, total=-1.445
  Gen 4: A=-0.655, logp/len=-7.438, kl=-5.719
         PG=-4.875, KL_penalty=-0.570, total=-5.438
  üìä Prompt averages: PG=0.758, KL=-0.512

üéØ Prompt 3 - Corrected GRPO Loss:
  Gen 1: A=+0.000, logp/len=-4.750, kl=-4.375
         PG=0.000, KL_penalty=-0.438, total=-0.438
  Gen 2: A=+0.000, logp/len=-5.062, kl=-4.688
         PG=0.000, KL_penalty=-0.469, total=-0.469
  Gen 3: A=+0.000, logp/len=-6.688, kl=-6.312
         PG=0.000, KL_penalty=-0.633, total=-0.633
  Gen 4: A=+0.000, logp/len=-5.406, kl=-5.062
         PG=0.000, KL_penalty=-0.508, total=-0.508
  üìä Prompt averages: PG=0.000, KL=-0.512

üéØ Prompt 4 - Corrected GRPO Loss:
  Gen 1: A=+1.011, logp/len=-5.750, kl=-4.125
         PG=5.812, KL_penalty=-0.412, total=5.406
  Gen 2: A=-0.691, logp/len=-7.375, kl=-5.688
         PG=-5.094, KL_penalty=-0.570, total=-5.656
  Gen 3: A=+0.691, logp/len=-8.625, kl=-7.094
         PG=5.969, KL_penalty=-0.711, total=5.250
  Gen 4: A=-1.011, logp/len=-8.375, kl=-6.562
         PG=-8.500, KL_penalty=-0.656, total=-9.125
  üìä Prompt averages: PG=-0.453, KL=-0.586

üî¢ CORRECTED Loss Breakdown:
  Policy Gradient Loss:    0.084
  KL Loss (with gradients):   -0.523
  Total Loss:          =  -0.439
  KL penalty ratio: 86.0%

üîÑ Performing corrected gradient update (step 44)...
  Total gradient norm: 21.67
  ‚úÖ Gradient update applied with proper KL regularization

==================== PHASE 9: POST-UPDATE PERFORMANCE (step 44) ====================
  Prompt 1: avg reward = 0.145
  Prompt 2: avg reward = 0.387
  Prompt 3: avg reward = 0.630
  Prompt 4: avg reward = 0.559
üìä POST-UPDATE (step 44) Performance:
  Average reward: 0.4302
  Positive ratio: 100.0%

==================== PHASE 10: ANALYSIS (step 44) ====================
üîç GRPO Step Impact (step 44):
  Post-update performance: 0.4302
  Step performance change: +0.0437

======================================================================
üß≠ SEQUENTIAL STEP 45/50
======================================================================
üßæ Overfit mode: reusing initial batch (no dataset slice advance)

ü§ñ Generating completions for GRPO training (step 45)...

üéØ Prompt 1: P: 2k4r/ppp1q1p1/nbb1N3/4PP2/3p2P1/PB5P/1PPK3R/3RQ3 b - - 3 ...
    Gen 1: Train_LP= -524.0, Ref_LP=  -45.0
           Text:                                M: e7e8 a6c5 d4d3 c8b8 e7f7  ...
           Normalized: M: e7e8 a6c5 d4d3 c8b8 e7f7 E: -3.68 -3.3 -3.52 -3.54 -3.53 ...
    Gen 2: Train_LP= -454.0, Ref_LP=  -32.2
           Text:                                M: e7d7 c8b8 a6c5 h8e8 d4d3  ...
           Normalized: M: e7d7 c8b8 a6c5 h8e8 d4d3 E: -3.69 -3.7 -3.32 -3.7 -3.69 B...
    Gen 3: Train_LP= -572.0, Ref_LP=  -44.0
           Text:                                M: b6a5 e7d7 a6c5 c8b8 h8e8  ...
           Normalized: M: b6a5 e7d7 a6c5 c8b8 h8e8 E: -4.17 -4.21 -3.71 -4.2 -4.05 ...
    Gen 4: Train_LP= -510.0, Ref_LP=  -48.0
           Text:                                M: a6c5 h8h4 b6a5 h8h6 c6f3  ...
           Normalized: M: a6c5 h8h4 b6a5 h8h6 c6f3 E: -3.18 -3.62 -3.6 -3.78 -3.65 ...

üéØ Prompt 2: P: 2N5/2k5/3p1p2/3P1BbP/4Pp2/1n6/4K3/8 w - - 9 49...
    Gen 1: Train_LP= -468.0, Ref_LP=  -69.0
           Text:                                             M: e2f1 c8e7 e2f...
           Normalized: M: e2f1 c8e7 e2f3 e2f2 c8a7 E: 0.2 0.26 0.27 0.24 0.18 B: e2...
    Gen 2: Train_LP= -520.0, Ref_LP=  -13.7
           Text:                                             M: e2f2 e2f1 c8e...
           Normalized: M: e2f2 e2f1 c8e7 c8a7 e2f3 E: 0.0 0.0 0.0 0.0 0.0 B: e2f1...
    Gen 3: Train_LP= -652.0, Ref_LP=  -79.5
           Text:                                             M: e2f3 e2f2 c8e...
           Normalized: M: e2f3 e2f2 c8e7 e2d3 c8a7 E: 0.24 0.24 0.22 0.21 0.24 B: e...
    Gen 4: Train_LP= -432.0, Ref_LP=  -14.3
           Text:                                             M: e2d3 e2f3 c8a...
           Normalized: M: e2d3 e2f3 c8a7 e2f2 c8e7 E: 0.0 0.0 0.0 0.0 0.0 B: e2f3...

üéØ Prompt 3: A: 8/2k2b2/R1p4r/3p1p2/P7/2P2B1P/5PPK/4R3 w - - 1 31+a6a5+f7...
    Gen 1: Train_LP= -216.0, Ref_LP=   -0.0
           Text: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
           Normalized: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
    Gen 2: Train_LP= -344.0, Ref_LP=   -0.0
           Text: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
           Normalized: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
    Gen 3: Train_LP= -255.0, Ref_LP=   -0.0
           Text: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
           Normalized: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
    Gen 4: Train_LP= -310.0, Ref_LP=   -0.0
           Text: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
           Normalized: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...

üéØ Prompt 4: P: rnbqkbnr/pp1ppppp/2p5/8/4P3/2N5/PPPP1PPP/R1BQKBNR b KQkq ...
    Gen 1: Train_LP= -422.0, Ref_LP=  -24.1
           Text:                             M: d7d5 g7g6 e7e5 d7d6 e7e6     ...
           Normalized: M: d7d5 g7g6 e7e5 d7d6 e7e6 E: -0.32 -0.74 -0.63 -0.64 -0.82...
    Gen 2: Train_LP= -632.0, Ref_LP=  -23.8
           Text:                             M: d7d5 d7d6 g7g6 e7e5 e7e6     ...
           Normalized: M: d7d5 d7d6 g7g6 e7e5 e7e6 E: -0.38 -0.65 -0.74 -0.64 -0.89...
    Gen 3: Train_LP= -596.0, Ref_LP=  -24.2
           Text:                             M: d7d6 e7e5 g7g6 d7d5 g8f6     ...
           Normalized: M: d7d6 e7e5 g7g6 d7d5 g8f6 E: -0.65 -0.51 -0.66 -0.51 -0.68...
    Gen 4: Train_LP= -508.0, Ref_LP=  -23.8
           Text:                             M: d7d5 g7g6 d7d6 e7e5 e7e6     ...
           Normalized: M: d7d5 g7g6 d7d6 e7e5 e7e6 E: -0.37 -0.75 -0.69 -0.59 -0.78...

==================== PHASE 4: REWARD CALCULATION (step 45) ====================

üéØ Scoring Prompt 1 completions:
  Gen 1: Reward= 0.199 | M: e7e8 a6c5 d4d3 c8b8 e7f7 E: -3.68 -3....
  Gen 2: Reward= 0.243 | M: e7d7 c8b8 a6c5 h8e8 d4d3 E: -3.69 -3....
  Gen 3: Reward= 0.257 | M: b6a5 e7d7 a6c5 c8b8 h8e8 E: -4.17 -4....
  Gen 4: Reward= 0.179 | M: a6c5 h8h4 b6a5 h8h6 c6f3 E: -3.18 -3....
  üìä Average reward: 0.219

üéØ Scoring Prompt 2 completions:
  Gen 1: Reward= 0.019 | M: e2f1 c8e7 e2f3 e2f2 c8a7 E: 0.2 0.26 ...
  Gen 2: Reward=-0.021 | M: e2f2 e2f1 c8e7 c8a7 e2f3 E: 0.0 0.0 0...
  Gen 3: Reward= 0.119 | M: e2f3 e2f2 c8e7 e2d3 c8a7 E: 0.24 0.24...
  Gen 4: Reward= 0.029 | M: e2d3 e2f3 c8a7 e2f2 c8e7 E: 0.0 0.0 0...
  üìä Average reward: 0.036

üéØ Scoring Prompt 3 completions:
  Gen 1: Reward= 0.630 | 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b...
  Gen 2: Reward= 0.630 | 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b...
  Gen 3: Reward= 0.630 | 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b...
  Gen 4: Reward= 0.630 | 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b...
  üìä Average reward: 0.630

üéØ Scoring Prompt 4 completions:
  Gen 1: Reward= 0.414 | M: d7d5 g7g6 e7e5 d7d6 e7e6 E: -0.32 -0....
  Gen 2: Reward= 0.393 | M: d7d5 d7d6 g7g6 e7e5 e7e6 E: -0.38 -0....
  Gen 3: Reward= 0.479 | M: d7d6 e7e5 g7g6 d7d5 g8f6 E: -0.65 -0....
  Gen 4: Reward= 0.417 | M: d7d5 g7g6 d7d6 e7e5 e7e6 E: -0.37 -0....
  üìä Average reward: 0.426

==================== PHASE 5: TRL EXACT ADVANTAGE CALCULATION (step 45) ====================
üî¢ TRL Advantage Calculation (exact formula):
  Total rewards shape: torch.Size([16])
  Rewards: [ 0.19857143  0.24257143  0.25737143  0.17937143  0.01857143 -0.02142857
  0.11857143  0.02857143  0.63        0.63        0.63        0.63
  0.41417143  0.39297143  0.4788      0.41697143]

==================== PHASE 6-8: CORRECTED GRPO LOSS CALCULATION (step 45) ====================
üîÑ Computing GRPO loss with proper token-level KL...

üéØ Prompt 1 - Corrected GRPO Loss:
  Gen 1: A=-0.571, logp/len=-6.469, kl=-4.750
         PG=-3.703, KL_penalty=-0.475, total=-4.188
  Gen 2: A=+0.631, logp/len=-8.688, kl=-6.969
         PG=5.500, KL_penalty=-0.695, total=4.812
  Gen 3: A=+1.036, logp/len=-9.062, kl=-7.438
         PG=9.375, KL_penalty=-0.742, total=8.625
  Gen 4: A=-1.096, logp/len=-6.469, kl=-4.719
         PG=-7.094, KL_penalty=-0.473, total=-7.562
  üìä Prompt averages: PG=1.023, KL=-0.598

üéØ Prompt 2 - Corrected GRPO Loss:
  Gen 1: A=-0.296, logp/len=-6.812, kl=-4.906
         PG=-2.016, KL_penalty=-0.490, total=-2.500
  Gen 2: A=-0.973, logp/len=-4.938, kl=-3.266
         PG=-4.812, KL_penalty=-0.326, total=-5.125
  Gen 3: A=+1.396, logp/len=-8.688, kl=-6.844
         PG=12.125, KL_penalty=-0.684, total=11.438
  Gen 4: A=-0.127, logp/len=-7.781, kl=-6.188
         PG=-0.988, KL_penalty=-0.617, total=-1.609
  üìä Prompt averages: PG=1.078, KL=-0.531

üéØ Prompt 3 - Corrected GRPO Loss:
  Gen 1: A=+0.000, logp/len=-5.156, kl=-4.781
         PG=0.000, KL_penalty=-0.479, total=-0.479
  Gen 2: A=+0.000, logp/len=-6.469, kl=-6.125
         PG=0.000, KL_penalty=-0.613, total=-0.613
  Gen 3: A=+0.000, logp/len=-6.125, kl=-5.750
         PG=0.000, KL_penalty=-0.574, total=-0.574
  Gen 4: A=+0.000, logp/len=-4.969, kl=-4.625
         PG=0.000, KL_penalty=-0.463, total=-0.463
  üìä Prompt averages: PG=0.000, KL=-0.535

üéØ Prompt 4 - Corrected GRPO Loss:
  Gen 1: A=-0.313, logp/len=-7.156, kl=-5.438
         PG=-2.234, KL_penalty=-0.543, total=-2.781
  Gen 2: A=-0.886, logp/len=-7.375, kl=-5.594
         PG=-6.531, KL_penalty=-0.559, total=-7.094
  Gen 3: A=+1.436, logp/len=-6.688, kl=-5.156
         PG=9.625, KL_penalty=-0.516, total=9.125
  Gen 4: A=-0.237, logp/len=-8.625, kl=-6.875
         PG=-2.047, KL_penalty=-0.688, total=-2.734
  üìä Prompt averages: PG=-0.293, KL=-0.578

üî¢ CORRECTED Loss Breakdown:
  Policy Gradient Loss:    0.449
  KL Loss (with gradients):   -0.559
  Total Loss:          =  -0.109
  KL penalty ratio: 55.5%

üîÑ Performing corrected gradient update (step 45)...
  Total gradient norm: 36.59
  ‚úÖ Gradient update applied with proper KL regularization

==================== PHASE 9: POST-UPDATE PERFORMANCE (step 45) ====================
  Prompt 1: avg reward = 0.145
  Prompt 2: avg reward = 0.387
  Prompt 3: avg reward = 0.630
  Prompt 4: avg reward = 0.559
üìä POST-UPDATE (step 45) Performance:
  Average reward: 0.4302
  Positive ratio: 100.0%

==================== PHASE 10: ANALYSIS (step 45) ====================
üîç GRPO Step Impact (step 45):
  Post-update performance: 0.4302
  Step performance change: +0.0000

======================================================================
üß≠ SEQUENTIAL STEP 46/50
======================================================================
üßæ Overfit mode: reusing initial batch (no dataset slice advance)

ü§ñ Generating completions for GRPO training (step 46)...

üéØ Prompt 1: P: 2k4r/ppp1q1p1/nbb1N3/4PP2/3p2P1/PB5P/1PPK3R/3RQ3 b - - 3 ...
    Gen 1: Train_LP= -346.0, Ref_LP=  -34.2
           Text:                                M: c8b8 a6c5 e7d7 e7e8 c6d7  ...
           Normalized: M: c8b8 a6c5 e7d7 e7e8 c6d7 E: -4.27 -3.69 -4.1 -4.09 -3.84 ...
    Gen 2: Train_LP= -438.0, Ref_LP=  -34.2
           Text:                                M: c6d7 c6b5 a6c5 d4d3 c8b8  ...
           Normalized: M: c6d7 c6b5 a6c5 d4d3 c8b8 E: -3.52 -3.72 -3.3 -3.48 -3.57 ...
    Gen 3: Train_LP= -504.0, Ref_LP=  -44.5
           Text:                                M: d4d3 c6d7 a6c5 c8b8 h8e8  ...
           Normalized: M: d4d3 c6d7 a6c5 c8b8 h8e8 E: -4.06 -3.77 -3.69 -3.73 -4.05...
    Gen 4: Train_LP= -430.0, Ref_LP=  -36.0
           Text:                                M: d4d3 c6d7 a6c5 h8h3 e7h4  ...
           Normalized: M: d4d3 c6d7 a6c5 h8h3 e7h4 E: -3.72 -3.9 -3.68 -3.73 -4.16 ...

üéØ Prompt 2: P: 2N5/2k5/3p1p2/3P1BbP/4Pp2/1n6/4K3/8 w - - 9 49...
    Gen 1: Train_LP= -524.0, Ref_LP=  -13.4
           Text:                                             M: e2f2 c8a7 e2f...
           Normalized: M: e2f2 c8a7 e2f3 c8e7 e2f1 E: 0.0 0.0 0.0 0.0 0.0 B: e2f3...
    Gen 2: Train_LP= -800.0, Ref_LP=  -81.0
           Text:                                             M: f5e6 e2f3 e2f...
           Normalized: M: f5e6 e2f3 e2f2 f5g4 c8e7 E: 0.27 0.23 0.23 0.25 0.29 B: c...
    Gen 3: Train_LP= -354.0, Ref_LP=  -80.5
           Text:                                             M: e2f3 e2f2 c8e...
           Normalized: M: e2f3 e2f2 c8e7 e2f1 c8a7 E: 0.35 0.33 0.29 0.31 0.35 B: e...
    Gen 4: Train_LP= -760.0, Ref_LP=  -79.0
           Text:                                             M: c8a7 c8e7 e2d...
           Normalized: M: c8a7 c8e7 e2d3 e2f3 e2f2 E: 0.26 0.27 0.24 0.26 0.27 B: e...

üéØ Prompt 3: A: 8/2k2b2/R1p4r/3p1p2/P7/2P2B1P/5PPK/4R3 w - - 1 31+a6a5+f7...
    Gen 1: Train_LP= -241.0, Ref_LP=   -0.0
           Text: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
           Normalized: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
    Gen 2: Train_LP= -272.0, Ref_LP=   -0.0
           Text: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
           Normalized: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
    Gen 3: Train_LP= -254.0, Ref_LP=   -0.0
           Text: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
           Normalized: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
    Gen 4: Train_LP= -262.0, Ref_LP=   -0.0
           Text: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
           Normalized: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...

üéØ Prompt 4: P: rnbqkbnr/pp1ppppp/2p5/8/4P3/2N5/PPPP1PPP/R1BQKBNR b KQkq ...
    Gen 1: Train_LP= -700.0, Ref_LP=  -24.0
           Text:                             M: d7d5 d7d6 e7e5 g7g6 e7e6     ...
           Normalized: M: d7d5 d7d6 e7e5 g7g6 e7e6 E: -0.45 -0.71 -0.49 -0.63 -0.67...
    Gen 2: Train_LP= -672.0, Ref_LP=  -24.4
           Text:                             M: d7d6 e7e6 d7d5 e7e5 g7g6     ...
           Normalized: M: d7d6 e7e6 d7d5 e7e5 g7g6 E: -0.79 -0.82 -0.48 -0.49 -0.71...
    Gen 3: Train_LP= -404.0, Ref_LP=  -24.4
           Text:                             M: d7d6 e7e5 d7d5 e7e6 c6c5     ...
           Normalized: M: d7d6 e7e5 d7d5 e7e6 c6c5 E: -0.76 -0.62 -0.51 -0.79 -0.85...
    Gen 4: Train_LP= -524.0, Ref_LP=  -23.8
           Text:                             M: d7d6 d7d5 e7e5 g7g6 e7e6     ...
           Normalized: M: d7d6 d7d5 e7e5 g7g6 e7e6 E: -0.63 -0.35 -0.67 -0.73 -0.79...

==================== PHASE 4: REWARD CALCULATION (step 46) ====================

üéØ Scoring Prompt 1 completions:
  Gen 1: Reward= 0.360 | M: c8b8 a6c5 e7d7 e7e8 c6d7 E: -4.27 -3....
  Gen 2: Reward= 0.231 | M: c6d7 c6b5 a6c5 d4d3 c8b8 E: -3.52 -3....
  Gen 3: Reward= 0.295 | M: d4d3 c6d7 a6c5 c8b8 h8e8 E: -4.06 -3....
  Gen 4: Reward= 0.125 | M: d4d3 c6d7 a6c5 h8h3 e7h4 E: -3.72 -3....
  üìä Average reward: 0.253

üéØ Scoring Prompt 2 completions:
  Gen 1: Reward=-0.021 | M: e2f2 c8a7 e2f3 c8e7 e2f1 E: 0.0 0.0 0...
  Gen 2: Reward= 0.375 | M: f5e6 e2f3 e2f2 f5g4 c8e7 E: 0.27 0.23...
  Gen 3: Reward= 0.039 | M: e2f3 e2f2 c8e7 e2f1 c8a7 E: 0.35 0.33...
  Gen 4: Reward= 0.449 | M: c8a7 c8e7 e2d3 e2f3 e2f2 E: 0.26 0.27...
  üìä Average reward: 0.210

üéØ Scoring Prompt 3 completions:
  Gen 1: Reward= 0.630 | 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b...
  Gen 2: Reward= 0.630 | 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b...
  Gen 3: Reward= 0.630 | 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b...
  Gen 4: Reward= 0.630 | 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b...
  üìä Average reward: 0.630

üéØ Scoring Prompt 4 completions:
  Gen 1: Reward= 0.539 | M: d7d5 d7d6 e7e5 g7g6 e7e6 E: -0.45 -0....
  Gen 2: Reward= 0.361 | M: d7d6 e7e6 d7d5 e7e5 g7g6 E: -0.79 -0....
  Gen 3: Reward= 0.607 | M: d7d6 e7e5 d7d5 e7e6 c6c5 E: -0.76 -0....
  Gen 4: Reward= 0.365 | M: d7d6 d7d5 e7e5 g7g6 e7e6 E: -0.63 -0....
  üìä Average reward: 0.468

==================== PHASE 5: TRL EXACT ADVANTAGE CALCULATION (step 46) ====================
üî¢ TRL Advantage Calculation (exact formula):
  Total rewards shape: torch.Size([16])
  Rewards: [ 0.3598      0.23137143  0.29457143  0.125      -0.02142857  0.375
  0.03857143  0.44857143  0.63        0.63        0.63        0.63
  0.53857143  0.36097143  0.60697143  0.36497143]

==================== PHASE 6-8: CORRECTED GRPO LOSS CALCULATION (step 46) ====================
üîÑ Computing GRPO loss with proper token-level KL...

üéØ Prompt 1 - Corrected GRPO Loss:
  Gen 1: A=+1.071, logp/len=-5.812, kl=-3.922
         PG=6.219, KL_penalty=-0.393, total=5.812
  Gen 2: A=-0.213, logp/len=-7.438, kl=-5.688
         PG=-1.586, KL_penalty=-0.570, total=-2.156
  Gen 3: A=+0.419, logp/len=-7.438, kl=-5.750
         PG=3.109, KL_penalty=-0.574, total=2.531
  Gen 4: A=-1.277, logp/len=-6.750, kl=-4.812
         PG=-8.625, KL_penalty=-0.480, total=-9.125
  üìä Prompt averages: PG=-0.219, KL=-0.504

üéØ Prompt 2 - Corrected GRPO Loss:
  Gen 1: A=-0.981, logp/len=-7.562, kl=-5.844
         PG=-7.406, KL_penalty=-0.586, total=-8.000
  Gen 2: A=+0.698, logp/len=-8.062, kl=-6.219
         PG=5.625, KL_penalty=-0.621, total=5.000
  Gen 3: A=-0.727, logp/len=-5.688, kl=-3.844
         PG=-4.125, KL_penalty=-0.385, total=-4.500
  Gen 4: A=+1.010, logp/len=-8.375, kl=-6.250
         PG=8.438, KL_penalty=-0.625, total=7.812
  üìä Prompt averages: PG=0.633, KL=-0.555

üéØ Prompt 3 - Corrected GRPO Loss:
  Gen 1: A=+0.000, logp/len=-6.188, kl=-5.844
         PG=0.000, KL_penalty=-0.586, total=-0.586
  Gen 2: A=+0.000, logp/len=-5.312, kl=-4.969
         PG=0.000, KL_penalty=-0.496, total=-0.496
  Gen 3: A=+0.000, logp/len=-6.188, kl=-5.812
         PG=0.000, KL_penalty=-0.582, total=-0.582
  Gen 4: A=+0.000, logp/len=-5.188, kl=-4.812
         PG=0.000, KL_penalty=-0.480, total=-0.480
  üìä Prompt averages: PG=0.000, KL=-0.535

üéØ Prompt 4 - Corrected GRPO Loss:
  Gen 1: A=+0.569, logp/len=-6.156, kl=-4.531
         PG=3.500, KL_penalty=-0.453, total=3.047
  Gen 2: A=-0.860, logp/len=-7.594, kl=-5.938
         PG=-6.531, KL_penalty=-0.594, total=-7.125
  Gen 3: A=+1.119, logp/len=-7.469, kl=-5.781
         PG=8.375, KL_penalty=-0.578, total=7.812
  Gen 4: A=-0.828, logp/len=-8.062, kl=-6.438
         PG=-6.688, KL_penalty=-0.645, total=-7.344
  üìä Prompt averages: PG=-0.336, KL=-0.566

üî¢ CORRECTED Loss Breakdown:
  Policy Gradient Loss:    0.020
  KL Loss (with gradients):   -0.539
  Total Loss:          =  -0.520
  KL penalty ratio: 96.5%

üîÑ Performing corrected gradient update (step 46)...
  Total gradient norm: 28.22
  ‚úÖ Gradient update applied with proper KL regularization

==================== PHASE 9: POST-UPDATE PERFORMANCE (step 46) ====================
  Prompt 1: avg reward = 0.145
  Prompt 2: avg reward = 0.387
  Prompt 3: avg reward = 0.630
  Prompt 4: avg reward = 0.559
üìä POST-UPDATE (step 46) Performance:
  Average reward: 0.4302
  Positive ratio: 100.0%

==================== PHASE 10: ANALYSIS (step 46) ====================
üîç GRPO Step Impact (step 46):
  Post-update performance: 0.4302
  Step performance change: +0.0000

======================================================================
üß≠ SEQUENTIAL STEP 47/50
======================================================================
üßæ Overfit mode: reusing initial batch (no dataset slice advance)

ü§ñ Generating completions for GRPO training (step 47)...

üéØ Prompt 1: P: 2k4r/ppp1q1p1/nbb1N3/4PP2/3p2P1/PB5P/1PPK3R/3RQ3 b - - 3 ...
    Gen 1: Train_LP= -648.0, Ref_LP=  -34.0
           Text:                                M: a6c5 c8b8 e7d7 d4d3 c6d7  ...
           Normalized: M: a6c5 c8b8 e7d7 d4d3 c6d7 E: -3.5 -4.07 -3.87 -3.57 -3.84 ...
    Gen 2: Train_LP= -608.0, Ref_LP=  -44.0
           Text:                                M: d4d3 b6a5 a6c5 c6d7 e7d7  ...
           Normalized: M: d4d3 b6a5 a6c5 c6d7 e7d7 E: -4.02 -4.03 -4.03 -4.08 -4.11...
    Gen 3: Train_LP= -532.0, Ref_LP=  -45.0
           Text:                                M: a6c5 h8e8 c6f3 d4d3 c6d7  ...
           Normalized: M: a6c5 h8e8 c6f3 d4d3 c6d7 E: -3.27 -3.59 -3.49 -3.62 -3.61...
    Gen 4: Train_LP= -484.0, Ref_LP=  -43.8
           Text:                                M: a6c5 d4d3 c8b8 e7d7 e7e8  ...
           Normalized: M: a6c5 d4d3 c8b8 e7d7 e7e8 E: -3.31 -3.52 -3.71 -3.57 -3.57...

üéØ Prompt 2: P: 2N5/2k5/3p1p2/3P1BbP/4Pp2/1n6/4K3/8 w - - 9 49...
    Gen 1: Train_LP= -564.0, Ref_LP=  -68.5
           Text:                                             M: e2d3 e2f2 c8a...
           Normalized: M: e2d3 e2f2 c8a7 e2f3 e2f1 E: 0.1 0.07 0.07 0.11 0.08 B: e2...
    Gen 2: Train_LP= -330.0, Ref_LP=  -13.8
           Text:                                             M: e2f1 c8e7 e2f...
           Normalized: M: e2f1 c8e7 e2f2 e2f3 e2e1 E: 0.0 0.0 0.0 0.0 0.0 B: e2f3...
    Gen 3: Train_LP= -544.0, Ref_LP=  -76.0
           Text:                                             M: e2f2 e2f3 c8a...
           Normalized: M: e2f2 e2f3 c8a7 c8e7 e2f1 E: 0.15 0.14 0.15 0.15 0.15 B: c...
    Gen 4: Train_LP= -568.0, Ref_LP=  -79.5
           Text:                                             M: e2f2 e2f1 e2e...
           Normalized: M: e2f2 e2f1 e2e1 c8e7 e2f3 E: 0.16 0.19 0.14 0.15 0.19 B: e...

üéØ Prompt 3: A: 8/2k2b2/R1p4r/3p1p2/P7/2P2B1P/5PPK/4R3 w - - 1 31+a6a5+f7...
    Gen 1: Train_LP= -286.0, Ref_LP=   -0.0
           Text: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
           Normalized: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
    Gen 2: Train_LP= -278.0, Ref_LP=   -0.0
           Text: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
           Normalized: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
    Gen 3: Train_LP= -264.0, Ref_LP=   -0.0
           Text: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
           Normalized: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
    Gen 4: Train_LP= -278.0, Ref_LP=   -0.0
           Text: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
           Normalized: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...

üéØ Prompt 4: P: rnbqkbnr/pp1ppppp/2p5/8/4P3/2N5/PPPP1PPP/R1BQKBNR b KQkq ...
    Gen 1: Train_LP= -692.0, Ref_LP=  -23.8
           Text:                             M: d7d5 d7d6 e7e6 g7g6 e7e5     ...
           Normalized: M: d7d5 d7d6 e7e6 g7g6 e7e5 E: -0.32 -0.67 -0.73 -0.71 -0.6 ...
    Gen 2: Train_LP= -628.0, Ref_LP=  -35.0
           Text:                             M: d7d6 d7d5 g7g6 e7e5 e7e6     ...
           Normalized: M: d7d6 d7d5 g7g6 e7e5 e7e6 E: -0.58 -0.42 -0.75 -0.67 -0.77...
    Gen 3: Train_LP= -604.0, Ref_LP=  -24.4
           Text:                             M: d7d6 e7e5 g7g6 d7d5 d8c7     ...
           Normalized: M: d7d6 e7e5 g7g6 d7d5 d8c7 E: -0.69 -0.65 -0.74 -0.4 -0.81 ...
    Gen 4: Train_LP= -382.0, Ref_LP=  -34.8
           Text:                             M: d7d6 e7e5 d7d5 g7g6 c6c5     ...
           Normalized: M: d7d6 e7e5 d7d5 g7g6 c6c5 E: -0.57 -0.46 -0.46 -0.67 -0.73...

==================== PHASE 4: REWARD CALCULATION (step 47) ====================

üéØ Scoring Prompt 1 completions:
  Gen 1: Reward= 0.399 | M: a6c5 c8b8 e7d7 d4d3 c6d7 E: -3.5 -4.0...
  Gen 2: Reward= 0.208 | M: d4d3 b6a5 a6c5 c6d7 e7d7 E: -4.02 -4....
  Gen 3: Reward= 0.295 | M: a6c5 h8e8 c6f3 d4d3 c6d7 E: -3.27 -3....
  Gen 4: Reward= 0.365 | M: a6c5 d4d3 c8b8 e7d7 e7e8 E: -3.31 -3....
  üìä Average reward: 0.317

üéØ Scoring Prompt 2 completions:
  Gen 1: Reward=-0.045 | M: e2d3 e2f2 c8a7 e2f3 e2f1 E: 0.1 0.07 ...
  Gen 2: Reward=-0.075 | M: e2f1 c8e7 e2f2 e2f3 e2e1 E: 0.0 0.0 0...
  Gen 3: Reward= 0.279 | M: e2f2 e2f3 c8a7 c8e7 e2f1 E: 0.15 0.14...
  Gen 4: Reward=-0.035 | M: e2f2 e2f1 e2e1 c8e7 e2f3 E: 0.16 0.19...
  üìä Average reward: 0.031

üéØ Scoring Prompt 3 completions:
  Gen 1: Reward= 0.630 | 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b...
  Gen 2: Reward= 0.630 | 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b...
  Gen 3: Reward= 0.630 | 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b...
  Gen 4: Reward= 0.630 | 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b...
  üìä Average reward: 0.630

üéØ Scoring Prompt 4 completions:
  Gen 1: Reward= 0.392 | M: d7d5 d7d6 e7e6 g7g6 e7e5 E: -0.32 -0....
  Gen 2: Reward= 0.370 | M: d7d6 d7d5 g7g6 e7e5 e7e6 E: -0.58 -0....
  Gen 3: Reward= 0.363 | M: d7d6 e7e5 g7g6 d7d5 d8c7 E: -0.69 -0....
  Gen 4: Reward= 0.688 | M: d7d6 e7e5 d7d5 g7g6 c6c5 E: -0.57 -0....
  üìä Average reward: 0.453

==================== PHASE 5: TRL EXACT ADVANTAGE CALCULATION (step 47) ====================
üî¢ TRL Advantage Calculation (exact formula):
  Total rewards shape: torch.Size([16])
  Rewards: [ 0.39937143  0.20777143  0.29457143  0.36457143 -0.045      -0.075
  0.27857143 -0.035       0.63        0.63        0.63        0.63
  0.39177143  0.36977143  0.36337143  0.6876    ]

==================== PHASE 6-8: CORRECTED GRPO LOSS CALCULATION (step 47) ====================
üîÑ Computing GRPO loss with proper token-level KL...

üéØ Prompt 1 - Corrected GRPO Loss:
  Gen 1: A=+0.979, logp/len=-6.875, kl=-5.094
         PG=6.719, KL_penalty=-0.508, total=6.219
  Gen 2: A=-1.286, logp/len=-6.719, kl=-4.844
         PG=-8.625, KL_penalty=-0.484, total=-9.125
  Gen 3: A=-0.260, logp/len=-6.969, kl=-5.094
         PG=-1.812, KL_penalty=-0.508, total=-2.312
  Gen 4: A=+0.567, logp/len=-7.250, kl=-5.375
         PG=4.125, KL_penalty=-0.539, total=3.594
  üìä Prompt averages: PG=0.102, KL=-0.508

üéØ Prompt 2 - Corrected GRPO Loss:
  Gen 1: A=-0.457, logp/len=-7.781, kl=-5.875
         PG=-3.562, KL_penalty=-0.586, total=-4.156
  Gen 2: A=-0.638, logp/len=-4.969, kl=-3.188
         PG=-3.172, KL_penalty=-0.318, total=-3.484
  Gen 3: A=+1.492, logp/len=-6.219, kl=-4.344
         PG=9.250, KL_penalty=-0.434, total=8.812
  Gen 4: A=-0.397, logp/len=-7.875, kl=-5.969
         PG=-3.125, KL_penalty=-0.598, total=-3.719
  üìä Prompt averages: PG=-0.156, KL=-0.484

üéØ Prompt 3 - Corrected GRPO Loss:
  Gen 1: A=+0.000, logp/len=-5.594, kl=-5.219
         PG=0.000, KL_penalty=-0.523, total=-0.523
  Gen 2: A=+0.000, logp/len=-5.969, kl=-5.594
         PG=0.000, KL_penalty=-0.559, total=-0.559
  Gen 3: A=+0.000, logp/len=-4.781, kl=-4.406
         PG=0.000, KL_penalty=-0.441, total=-0.441
  Gen 4: A=+0.000, logp/len=-6.375, kl=-6.000
         PG=0.000, KL_penalty=-0.602, total=-0.602
  üìä Prompt averages: PG=0.000, KL=-0.531

üéØ Prompt 4 - Corrected GRPO Loss:
  Gen 1: A=-0.391, logp/len=-7.312, kl=-5.844
         PG=-2.859, KL_penalty=-0.586, total=-3.438
  Gen 2: A=-0.532, logp/len=-7.969, kl=-6.250
         PG=-4.250, KL_penalty=-0.625, total=-4.875
  Gen 3: A=-0.572, logp/len=-7.750, kl=-6.156
         PG=-4.438, KL_penalty=-0.617, total=-5.062
  Gen 4: A=+1.495, logp/len=-7.875, kl=-6.281
         PG=11.750, KL_penalty=-0.629, total=11.125
  üìä Prompt averages: PG=0.047, KL=-0.613

üî¢ CORRECTED Loss Breakdown:
  Policy Gradient Loss:   -0.002
  KL Loss (with gradients):   -0.535
  Total Loss:          =  -0.539
  KL penalty ratio: 99.0%

üîÑ Performing corrected gradient update (step 47)...
  Total gradient norm: 17.05
  ‚úÖ Gradient update applied with proper KL regularization

==================== PHASE 9: POST-UPDATE PERFORMANCE (step 47) ====================
  Prompt 1: avg reward = 0.143
  Prompt 2: avg reward = 0.387
  Prompt 3: avg reward = 0.630
  Prompt 4: avg reward = 0.559
üìä POST-UPDATE (step 47) Performance:
  Average reward: 0.4299
  Positive ratio: 100.0%

==================== PHASE 10: ANALYSIS (step 47) ====================
üîç GRPO Step Impact (step 47):
  Post-update performance: 0.4299
  Step performance change: -0.0003

======================================================================
üß≠ SEQUENTIAL STEP 48/50
======================================================================
üßæ Overfit mode: reusing initial batch (no dataset slice advance)

ü§ñ Generating completions for GRPO training (step 48)...

üéØ Prompt 1: P: 2k4r/ppp1q1p1/nbb1N3/4PP2/3p2P1/PB5P/1PPK3R/3RQ3 b - - 3 ...
    Gen 1: Train_LP= -400.0, Ref_LP=  -35.0
           Text:                                M: d4d3 h8h4 a6c5 e7d7 c6d7  ...
           Normalized: M: d4d3 h8h4 a6c5 e7d7 c6d7 E: -4.0 -3.96 -3.91 -4.0 -3.93 B...
    Gen 2: Train_LP= -580.0, Ref_LP=  -57.0
           Text:                                M: e7d7 h8h4 d4d3 c8b8 a6c5  ...
           Normalized: M: e7d7 h8h4 d4d3 c8b8 a6c5 E: -4.11 -3.78 -3.67 -3.74 -3.72...
    Gen 3: Train_LP= -438.0, Ref_LP=  -55.5
           Text:                                M: a6c5 c8b8 e7e8 c6d7 e7d7  ...
           Normalized: M: a6c5 c8b8 e7e8 c6d7 e7d7 E: -3.54 -3.84 -3.98 -3.96 -3.89...
    Gen 4: Train_LP= -680.0, Ref_LP=  -56.0
           Text:                                M: a6c5 b6a5 e7d7 c6d7 c8b8  ...
           Normalized: M: a6c5 b6a5 e7d7 c6d7 c8b8 E: -3.39 -3.65 -3.93 -3.56 -3.66...

üéØ Prompt 2: P: 2N5/2k5/3p1p2/3P1BbP/4Pp2/1n6/4K3/8 w - - 9 49...
    Gen 1: Train_LP= -556.0, Ref_LP=  -22.4
           Text:                                             M: e2f2 e2f1 e2f...
           Normalized: M: e2f2 e2f1 e2f3 c8e7 c8a7 E: 0.11 0.11 0.11 0.08 0.12 B: c...
    Gen 2: Train_LP= -322.0, Ref_LP=  -22.5
           Text:                                             M: e2f2 e2f3 c8e...
           Normalized: M: e2f2 e2f3 c8e7 e2f1 c8a7 E: 0.17 0.17 0.17 0.15 0.17 B: e...
    Gen 3: Train_LP= -368.0, Ref_LP=  -28.5
           Text:                                             M: e2f3 e2f2 c8e...
           Normalized: M: e2f3 e2f2 c8e7 e2d3 e2e1 E: 0.41 0.36 0.38 0.43 0.36 B: e...
    Gen 4: Train_LP= -536.0, Ref_LP=  -24.8
           Text:                                             M: e2f2 e2d3 c8a...
           Normalized: M: e2f2 e2d3 c8a7 e2f3 c8e7 E: 0.14 0.16 0.18 0.17 0.18 B: c...

üéØ Prompt 3: A: 8/2k2b2/R1p4r/3p1p2/P7/2P2B1P/5PPK/4R3 w - - 1 31+a6a5+f7...
    Gen 1: Train_LP= -258.0, Ref_LP=   -0.0
           Text: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
           Normalized: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
    Gen 2: Train_LP= -302.0, Ref_LP=   -0.0
           Text: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
           Normalized: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
    Gen 3: Train_LP= -268.0, Ref_LP=   -0.0
           Text: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
           Normalized: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
    Gen 4: Train_LP= -249.0, Ref_LP=   -0.0
           Text: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
           Normalized: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...

üéØ Prompt 4: P: rnbqkbnr/pp1ppppp/2p5/8/4P3/2N5/PPPP1PPP/R1BQKBNR b KQkq ...
    Gen 1: Train_LP= -452.0, Ref_LP=  -24.8
           Text:                             M: d7d5 e7e5 e7e6 d7d6 g7g6     ...
           Normalized: M: d7d5 e7e5 e7e6 d7d6 g7g6 E: -0.35 -0.4 -0.8 -0.7 -0.78 B:...
    Gen 2: Train_LP= -532.0, Ref_LP=  -34.0
           Text:                             M: d7d6 g7g6 e7e5 d7d5 e7e6     ...
           Normalized: M: d7d6 g7g6 e7e5 d7d5 e7e6 E: -0.7 -0.73 -0.7 -0.48 -0.76 B...
    Gen 3: Train_LP= -436.0, Ref_LP=  -57.5
           Text:                             M: d7d5 e7e5 d7d6 g7g6 g8f6     ...
           Normalized: M: d7d5 e7e5 d7d6 g7g6 g8f6 E: -0.52 -0.48 -0.68 -0.72 -0.68...
    Gen 4: Train_LP= -466.0, Ref_LP=  -45.5
           Text:                             M: e7e6 e7e5 d7d5 d7d6 g7g6     ...
           Normalized: M: e7e6 e7e5 d7d5 d7d6 g7g6 E: -0.8 -0.61 -0.37 -0.62 -0.81 ...

==================== PHASE 4: REWARD CALCULATION (step 48) ====================

üéØ Scoring Prompt 1 completions:
  Gen 1: Reward= 0.175 | M: d4d3 h8h4 a6c5 e7d7 c6d7 E: -4.0 -3.9...
  Gen 2: Reward= 0.186 | M: e7d7 h8h4 d4d3 c8b8 a6c5 E: -4.11 -3....
  Gen 3: Reward= 0.168 | M: a6c5 c8b8 e7e8 c6d7 e7d7 E: -3.54 -3....
  Gen 4: Reward= 0.197 | M: a6c5 b6a5 e7d7 c6d7 c8b8 E: -3.39 -3....
  üìä Average reward: 0.181

üéØ Scoring Prompt 2 completions:
  Gen 1: Reward= 0.269 | M: e2f2 e2f1 e2f3 c8e7 c8a7 E: 0.11 0.11...
  Gen 2: Reward= 0.019 | M: e2f2 e2f3 c8e7 e2f1 c8a7 E: 0.17 0.17...
  Gen 3: Reward=-0.015 | M: e2f3 e2f2 c8e7 e2d3 e2e1 E: 0.41 0.36...
  Gen 4: Reward= 0.479 | M: e2f2 e2d3 c8a7 e2f3 c8e7 E: 0.14 0.16...
  üìä Average reward: 0.188

üéØ Scoring Prompt 3 completions:
  Gen 1: Reward= 0.630 | 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b...
  Gen 2: Reward= 0.630 | 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b...
  Gen 3: Reward= 0.630 | 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b...
  Gen 4: Reward= 0.630 | 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b...
  üìä Average reward: 0.630

üéØ Scoring Prompt 4 completions:
  Gen 1: Reward= 0.393 | M: d7d5 e7e5 e7e6 d7d6 g7g6 E: -0.35 -0....
  Gen 2: Reward= 0.382 | M: d7d6 g7g6 e7e5 d7d5 e7e6 E: -0.7 -0.7...
  Gen 3: Reward= 0.807 | M: d7d5 e7e5 d7d6 g7g6 g8f6 E: -0.52 -0....
  Gen 4: Reward= 0.719 | M: e7e6 e7e5 d7d5 d7d6 g7g6 E: -0.8 -0.6...
  üìä Average reward: 0.575

==================== PHASE 5: TRL EXACT ADVANTAGE CALCULATION (step 48) ====================
üî¢ TRL Advantage Calculation (exact formula):
  Total rewards shape: torch.Size([16])
  Rewards: [ 0.175       0.18617143  0.1678      0.19697143  0.26857143  0.01857143
 -0.015       0.47857143  0.63        0.63        0.63        0.63
  0.39257143  0.38177143  0.8072      0.71897143]

==================== PHASE 6-8: CORRECTED GRPO LOSS CALCULATION (step 48) ====================
üîÑ Computing GRPO loss with proper token-level KL...

üéØ Prompt 1 - Corrected GRPO Loss:
  Gen 1: A=-0.507, logp/len=-7.438, kl=-5.688
         PG=-3.766, KL_penalty=-0.570, total=-4.344
  Gen 2: A=+0.366, logp/len=-6.625, kl=-4.938
         PG=2.422, KL_penalty=-0.494, total=1.930
  Gen 3: A=-1.070, logp/len=-7.000, kl=-4.938
         PG=-7.500, KL_penalty=-0.494, total=-8.000
  Gen 4: A=+1.210, logp/len=-7.031, kl=-5.312
         PG=8.500, KL_penalty=-0.531, total=7.969
  üìä Prompt averages: PG=-0.094, KL=-0.523

üéØ Prompt 2 - Corrected GRPO Loss:
  Gen 1: A=+0.349, logp/len=-7.969, kl=-6.219
         PG=2.781, KL_penalty=-0.621, total=2.156
  Gen 2: A=-0.730, logp/len=-6.281, kl=-4.531
         PG=-4.594, KL_penalty=-0.453, total=-5.062
  Gen 3: A=-0.875, logp/len=-7.438, kl=-5.250
         PG=-6.500, KL_penalty=-0.523, total=-7.031
  Gen 4: A=+1.256, logp/len=-7.125, kl=-5.219
         PG=8.938, KL_penalty=-0.523, total=8.438
  üìä Prompt averages: PG=0.156, KL=-0.531

üéØ Prompt 3 - Corrected GRPO Loss:
  Gen 1: A=+0.000, logp/len=-5.406, kl=-5.031
         PG=0.000, KL_penalty=-0.504, total=-0.504
  Gen 2: A=+0.000, logp/len=-5.312, kl=-4.969
         PG=0.000, KL_penalty=-0.496, total=-0.496
  Gen 3: A=+0.000, logp/len=-4.469, kl=-4.125
         PG=0.000, KL_penalty=-0.412, total=-0.412
  Gen 4: A=+0.000, logp/len=-4.812, kl=-4.469
         PG=0.000, KL_penalty=-0.447, total=-0.447
  üìä Prompt averages: PG=0.000, KL=-0.465

üéØ Prompt 4 - Corrected GRPO Loss:
  Gen 1: A=-0.830, logp/len=-7.281, kl=-5.594
         PG=-6.031, KL_penalty=-0.559, total=-6.594
  Gen 2: A=-0.879, logp/len=-7.188, kl=-5.531
         PG=-6.312, KL_penalty=-0.555, total=-6.875
  Gen 3: A=+1.055, logp/len=-6.844, kl=-5.406
         PG=7.219, KL_penalty=-0.539, total=6.688
  Gen 4: A=+0.654, logp/len=-7.125, kl=-5.469
         PG=4.656, KL_penalty=-0.547, total=4.125
  üìä Prompt averages: PG=-0.125, KL=-0.547

üî¢ CORRECTED Loss Breakdown:
  Policy Gradient Loss:   -0.016
  KL Loss (with gradients):   -0.516
  Total Loss:          =  -0.531
  KL penalty ratio: 97.0%

üîÑ Performing corrected gradient update (step 48)...
  Total gradient norm: 20.07
  ‚úÖ Gradient update applied with proper KL regularization

==================== PHASE 9: POST-UPDATE PERFORMANCE (step 48) ====================
  Prompt 1: avg reward = 0.143
  Prompt 2: avg reward = 0.387
  Prompt 3: avg reward = 0.630
  Prompt 4: avg reward = 0.505
üìä POST-UPDATE (step 48) Performance:
  Average reward: 0.4163
  Positive ratio: 100.0%

==================== PHASE 10: ANALYSIS (step 48) ====================
üîç GRPO Step Impact (step 48):
  Post-update performance: 0.4163
  Step performance change: -0.0136

======================================================================
üß≠ SEQUENTIAL STEP 49/50
======================================================================
üßæ Overfit mode: reusing initial batch (no dataset slice advance)

ü§ñ Generating completions for GRPO training (step 49)...

üéØ Prompt 1: P: 2k4r/ppp1q1p1/nbb1N3/4PP2/3p2P1/PB5P/1PPK3R/3RQ3 b - - 3 ...
    Gen 1: Train_LP= -524.0, Ref_LP=  -33.2
           Text:                                M: c8b8 a6c5 d4d3 c6d7 h8e8  ...
           Normalized: M: c8b8 a6c5 d4d3 c6d7 h8e8 E: -3.86 -3.56 -3.69 -3.88 -3.9 ...
    Gen 2: Train_LP= -588.0, Ref_LP=  -44.5
           Text:                                M: c8b8 a6c5 d4d3 e7d7 e7h4  ...
           Normalized: M: c8b8 a6c5 d4d3 e7d7 e7h4 E: -4.09 -3.93 -4.16 -4.07 -4.23...
    Gen 3: Train_LP= -390.0, Ref_LP=  -34.5
           Text:                                M: h8h4 h8e8 a6c5 d4d3 c8b8  ...
           Normalized: M: h8h4 h8e8 a6c5 d4d3 c8b8 E: -3.62 -3.56 -3.53 -3.36 -3.7 ...
    Gen 4: Train_LP= -448.0, Ref_LP=  -46.0
           Text:                                M: a6c5 e7d7 h8h4 c6d7 h8e8  ...
           Normalized: M: a6c5 e7d7 h8h4 c6d7 h8e8 E: -3.76 -3.85 -3.86 -3.85 -3.86...

üéØ Prompt 2: P: 2N5/2k5/3p1p2/3P1BbP/4Pp2/1n6/4K3/8 w - - 9 49...
    Gen 1: Train_LP= -249.0, Ref_LP=  -15.4
           Text:                                             M: e2d3 e2f3 e2f...
           Normalized: M: e2d3 e2f3 e2f1 e2f2 c8e7 E: 0.0 0.0 0.0 0.0 0.0 B: e2f3...
    Gen 2: Train_LP= -450.0, Ref_LP=  -75.5
           Text:                                             M: e2f2 e2f1 e2f...
           Normalized: M: e2f2 e2f1 e2f3 c8e7 c8a7 E: 0.35 0.35 0.35 0.35 0.35 B: c...
    Gen 3: Train_LP= -732.0, Ref_LP=  -13.6
           Text:                                             M: e2f2 c8a7 e2f...
           Normalized: M: e2f2 c8a7 e2f3 c8e7 e2f1 E: 0.0 0.0 0.0 0.0 0.0 B: e2f2...
    Gen 4: Train_LP= -780.0, Ref_LP=  -76.5
           Text:                                             M: e2f2 e2d3 e2f...
           Normalized: M: e2f2 e2d3 e2f1 e2f3 c8e7 E: 0.35 0.35 0.35 0.35 0.36 B: c...

üéØ Prompt 3: A: 8/2k2b2/R1p4r/3p1p2/P7/2P2B1P/5PPK/4R3 w - - 1 31+a6a5+f7...
    Gen 1: Train_LP= -286.0, Ref_LP=   -0.0
           Text: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
           Normalized: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
    Gen 2: Train_LP= -274.0, Ref_LP=   -0.0
           Text: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
           Normalized: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
    Gen 3: Train_LP= -322.0, Ref_LP=   -0.0
           Text: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
           Normalized: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
    Gen 4: Train_LP= -304.0, Ref_LP=   -0.0
           Text: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
           Normalized: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...

üéØ Prompt 4: P: rnbqkbnr/pp1ppppp/2p5/8/4P3/2N5/PPPP1PPP/R1BQKBNR b KQkq ...
    Gen 1: Train_LP= -616.0, Ref_LP=  -24.9
           Text:                             M: e7e5 d7d5 d7d6 e7e6 c6c5     ...
           Normalized: M: e7e5 d7d5 d7d6 e7e6 c6c5 E: -0.51 -0.4 -0.72 -0.81 -0.77 ...
    Gen 2: Train_LP= -572.0, Ref_LP=  -23.8
           Text:                             M: d7d5 d7d6 e7e5 g7g6 c6c5     ...
           Normalized: M: d7d5 d7d6 e7e5 g7g6 c6c5 E: -0.46 -0.73 -0.53 -0.7 -0.67 ...
    Gen 3: Train_LP= -404.0, Ref_LP=  -35.0
           Text:                             M: d7d6 d7d5 e7e5 g7g6 e7e6     ...
           Normalized: M: d7d6 d7d5 e7e5 g7g6 e7e6 E: -0.57 -0.42 -0.49 -0.56 -0.84...
    Gen 4: Train_LP= -414.0, Ref_LP=  -35.0
           Text:                             M: d7d6 g7g6 e7e5 d7d5 e7e6     ...
           Normalized: M: d7d6 g7g6 e7e5 d7d5 e7e6 E: -0.61 -0.72 -0.61 -0.51 -0.77...

==================== PHASE 4: REWARD CALCULATION (step 49) ====================

üéØ Scoring Prompt 1 completions:
  Gen 1: Reward= 0.256 | M: c8b8 a6c5 d4d3 c6d7 h8e8 E: -3.86 -3....
  Gen 2: Reward= 0.289 | M: c8b8 a6c5 d4d3 e7d7 e7h4 E: -4.09 -3....
  Gen 3: Reward= 0.329 | M: h8h4 h8e8 a6c5 d4d3 c8b8 E: -3.62 -3....
  Gen 4: Reward= 0.083 | M: a6c5 e7d7 h8h4 c6d7 h8e8 E: -3.76 -3....
  üìä Average reward: 0.239

üéØ Scoring Prompt 2 completions:
  Gen 1: Reward=-0.075 | M: e2d3 e2f3 e2f1 e2f2 c8e7 E: 0.0 0.0 0...
  Gen 2: Reward= 0.269 | M: e2f2 e2f1 e2f3 c8e7 c8a7 E: 0.35 0.35...
  Gen 3: Reward= 0.329 | M: e2f2 c8a7 e2f3 c8e7 e2f1 E: 0.0 0.0 0...
  Gen 4: Reward= 0.375 | M: e2f2 e2d3 e2f1 e2f3 c8e7 E: 0.35 0.35...
  üìä Average reward: 0.224

üéØ Scoring Prompt 3 completions:
  Gen 1: Reward= 0.630 | 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b...
  Gen 2: Reward= 0.630 | 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b...
  Gen 3: Reward= 0.630 | 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b...
  Gen 4: Reward= 0.630 | 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b...
  üìä Average reward: 0.630

üéØ Scoring Prompt 4 completions:
  Gen 1: Reward= 0.363 | M: e7e5 d7d5 d7d6 e7e6 c6c5 E: -0.51 -0....
  Gen 2: Reward= 0.618 | M: d7d5 d7d6 e7e5 g7g6 c6c5 E: -0.46 -0....
  Gen 3: Reward= 0.364 | M: d7d6 d7d5 e7e5 g7g6 e7e6 E: -0.57 -0....
  Gen 4: Reward= 0.388 | M: d7d6 g7g6 e7e5 d7d5 e7e6 E: -0.61 -0....
  üìä Average reward: 0.433

==================== PHASE 5: TRL EXACT ADVANTAGE CALCULATION (step 49) ====================
üî¢ TRL Advantage Calculation (exact formula):
  Total rewards shape: torch.Size([16])
  Rewards: [ 0.25577143  0.28897143  0.32857143  0.08333333 -0.075       0.26857143
  0.32857143  0.375       0.63        0.63        0.63        0.63
  0.36337143  0.6176      0.36377143  0.38777143]

==================== PHASE 6-8: CORRECTED GRPO LOSS CALCULATION (step 49) ====================
üîÑ Computing GRPO loss with proper token-level KL...

üéØ Prompt 1 - Corrected GRPO Loss:
  Gen 1: A=+0.154, logp/len=-6.312, kl=-4.562
         PG=0.969, KL_penalty=-0.457, total=0.512
  Gen 2: A=+0.461, logp/len=-7.000, kl=-5.094
         PG=3.219, KL_penalty=-0.508, total=2.719
  Gen 3: A=+0.827, logp/len=-5.906, kl=-4.156
         PG=4.875, KL_penalty=-0.416, total=4.469
  Gen 4: A=-1.442, logp/len=-7.125, kl=-5.406
         PG=-10.250, KL_penalty=-0.539, total=-10.812
  üìä Prompt averages: PG=-0.297, KL=-0.480

üéØ Prompt 2 - Corrected GRPO Loss:
  Gen 1: A=-1.465, logp/len=-6.094, kl=-4.562
         PG=-8.938, KL_penalty=-0.457, total=-9.375
  Gen 2: A=+0.217, logp/len=-7.781, kl=-6.094
         PG=1.688, KL_penalty=-0.609, total=1.078
  Gen 3: A=+0.511, logp/len=-7.375, kl=-5.656
         PG=3.766, KL_penalty=-0.566, total=3.203
  Gen 4: A=+0.738, logp/len=-5.875, kl=-4.156
         PG=4.344, KL_penalty=-0.416, total=3.922
  üìä Prompt averages: PG=0.215, KL=-0.512

üéØ Prompt 3 - Corrected GRPO Loss:
  Gen 1: A=+0.000, logp/len=-7.812, kl=-7.438
         PG=0.000, KL_penalty=-0.742, total=-0.742
  Gen 2: A=+0.000, logp/len=-4.750, kl=-4.375
         PG=0.000, KL_penalty=-0.438, total=-0.438
  Gen 3: A=+0.000, logp/len=-6.875, kl=-6.500
         PG=0.000, KL_penalty=-0.648, total=-0.648
  Gen 4: A=+0.000, logp/len=-4.938, kl=-4.562
         PG=0.000, KL_penalty=-0.457, total=-0.457
  üìä Prompt averages: PG=0.000, KL=-0.570

üéØ Prompt 4 - Corrected GRPO Loss:
  Gen 1: A=-0.565, logp/len=-8.250, kl=-6.500
         PG=-4.656, KL_penalty=-0.648, total=-5.312
  Gen 2: A=+1.494, logp/len=-8.750, kl=-7.156
         PG=13.062, KL_penalty=-0.715, total=12.375
  Gen 3: A=-0.562, logp/len=-8.562, kl=-6.938
         PG=-4.812, KL_penalty=-0.695, total=-5.500
  Gen 4: A=-0.367, logp/len=-6.719, kl=-5.031
         PG=-2.469, KL_penalty=-0.504, total=-2.969
  üìä Prompt averages: PG=0.273, KL=-0.641

üî¢ CORRECTED Loss Breakdown:
  Policy Gradient Loss:    0.048
  KL Loss (with gradients):   -0.551
  Total Loss:          =  -0.504
  KL penalty ratio: 92.0%

üîÑ Performing corrected gradient update (step 49)...
  Total gradient norm: 17.71
  ‚úÖ Gradient update applied with proper KL regularization

==================== PHASE 9: POST-UPDATE PERFORMANCE (step 49) ====================
  Prompt 1: avg reward = 0.143
  Prompt 2: avg reward = 0.387
  Prompt 3: avg reward = 0.630
  Prompt 4: avg reward = 0.505
üìä POST-UPDATE (step 49) Performance:
  Average reward: 0.4163
  Positive ratio: 100.0%

==================== PHASE 10: ANALYSIS (step 49) ====================
üîç GRPO Step Impact (step 49):
  Post-update performance: 0.4163
  Step performance change: +0.0000

======================================================================
üß≠ SEQUENTIAL STEP 50/50
======================================================================
üßæ Overfit mode: reusing initial batch (no dataset slice advance)

ü§ñ Generating completions for GRPO training (step 50)...

üéØ Prompt 1: P: 2k4r/ppp1q1p1/nbb1N3/4PP2/3p2P1/PB5P/1PPK3R/3RQ3 b - - 3 ...
    Gen 1: Train_LP= -528.0, Ref_LP=  -56.0
           Text:                                M: b6a5 a6c5 e7d7 h8h3 d4d3  ...
           Normalized: M: b6a5 a6c5 e7d7 h8h3 d4d3 E: -4.07 -3.54 -4.23 -4.21 -3.88...
    Gen 2: Train_LP= -444.0, Ref_LP=  -33.2
           Text:                                M: a6c5 e7d7 c8b8 h8e8 e7e8  ...
           Normalized: M: a6c5 e7d7 c8b8 h8e8 e7e8 E: -3.48 -3.78 -3.7 -3.9 -3.88 B...
    Gen 3: Train_LP= -448.0, Ref_LP=  -46.5
           Text:                                M: h8h4 a6c5 e7d7 h8e8 c6d7  ...
           Normalized: M: h8h4 a6c5 e7d7 h8e8 c6d7 E: -4.05 -3.59 -3.93 -4.05 -3.9 ...
    Gen 4: Train_LP= -636.0, Ref_LP=  -53.8
           Text:                                M: d4d3 e7d7 a6c5 h8e8 c8b8  ...
           Normalized: M: d4d3 e7d7 a6c5 h8e8 c8b8 E: -3.71 -3.82 -3.29 -3.77 -3.78...

üéØ Prompt 2: P: 2N5/2k5/3p1p2/3P1BbP/4Pp2/1n6/4K3/8 w - - 9 49...
    Gen 1: Train_LP= -308.0, Ref_LP=  -75.0
           Text:                                             M: e2f1 c8e7 e2f...
           Normalized: M: e2f1 c8e7 e2f2 e2f3 c8a7 E: 0.16 0.16 0.16 0.16 0.16 B: c...
    Gen 2: Train_LP= -512.0, Ref_LP=  -13.9
           Text:                                             M: e2f1 c8e7 e2f...
           Normalized: M: e2f1 c8e7 e2f3 e2f2 c8a7 E: 0.0 0.0 0.0 0.0 0.0 B: e2f2...
    Gen 3: Train_LP= -616.0, Ref_LP=  -77.0
           Text:                                             M: e2f2 c8a7 e2f...
           Normalized: M: e2f2 c8a7 e2f3 c8e7 e2f1 E: 0.24 0.27 0.25 0.25 0.25 B: c...
    Gen 4: Train_LP= -356.0, Ref_LP=  -79.0
           Text:                                             M: c8a7 c8e7 e2f...
           Normalized: M: c8a7 c8e7 e2f3 e2f2 e2f1 E: 0.41 0.42 0.45 0.42 0.41 B: e...

üéØ Prompt 3: A: 8/2k2b2/R1p4r/3p1p2/P7/2P2B1P/5PPK/4R3 w - - 1 31+a6a5+f7...
    Gen 1: Train_LP= -292.0, Ref_LP=   -0.0
           Text: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
           Normalized: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
    Gen 2: Train_LP= -350.0, Ref_LP=   -0.0
           Text: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
           Normalized: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
    Gen 3: Train_LP= -282.0, Ref_LP=   -0.0
           Text: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
           Normalized: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
    Gen 4: Train_LP= -356.0, Ref_LP=   -0.0
           Text: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...
           Normalized: 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b - - 2 31+0.001+0+0...

üéØ Prompt 4: P: rnbqkbnr/pp1ppppp/2p5/8/4P3/2N5/PPPP1PPP/R1BQKBNR b KQkq ...
    Gen 1: Train_LP= -648.0, Ref_LP=  -24.5
           Text:                             M: d7d6 d7d5 e7e5 e7e6 g7g6     ...
           Normalized: M: d7d6 d7d5 e7e5 e7e6 g7g6 E: -0.69 -0.52 -0.56 -0.82 -0.62...
    Gen 2: Train_LP= -520.0, Ref_LP=  -25.2
           Text:                             M: e7e6 d7d6 e7e5 d7d5 c6c5     ...
           Normalized: M: e7e6 d7d6 e7e5 d7d5 c6c5 E: -0.82 -0.54 -0.54 -0.56 -0.79...
    Gen 3: Train_LP= -332.0, Ref_LP=  -23.0
           Text:                             M: d7d6 d7d5 g7g6 e7e5 e7e6     ...
           Normalized: M: d7d6 d7d5 g7g6 e7e5 e7e6 E: -0.68 -0.45 -0.67 -0.63 -0.75...
    Gen 4: Train_LP= -524.0, Ref_LP=  -23.5
           Text:                             M: d7d5 e7e5 d7d6 g7g6 e7e6     ...
           Normalized: M: d7d5 e7e5 d7d6 g7g6 e7e6 E: -0.38 -0.64 -0.62 -0.67 -0.76...

==================== PHASE 4: REWARD CALCULATION (step 50) ====================

üéØ Scoring Prompt 1 completions:
  Gen 1: Reward= 0.229 | M: b6a5 a6c5 e7d7 h8h3 d4d3 E: -4.07 -3....
  Gen 2: Reward= 0.163 | M: a6c5 e7d7 c8b8 h8e8 e7e8 E: -3.48 -3....
  Gen 3: Reward= 0.103 | M: h8h4 a6c5 e7d7 h8e8 c6d7 E: -4.05 -3....
  Gen 4: Reward= 0.253 | M: d4d3 e7d7 a6c5 h8e8 c8b8 E: -3.71 -3....
  üìä Average reward: 0.187

üéØ Scoring Prompt 2 completions:
  Gen 1: Reward= 0.469 | M: e2f1 c8e7 e2f2 e2f3 c8a7 E: 0.16 0.16...
  Gen 2: Reward= 0.329 | M: e2f1 c8e7 e2f3 e2f2 c8a7 E: 0.0 0.0 0...
  Gen 3: Reward= 0.269 | M: e2f2 c8a7 e2f3 c8e7 e2f1 E: 0.24 0.27...
  Gen 4: Reward= 0.019 | M: c8a7 c8e7 e2f3 e2f2 e2f1 E: 0.41 0.42...
  üìä Average reward: 0.271

üéØ Scoring Prompt 3 completions:
  Gen 1: Reward= 0.630 | 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b...
  Gen 2: Reward= 0.630 | 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b...
  Gen 3: Reward= 0.630 | 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b...
  Gen 4: Reward= 0.630 | 8/2k2b2/2p4r/R2p1p2/P7/2P2B1P/5PPK/4R3 b...
  üìä Average reward: 0.630

üéØ Scoring Prompt 4 completions:
  Gen 1: Reward= 0.362 | M: d7d6 d7d5 e7e5 e7e6 g7g6 E: -0.69 -0....
  Gen 2: Reward= 0.367 | M: e7e6 d7d6 e7e5 d7d5 c6c5 E: -0.82 -0....
  Gen 3: Reward= 0.364 | M: d7d6 d7d5 g7g6 e7e5 e7e6 E: -0.68 -0....
  Gen 4: Reward= 0.394 | M: d7d5 e7e5 d7d6 g7g6 e7e6 E: -0.38 -0....
  üìä Average reward: 0.372

==================== PHASE 5: TRL EXACT ADVANTAGE CALCULATION (step 50) ====================
üî¢ TRL Advantage Calculation (exact formula):
  Total rewards shape: torch.Size([16])
  Rewards: [0.22857143 0.1626     0.10333333 0.25297143 0.46857143 0.32857143
 0.26857143 0.01857143 0.63       0.63       0.63       0.63
 0.36177143 0.36657143 0.36377143 0.39417143]

==================== PHASE 6-8: CORRECTED GRPO LOSS CALCULATION (step 50) ====================
üîÑ Computing GRPO loss with proper token-level KL...

üéØ Prompt 1 - Corrected GRPO Loss:
  Gen 1: A=+0.618, logp/len=-5.594, kl=-3.875
         PG=3.453, KL_penalty=-0.387, total=3.062
  Gen 2: A=-0.359, logp/len=-6.375, kl=-4.438
         PG=-2.297, KL_penalty=-0.443, total=-2.734
  Gen 3: A=-1.237, logp/len=-6.156, kl=-4.406
         PG=-7.625, KL_penalty=-0.441, total=-8.062
  Gen 4: A=+0.979, logp/len=-8.062, kl=-6.406
         PG=7.906, KL_penalty=-0.641, total=7.250
  üìä Prompt averages: PG=0.359, KL=-0.477

üéØ Prompt 2 - Corrected GRPO Loss:
  Gen 1: A=+1.050, logp/len=-6.031, kl=-4.312
         PG=6.344, KL_penalty=-0.432, total=5.906
  Gen 2: A=+0.306, logp/len=-6.125, kl=-4.562
         PG=1.875, KL_penalty=-0.457, total=1.422
  Gen 3: A=-0.013, logp/len=-7.125, kl=-5.156
         PG=-0.095, KL_penalty=-0.516, total=-0.609
  Gen 4: A=-1.343, logp/len=-7.594, kl=-5.562
         PG=-10.188, KL_penalty=-0.555, total=-10.750
  üìä Prompt averages: PG=-0.516, KL=-0.490

üéØ Prompt 3 - Corrected GRPO Loss:
  Gen 1: A=+0.000, logp/len=-5.719, kl=-5.375
         PG=0.000, KL_penalty=-0.539, total=-0.539
  Gen 2: A=+0.000, logp/len=-5.688, kl=-5.312
         PG=0.000, KL_penalty=-0.531, total=-0.531
  Gen 3: A=+0.000, logp/len=-5.906, kl=-5.562
         PG=0.000, KL_penalty=-0.555, total=-0.555
  Gen 4: A=+0.000, logp/len=-6.312, kl=-5.938
         PG=0.000, KL_penalty=-0.594, total=-0.594
  üìä Prompt averages: PG=0.000, KL=-0.555

üéØ Prompt 4 - Corrected GRPO Loss:
  Gen 1: A=-0.645, logp/len=-8.625, kl=-7.062
         PG=-5.562, KL_penalty=-0.707, total=-6.281
  Gen 2: A=-0.329, logp/len=-7.688, kl=-5.969
         PG=-2.531, KL_penalty=-0.598, total=-3.125
  Gen 3: A=-0.513, logp/len=-6.156, kl=-4.438
         PG=-3.156, KL_penalty=-0.443, total=-3.594
  Gen 4: A=+1.487, logp/len=-8.000, kl=-6.406
         PG=11.875, KL_penalty=-0.641, total=11.250
  üìä Prompt averages: PG=0.156, KL=-0.598

üî¢ CORRECTED Loss Breakdown:
  Policy Gradient Loss:    0.000
  KL Loss (with gradients):   -0.531
  Total Loss:          =  -0.531
  KL penalty ratio: 100.0%

üîÑ Performing corrected gradient update (step 50)...
  Total gradient norm: 24.64
  ‚úÖ Gradient update applied with proper KL regularization

==================== PHASE 9: POST-UPDATE PERFORMANCE (step 50) ====================
  Prompt 1: avg reward = 0.143
  Prompt 2: avg reward = 0.387
  Prompt 3: avg reward = 0.630
  Prompt 4: avg reward = 0.505
üìä POST-UPDATE (step 50) Performance:
  Average reward: 0.4163
  Positive ratio: 100.0%

==================== PHASE 10: ANALYSIS (step 50) ====================
üîç GRPO Step Impact (step 50):
  Post-update performance: 0.4163
  Step performance change: +0.0000

üéØ FINAL SUMMARY:
  Initial model performance: 0.4266
  After 50 GRPO step(s): 0.4163
  Performance change: -0.0103
  Total loss: -0.53

üìã Per-step overview:
Step |    Pre |   Post |      Œî |      PG |      KL |   KL% |    Adv |  GradN |     Œ≤
-------------------------------------------------------------------------------------
   1 |  0.427 |  0.391 | -0.035 |   0.029 |  -0.484 |  94.5 |  1.711 |  20.18 | 0.100
   2 |  0.391 |  0.432 |  0.041 |  -0.062 |  -0.523 |  89.5 |  1.637 |  21.08 | 0.100
   3 |  0.432 |  0.433 |  0.001 |  -0.021 |  -0.492 |  96.0 |  1.596 |  19.25 | 0.100
   4 |  0.433 |  0.434 |  0.000 |  -0.223 |  -0.531 |  70.5 |  1.619 |  19.60 | 0.100
   5 |  0.434 |  0.418 | -0.016 |   0.061 |  -0.527 |  90.0 |  1.570 |  27.91 | 0.100
   6 |  0.418 |  0.434 |  0.017 |   0.033 |  -0.562 |  95.0 |  1.643 |  33.38 | 0.100
   7 |  0.434 |  0.375 | -0.059 |  -0.020 |  -0.531 |  96.5 |  1.640 |  25.21 | 0.100
   8 |  0.375 |  0.432 |  0.057 |  -0.359 |  -0.555 |  60.5 |  1.749 |  25.22 | 0.100
   9 |  0.432 |  0.417 | -0.015 |   0.226 |  -0.523 |  70.0 |  1.738 |  25.73 | 0.100
  10 |  0.417 |  0.418 |  0.000 |   0.023 |  -0.527 |  95.5 |  1.522 |  18.29 | 0.100
  11 |  0.418 |  0.417 | -0.000 |  -0.131 |  -0.523 |  79.5 |  1.447 |  18.97 | 0.100
  12 |  0.417 |  0.431 |  0.013 |  -0.066 |  -0.531 |  89.0 |  1.533 |  18.16 | 0.100
  13 |  0.431 |  0.433 |  0.002 |  -0.043 |  -0.547 |  92.5 |  1.659 |  21.58 | 0.100
  14 |  0.433 |  0.393 | -0.040 |  -0.165 |  -0.527 |  76.0 |  1.540 |  23.75 | 0.100
  15 |  0.393 |  0.433 |  0.040 |  -0.389 |  -0.486 |  55.5 |  1.642 |  36.48 | 0.100
  16 |  0.433 |  0.432 | -0.001 |   0.073 |  -0.570 |  88.5 |  1.647 |  17.25 | 0.100
  17 |  0.432 |  0.419 | -0.012 |   0.020 |  -0.520 |  96.5 |  1.651 |  27.07 | 0.100
  18 |  0.419 |  0.432 |  0.012 |  -0.039 |  -0.551 |  93.5 |  1.536 |  20.74 | 0.100
  19 |  0.432 |  0.419 | -0.012 |   0.029 |  -0.516 |  95.0 |  1.669 |  23.66 | 0.100
  20 |  0.419 |  0.433 |  0.014 |  -0.234 |  -0.539 |  69.5 |  1.491 |  18.92 | 0.100
  21 |  0.433 |  0.419 | -0.014 |   0.023 |  -0.543 |  95.5 |  1.555 |  17.88 | 0.100
  22 |  0.419 |  0.462 |  0.042 |  -0.156 |  -0.555 |  78.0 |  1.655 |  69.54 | 0.100
  23 |  0.462 |  0.419 | -0.042 |   0.305 |  -0.520 |  63.0 |  1.727 |  29.11 | 0.100
  24 |  0.419 |  0.419 |  0.000 |   0.008 |  -0.496 |  98.5 |  1.653 |  15.44 | 0.100
  25 |  0.419 |  0.374 | -0.045 |  -0.036 |  -0.520 |  94.0 |  1.557 |  41.16 | 0.100
  26 |  0.374 |  0.433 |  0.059 |  -0.212 |  -0.531 |  71.5 |  1.603 |  18.82 | 0.100
  27 |  0.433 |  0.419 | -0.014 |   0.146 |  -0.551 |  79.5 |  1.590 |  23.85 | 0.100
  28 |  0.419 |  0.433 |  0.014 |  -0.039 |  -0.539 |  93.5 |  1.519 |  19.54 | 0.100
  29 |  0.433 |  0.418 | -0.014 |  -0.233 |  -0.512 |  69.0 |  1.677 |  21.14 | 0.100
  30 |  0.418 |  0.433 |  0.014 |  -0.089 |  -0.531 |  85.5 |  1.691 |  16.27 | 0.100
  31 |  0.433 |  0.433 |  0.000 |  -0.031 |  -0.555 |  94.5 |  1.689 |  22.04 | 0.100
  32 |  0.433 |  0.433 |  0.000 |  -0.209 |  -0.539 |  72.0 |  1.519 |  20.70 | 0.100
  33 |  0.433 |  0.433 |  0.000 |  -0.102 |  -0.527 |  84.0 |  1.679 |  15.81 | 0.100
  34 |  0.433 |  0.433 |  0.000 |  -0.178 |  -0.500 |  73.5 |  1.634 |  18.49 | 0.100
  35 |  0.433 |  0.376 | -0.057 |  -0.212 |  -0.527 |  71.5 |  1.580 |  19.95 | 0.100
  36 |  0.376 |  0.392 |  0.017 |  -0.072 |  -0.551 |  88.5 |  1.522 |  18.96 | 0.100
  37 |  0.392 |  0.418 |  0.026 |   0.115 |  -0.539 |  82.0 |  1.607 |  31.16 | 0.100
  38 |  0.418 |  0.433 |  0.014 |   0.170 |  -0.551 |  76.5 |  1.582 |  19.91 | 0.100
  39 |  0.433 |  0.432 | -0.001 |   0.193 |  -0.520 |  73.0 |  1.595 |  33.21 | 0.100
  40 |  0.432 |  0.433 |  0.001 |  -0.281 |  -0.512 |  64.5 |  1.563 |  34.06 | 0.100
  41 |  0.433 |  0.388 | -0.045 |  -0.075 |  -0.523 |  87.5 |  1.566 |  23.18 | 0.100
  42 |  0.388 |  0.430 |  0.042 |   0.000 |  -0.516 | 100.0 |  1.650 |  24.97 | 0.100
  43 |  0.430 |  0.386 | -0.044 |   0.082 |  -0.527 |  86.5 |  1.592 |  15.11 | 0.100
  44 |  0.386 |  0.430 |  0.044 |   0.084 |  -0.523 |  86.0 |  1.567 |  21.67 | 0.100
  45 |  0.430 |  0.430 |  0.000 |   0.449 |  -0.559 |  55.5 |  1.706 |  36.59 | 0.100
  46 |  0.430 |  0.430 |  0.000 |   0.020 |  -0.539 |  96.5 |  1.580 |  28.22 | 0.100
  47 |  0.430 |  0.430 | -0.000 |  -0.002 |  -0.535 |  99.0 |  1.616 |  17.05 | 0.100
  48 |  0.430 |  0.416 | -0.014 |  -0.016 |  -0.516 |  97.0 |  1.586 |  20.07 | 0.100
  49 |  0.416 |  0.416 |  0.000 |   0.048 |  -0.551 |  92.0 |  1.633 |  17.71 | 0.100
  50 |  0.416 |  0.416 |  0.000 |   0.000 |  -0.531 | 100.0 |  1.685 |  24.64 | 0.100
