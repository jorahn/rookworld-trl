üö® DEBUGGING GRPO TRAINING - STEP BY STEP
======================================================================
üîç MANUAL GRPO DEBUG - SINGLE BATCH ANALYSIS
======================================================================
üìã Configuration (overfit defaults):
  Batch size: 4
  Generations per prompt: 8
  Max new tokens: 128
  Beta (KL penalty): 0.0 (warmup ‚Üí 0.02 after 20 steps)
  Steps: 500 (sequential GRPO updates)
  Beta adaptation: ON (target_KL‚âà0.15)
  Overfit mode: ON (reuse the same batch each step)
  Seed: 42
  Learning rate: 3e-06
  Sampling (task-conditional): P(temp=0.7, top_p=0.95) | A(temp=1.0, top_p=0.98)
  AdamW: Œ≤1=0.9, Œ≤2=0.999, Œµ=1e-08, decay=0.0

==================== PHASE 1: SETUP ====================
üì• Loading base model...
‚úÖ Loaded reference model (frozen) and training model
üèÜ Initializing reward function...
‚úì Auto-detected Stockfish at: /usr/games/stockfish
‚úì Stockfish initialized at /usr/games/stockfish (depth=10, time=0.1s)
üìä Loading mixed batch...
Loading 20 samples from RookWorld dataset...
‚úì Loaded 20 mixed task samples
üß™ Overfit preset: using P-only prompts (15 available)
üßæ Using dataset prompts [0:4) of 15 (wrap-around: no)
‚úÖ Setup complete. Batch composition:
  P: tasks: 4/4
  A: tasks: 0/4

==================== PHASE 2: INITIAL MODEL PERFORMANCE ====================
  Prompt 1: avg reward = 0.179
  Prompt 2: avg reward = -0.021
  Prompt 3: avg reward = 0.444
  Prompt 4: avg reward = 0.374
üìä INITIAL TRAINING MODEL Performance:
  Average reward: 0.2439
  Positive ratio: 75.0%

==================== PHASE 3: MANUAL GRPO STEP ====================
ü§ñ Generating completions for GRPO training...

üéØ Prompt 1: P: 2k4r/ppp1q1p1/nbb1N3/4PP2/3p2P1/PB5P/1PPK3R/3RQ3 b - - 3 ...
    Gen 1: Train_LP= -440.0, Ref_LP=  -46.5
           Text:                                M: d4d3 a6c5 e7f7 c6f3 c6d7  ...
           Normalized: M: d4d3 a6c5 e7f7 c6f3 c6d7 E: -3.89 -3.77 -4.03 -4.03 -3.94...
    Gen 2: Train_LP= -314.0, Ref_LP=  -36.0
           Text:                                M: c6b5 d4d3 h8e8 a6c5 e7e8  ...
           Normalized: M: c6b5 d4d3 h8e8 a6c5 e7e8 E: -4.31 -4.05 -4.4 -3.96 -4.46 ...
    Gen 3: Train_LP= -584.0, Ref_LP=  -38.2
           Text:                                M: a6c5 h8h6 d4d3 c6d5 c6b5  ...
           Normalized: M: a6c5 h8h6 d4d3 c6d5 c6b5 E: -3.8 -3.96 -3.79 -4.11 -3.87 ...
    Gen 4: Train_LP= -376.0, Ref_LP=  -43.5
           Text:                                M: a6c5 e7d7 d4d3 e7e8 h8e8  ...
           Normalized: M: a6c5 e7d7 d4d3 e7e8 h8e8 E: -3.81 -3.89 -3.87 -4.02 -3.91...
    Gen 5: Train_LP= -556.0, Ref_LP=  -35.8
           Text:                                M: a6c5 h8h4 c6d7 c8b8 h8e8  ...
           Normalized: M: a6c5 h8h4 c6d7 c8b8 h8e8 E: -4.2 -4.28 -3.96 -4.27 -4.28 ...
    Gen 6: Train_LP= -454.0, Ref_LP=  -45.8
           Text:                                M: e7f7 c8b8 a6c5 d4d3 e7e8  ...
           Normalized: M: e7f7 c8b8 a6c5 d4d3 e7e8 E: -4.27 -3.93 -3.92 -4.25 -4.21...
    Gen 7: Train_LP= -556.0, Ref_LP=  -46.8
           Text:                                M: e7e8 c6b5 a6c5 c8b8 e7d7  ...
           Normalized: M: e7e8 c6b5 a6c5 c8b8 e7d7 E: -4.43 -4.34 -3.81 -4.12 -3.94...
    Gen 8: Train_LP= -446.0, Ref_LP=  -33.2
           Text:                                M: c8b8 d4d3 a6c5 h8e8 e7e8  ...
           Normalized: M: c8b8 d4d3 a6c5 h8e8 e7e8 E: -3.81 -3.89 -3.8 -3.81 -3.97 ...

üéØ Prompt 2: P: 2N5/2k5/3p1p2/3P1BbP/4Pp2/1n6/4K3/8 w - - 9 49...
    Gen 1: Train_LP= -466.0, Ref_LP=  -70.5
           Text:                                             M: c8a7 e2f1 e2e...
           Normalized: M: c8a7 e2f1 e2e1 e2f2 e2d3 E: 0.1 0.12 0.13 0.13 0.09 B: e2...
    Gen 2: Train_LP= -600.0, Ref_LP=  -80.0
           Text:                                             M: e2f1 e2f2 c8e...
           Normalized: M: e2f1 e2f2 c8e7 c8a7 e2f3 E: 0.23 0.18 0.25 0.27 0.28 B: e...
    Gen 3: Train_LP= -424.0, Ref_LP=  -81.0
           Text:                                             M: e2f3 c8e7 e2d...
           Normalized: M: e2f3 c8e7 e2d3 e2f2 c8a7 E: 0.45 0.48 0.55 0.45 0.45 B: e...
    Gen 4: Train_LP= -652.0, Ref_LP=  -86.0
           Text:                                             M: e2f3 c8e7 e2f...
           Normalized: M: e2f3 c8e7 e2f1 e2f2 f5e6 E: 0.38 0.48 0.29 0.51 0.49 B: e...
    Gen 5: Train_LP= -648.0, Ref_LP=  -86.0
           Text:                                             M: e2f1 e2f2 c8e...
           Normalized: M: e2f1 e2f2 c8e7 f5g4 c8a7 E: 0.37 0.25 0.34 0.18 0.19 B: c...
    Gen 6: Train_LP= -764.0, Ref_LP=  -74.5
           Text:                                             M: e2f3 e2f2 c8e...
           Normalized: M: e2f3 e2f2 c8e7 e2e1 e2f1 E: 0.28 0.41 0.3 0.37 0.41 B: e2...
    Gen 7: Train_LP= -552.0, Ref_LP=  -16.5
           Text:                                             M: e2f2 f5g4 e2d...
           Normalized: M: e2f2 f5g4 e2d3 c8e7 c8a7 E: 0.0 0.0 0.0 0.0 0.0 B: c8a7...
    Gen 8: Train_LP= -556.0, Ref_LP=  -79.0
           Text:                                             M: e2d3 e2f3 e2f...
           Normalized: M: e2d3 e2f3 e2f2 c8a7 c8e7 E: 0.24 0.23 0.21 0.19 0.21 B: e...

üéØ Prompt 3: P: rnbqkbnr/pp1ppppp/2p5/8/4P3/2N5/PPPP1PPP/R1BQKBNR b KQkq ...
    Gen 1: Train_LP= -464.0, Ref_LP=  -35.8
           Text:                             M: d7d6 e7e6 d7d5 g7g6 e7e5     ...
           Normalized: M: d7d6 e7e6 d7d5 g7g6 e7e5 E: -0.59 -0.63 -0.46 -0.79 -0.5 ...
    Gen 2: Train_LP= -556.0, Ref_LP=  -36.5
           Text:                             M: d7d6 d7d5 g7g6 a7a5 e7e5     ...
           Normalized: M: d7d6 d7d5 g7g6 a7a5 e7e5 E: -0.61 -0.4 -0.63 -0.84 -0.56 ...
    Gen 3: Train_LP= -520.0, Ref_LP=  -47.0
           Text:                             M: d7d6 g7g6 d7d5 e7e5 a7a6     ...
           Normalized: M: d7d6 g7g6 d7d5 e7e5 a7a6 E: -0.64 -0.62 -0.46 -0.52 -0.76...
    Gen 4: Train_LP= -800.0, Ref_LP=  -48.5
           Text:                             M: d7d6 d7d5 e7e5 g8f6 e7e6     ...
           Normalized: M: d7d6 d7d5 e7e5 g8f6 e7e6 E: -0.54 -0.49 -0.46 -0.51 -0.79...
    Gen 5: Train_LP= -548.0, Ref_LP=  -24.5
           Text:                             M: d7d5 d7d6 e7e5 g7g6 d8c7     ...
           Normalized: M: d7d5 d7d6 e7e5 g7g6 d8c7 E: -0.49 -0.7 -0.7 -0.69 -0.85 B...
    Gen 6: Train_LP= -572.0, Ref_LP=  -34.2
           Text:                             M: d7d6 g7g6 d7d5 e7e5 e7e6     ...
           Normalized: M: d7d6 g7g6 d7d5 e7e5 e7e6 E: -0.62 -0.77 -0.4 -0.64 -0.71 ...
    Gen 7: Train_LP= -584.0, Ref_LP=  -35.2
           Text:                             M: d7d5 e7e6 g7g6 e7e5 d7d6     ...
           Normalized: M: d7d5 e7e6 g7g6 e7e5 d7d6 E: -0.31 -0.7 -0.67 -0.62 -0.59 ...
    Gen 8: Train_LP= -516.0, Ref_LP=  -37.5
           Text:                             M: e7e6 e7e5 d7d5 c6c5 d7d6     ...
           Normalized: M: e7e6 e7e5 d7d5 c6c5 d7d6 E: -0.87 -0.54 -0.56 -0.6 -0.74 ...

üéØ Prompt 4: P: r4rk1/pp1nq1pp/2p1p3/3p1p1b/2PP4/1P1N2P1/P3PPBP/R2Q1RK1 w...
    Gen 1: Train_LP= -356.0, Ref_LP=  -36.2
           Text:                         M: f1e1 d1c2 d1d2 d1c1 a2a4      E: ...
           Normalized: M: f1e1 d1c2 d1d2 d1c1 a2a4 E: 0.57 0.59 0.71 0.59 0.6 B: d1...
    Gen 2: Train_LP= -418.0, Ref_LP=  -35.2
           Text:                         M: a1c1 c4d5 d1d2 d1e1 f1e1      E: ...
           Normalized: M: a1c1 c4d5 d1d2 d1e1 f1e1 E: 0.59 0.59 0.6 0.59 0.52 B: d1...
    Gen 3: Train_LP= -524.0, Ref_LP=  -24.2
           Text:                         M: a1c1 d1d2 d1e1 f1e1 a2a4      E: ...
           Normalized: M: a1c1 d1d2 d1e1 f1e1 a2a4 E: 0.5 0.61 0.52 0.43 0.5 B: d1d...
    Gen 4: Train_LP= -342.0, Ref_LP=  -38.0
           Text:                         M: d1d2 d1c2 d1c1 d1e1 a1c1      E: ...
           Normalized: M: d1d2 d1c2 d1c1 d1e1 a1c1 E: 0.57 0.46 0.46 0.58 0.5 B: d1...
    Gen 5: Train_LP= -466.0, Ref_LP=  -47.0
           Text:                         M: f1e1 d1d2 a1c1 d1c2 d1e1      E: ...
           Normalized: M: f1e1 d1d2 a1c1 d1c2 d1e1 E: 0.52 0.75 0.65 0.61 0.59 B: d...
    Gen 6: Train_LP= -388.0, Ref_LP=  -36.5
           Text:                         M: a1c1 d1d2 c4d5 d1c2 d1e1      E: ...
           Normalized: M: a1c1 d1d2 c4d5 d1c2 d1e1 E: 0.56 0.57 0.5 0.58 0.48 B: d1...
    Gen 7: Train_LP= -412.0, Ref_LP=  -36.2
           Text:                         M: d1e1 a1c1 a2a4 d1d2 d1c2      E: ...
           Normalized: M: d1e1 a1c1 a2a4 d1d2 d1c2 E: 0.67 0.65 0.59 0.64 0.6 B: d1...
    Gen 8: Train_LP= -362.0, Ref_LP=  -37.5
           Text:                         M: f1e1 a2a4 d1e1 a1c1 d1c2      E: ...
           Normalized: M: f1e1 a2a4 d1e1 a1c1 d1c2 E: 0.43 0.37 0.43 0.49 0.4 B: a1...

==================== PHASE 4: REWARD CALCULATION ====================

üéØ Scoring Prompt 1 completions:
  Gen 1: Reward= 0.271 | M: d4d3 a6c5 e7f7 c6f3 c6d7 E: -3.89 -3....
  Gen 2: Reward= 0.161 | M: c6b5 d4d3 h8e8 a6c5 e7e8 E: -4.31 -4....
  Gen 3: Reward= 0.125 | M: a6c5 h8h6 d4d3 c6d5 c6b5 E: -3.8 -3.9...
  Gen 4: Reward= 0.136 | M: a6c5 e7d7 d4d3 e7e8 h8e8 E: -3.81 -3....
  Gen 5: Reward= 0.168 | M: a6c5 h8h4 c6d7 c8b8 h8e8 E: -4.2 -4.2...
  Gen 6: Reward= 0.225 | M: e7f7 c8b8 a6c5 d4d3 e7e8 E: -4.27 -3....
  Gen 7: Reward= 0.171 | M: e7e8 c6b5 a6c5 c8b8 e7d7 E: -4.43 -4....
  Gen 8: Reward= 0.193 | M: c8b8 d4d3 a6c5 h8e8 e7e8 E: -3.81 -3....
  üìä Average reward: 0.181

üéØ Scoring Prompt 2 completions:
  Gen 1: Reward= 0.617 | M: c8a7 e2f1 e2e1 e2f2 e2d3 E: 0.1 0.12 ...
  Gen 2: Reward= 0.076 | M: e2f1 e2f2 c8e7 c8a7 e2f3 E: 0.23 0.18...
  Gen 3: Reward= 0.099 | M: e2f3 c8e7 e2d3 e2f2 c8a7 E: 0.45 0.48...
  Gen 4: Reward= 0.275 | M: e2f3 c8e7 e2f1 e2f2 f5e6 E: 0.38 0.48...
  Gen 5: Reward= 0.530 | M: e2f1 e2f2 c8e7 f5g4 c8a7 E: 0.37 0.25...
  Gen 6: Reward= 0.039 | M: e2f3 e2f2 c8e7 e2e1 e2f1 E: 0.28 0.41...
  Gen 7: Reward= 0.337 | M: e2f2 f5g4 e2d3 c8e7 c8a7 E: 0.0 0.0 0...
  Gen 8: Reward= 0.126 | M: e2d3 e2f3 e2f2 c8a7 c8e7 E: 0.24 0.23...
  üìä Average reward: 0.262

üéØ Scoring Prompt 3 completions:
  Gen 1: Reward= 0.578 | M: d7d6 e7e6 d7d5 g7g6 e7e5 E: -0.59 -0....
  Gen 2: Reward= 0.421 | M: d7d6 d7d5 g7g6 a7a5 e7e5 E: -0.61 -0....
  Gen 3: Reward= 0.420 | M: d7d6 g7g6 d7d5 e7e5 a7a6 E: -0.64 -0....
  Gen 4: Reward= 0.361 | M: d7d6 d7d5 e7e5 g8f6 e7e6 E: -0.54 -0....
  Gen 5: Reward= 0.492 | M: d7d5 d7d6 e7e5 g7g6 d8c7 E: -0.49 -0....
  Gen 6: Reward= 0.581 | M: d7d6 g7g6 d7d5 e7e5 e7e6 E: -0.62 -0....
  Gen 7: Reward= 0.462 | M: d7d5 e7e6 g7g6 e7e5 d7d6 E: -0.31 -0....
  Gen 8: Reward= 0.412 | M: e7e6 e7e5 d7d5 c6c5 d7d6 E: -0.87 -0....
  üìä Average reward: 0.466

üéØ Scoring Prompt 4 completions:
  Gen 1: Reward= 0.149 | M: f1e1 d1c2 d1d2 d1c1 a2a4 E: 0.57 0.59...
  Gen 2: Reward= 0.337 | M: a1c1 c4d5 d1d2 d1e1 f1e1 E: 0.59 0.59...
  Gen 3: Reward= 0.345 | M: a1c1 d1d2 d1e1 f1e1 a2a4 E: 0.5 0.61 ...
  Gen 4: Reward= 0.127 | M: d1d2 d1c2 d1c1 d1e1 a1c1 E: 0.57 0.46...
  Gen 5: Reward= 0.377 | M: f1e1 d1d2 a1c1 d1c2 d1e1 E: 0.52 0.75...
  Gen 6: Reward= 0.744 | M: a1c1 d1d2 c4d5 d1c2 d1e1 E: 0.56 0.57...
  Gen 7: Reward= 0.267 | M: d1e1 a1c1 a2a4 d1d2 d1c2 E: 0.67 0.65...
  Gen 8: Reward= 0.297 | M: f1e1 a2a4 d1e1 a1c1 d1c2 E: 0.43 0.37...
  üìä Average reward: 0.330

==================== PHASE 5: TRL EXACT ADVANTAGE CALCULATION ====================
üî¢ TRL Advantage Calculation (exact formula):
  Total rewards shape: torch.Size([32])
  Rewards: [0.27057143 0.1614     0.125      0.1358     0.1682     0.22457143
 0.1714     0.19257143 0.61697143 0.07577143 0.09857143 0.275
 0.52977143 0.03857143 0.33737143 0.12617143 0.5776     0.4212
 0.4196     0.36057143 0.4916     0.5812     0.4616     0.412
 0.14857143 0.3374     0.3454     0.127      0.37697143 0.7442
 0.2666     0.2966    ]
  Grouped rewards shape: torch.Size([4, 8])
  Group means: [0.18118929 0.262275   0.46567143 0.33034286]
  Expanded means: [0.18118929 0.18118929 0.18118929 0.18118929 0.18118929 0.18118929
 0.18118929 0.18118929 0.262275   0.262275   0.262275   0.262275
 0.262275   0.262275   0.262275   0.262275   0.46567143 0.46567143
 0.46567143 0.46567143 0.46567143 0.46567143 0.46567143 0.46567143
 0.33034286 0.33034286 0.33034286 0.33034286 0.33034286 0.33034286
 0.33034286 0.33034286]
  Raw advantages: [ 0.08938214 -0.01978929 -0.05618929 -0.04538929 -0.01298929  0.04338214
 -0.00978929  0.01138214  0.35469643 -0.18650357 -0.16370357  0.012725
  0.26749643 -0.22370357  0.07509643 -0.13610357  0.11192857 -0.04447143
 -0.04607143 -0.1051      0.02592857  0.11552857 -0.00407143 -0.05367143
 -0.18177143  0.00705714  0.01505714 -0.20334286  0.04662857  0.41385714
 -0.06374286 -0.03374286]
  Group std devs: [0.04762101 0.21821196 0.07981285 0.19023271]
  Normalized advantages: [ 1.78764286 -0.39578571 -1.12378571 -0.90778571 -0.25978571  0.86764286
 -0.19578571  0.22764286  1.62546743 -0.85468997 -0.75020441  0.05831486
  1.22585596 -1.02516642  0.34414443 -0.62372188  1.40238781 -0.55719633
 -0.57724323 -1.31683052  0.32486712  1.44749333 -0.05101219 -0.67246599
 -0.95552143  0.03709742  0.07915118 -1.06891638  0.24511332  2.17553095
 -0.33507833 -0.17737674]

üìä TRL Advantage Summary:
  Prompt 1: ['+1.788', '-0.396', '-1.124', '-0.908', '-0.260', '+0.868', '-0.196', '+0.228']
    Stats: mean=+0.000 (expect ‚âà0), std=0.891 (expect >0)
    ‚úÖ Good range (2.911) - clear learning signal
  Prompt 2: ['+1.625', '-0.855', '-0.750', '+0.058', '+1.226', '-1.025', '+0.344', '-0.624']
    Stats: mean=-0.000 (expect ‚âà0), std=0.935 (expect >0)
    ‚úÖ Good range (2.651) - clear learning signal
  Prompt 3: ['+1.402', '-0.557', '-0.577', '-1.317', '+0.325', '+1.447', '-0.051', '-0.672']
    Stats: mean=-0.000 (expect ‚âà0), std=0.935 (expect >0)
    ‚úÖ Good range (2.764) - clear learning signal
  Prompt 4: ['-0.956', '+0.037', '+0.079', '-1.069', '+0.245', '+2.176', '-0.335', '-0.177']
    Stats: mean=+0.000 (expect ‚âà0), std=0.935 (expect >0)
    ‚úÖ Good range (3.244) - clear learning signal

==================== PHASE 6-8: CORRECTED GRPO LOSS CALCULATION ====================
üîÑ Computing GRPO loss with proper token-level KL...

üéØ Prompt 1 - Corrected GRPO Loss:
  Gen 1: A=+1.788, logp/len=-6.969, kl=39.000, H=3.141
         PG=12.438, KL_penalty=0.000, EntReg=-0.031, total=12.375
  Gen 2: A=-0.396, logp/len=-6.562, kl=45.500, H=2.688
         PG=-2.594, KL_penalty=0.000, EntReg=-0.027, total=-2.625
  Gen 3: A=-1.124, logp/len=-7.125, kl=46.250, H=3.250
         PG=-8.000, KL_penalty=0.000, EntReg=-0.032, total=-8.062
  Gen 4: A=-0.908, logp/len=-5.719, kl=29.250, H=3.281
         PG=-5.188, KL_penalty=0.000, EntReg=-0.033, total=-5.219
  Gen 5: A=-0.260, logp/len=-8.188, kl=58.500, H=1.844
         PG=-2.125, KL_penalty=0.000, EntReg=-0.018, total=-2.141
  Gen 6: A=+0.868, logp/len=-6.875, kl=49.750, H=3.016
         PG=5.969, KL_penalty=0.000, EntReg=-0.030, total=5.938
  Gen 7: A=-0.196, logp/len=-6.844, kl=46.750, H=2.891
         PG=-1.344, KL_penalty=0.000, EntReg=-0.029, total=-1.375
  Gen 8: A=+0.228, logp/len=-5.750, kl=38.250, H=2.609
         PG=1.312, KL_penalty=0.000, EntReg=-0.026, total=1.289
  üìä Prompt averages: PG=0.062, KL=0.000

üéØ Prompt 2 - Corrected GRPO Loss:
  Gen 1: A=+1.625, logp/len=-5.875, kl=44.500, H=3.125
         PG=9.562, KL_penalty=0.000, EntReg=-0.031, total=9.500
  Gen 2: A=-0.855, logp/len=-5.219, kl=29.125, H=2.734
         PG=-4.469, KL_penalty=0.000, EntReg=-0.027, total=-4.500
  Gen 3: A=-0.750, logp/len=-6.875, kl=49.500, H=2.172
         PG=-5.156, KL_penalty=0.000, EntReg=-0.022, total=-5.188
  Gen 4: A=+0.058, logp/len=-6.562, kl=37.000, H=2.969
         PG=0.383, KL_penalty=0.000, EntReg=-0.030, total=0.354
  Gen 5: A=+1.226, logp/len=-5.219, kl=27.750, H=3.547
         PG=6.406, KL_penalty=0.000, EntReg=-0.035, total=6.375
  Gen 6: A=-1.025, logp/len=-8.938, kl=66.000, H=2.188
         PG=-9.188, KL_penalty=0.000, EntReg=-0.022, total=-9.188
  Gen 7: A=+0.344, logp/len=-8.188, kl=65.000, H=2.266
         PG=2.812, KL_penalty=0.000, EntReg=-0.023, total=2.797
  Gen 8: A=-0.624, logp/len=-7.031, kl=54.250, H=2.469
         PG=-4.375, KL_penalty=0.000, EntReg=-0.025, total=-4.406
  üìä Prompt averages: PG=-0.504, KL=0.000

üéØ Prompt 3 - Corrected GRPO Loss:
  Gen 1: A=+1.402, logp/len=-7.406, kl=54.000, H=2.531
         PG=10.375, KL_penalty=0.000, EntReg=-0.025, total=10.375
  Gen 2: A=-0.557, logp/len=-6.719, kl=44.750, H=2.922
         PG=-3.750, KL_penalty=0.000, EntReg=-0.029, total=-3.781
  Gen 3: A=-0.577, logp/len=-8.375, kl=67.000, H=1.469
         PG=-4.844, KL_penalty=0.000, EntReg=-0.015, total=-4.844
  Gen 4: A=-1.317, logp/len=-7.750, kl=55.000, H=2.188
         PG=-10.188, KL_penalty=0.000, EntReg=-0.022, total=-10.188
  Gen 5: A=+0.325, logp/len=-5.906, kl=35.500, H=2.938
         PG=1.922, KL_penalty=0.000, EntReg=-0.029, total=1.891
  Gen 6: A=+1.447, logp/len=-6.281, kl=39.000, H=4.031
         PG=9.062, KL_penalty=0.000, EntReg=-0.040, total=9.000
  Gen 7: A=-0.051, logp/len=-5.438, kl=33.500, H=2.578
         PG=-0.277, KL_penalty=0.000, EntReg=-0.026, total=-0.303
  Gen 8: A=-0.672, logp/len=-8.062, kl=62.000, H=2.828
         PG=-5.406, KL_penalty=0.000, EntReg=-0.028, total=-5.438
  üìä Prompt averages: PG=-0.383, KL=0.000

üéØ Prompt 4 - Corrected GRPO Loss:
  Gen 1: A=-0.956, logp/len=-8.938, kl=85.500, H=2.781
         PG=-8.562, KL_penalty=0.000, EntReg=-0.028, total=-8.562
  Gen 2: A=+0.037, logp/len=-7.500, kl=57.250, H=2.344
         PG=0.277, KL_penalty=0.000, EntReg=-0.023, total=0.254
  Gen 3: A=+0.079, logp/len=-7.500, kl=56.250, H=2.391
         PG=0.594, KL_penalty=0.000, EntReg=-0.024, total=0.570
  Gen 4: A=-1.069, logp/len=-6.188, kl=38.750, H=2.812
         PG=-6.625, KL_penalty=0.000, EntReg=-0.028, total=-6.656
  Gen 5: A=+0.245, logp/len=-4.844, kl=37.250, H=2.953
         PG=1.188, KL_penalty=0.000, EntReg=-0.030, total=1.156
  Gen 6: A=+2.176, logp/len=-6.094, kl=42.000, H=2.953
         PG=13.250, KL_penalty=0.000, EntReg=-0.030, total=13.250
  Gen 7: A=-0.335, logp/len=-6.188, kl=43.250, H=2.750
         PG=-2.078, KL_penalty=0.000, EntReg=-0.027, total=-2.109
  Gen 8: A=-0.177, logp/len=-7.375, kl=56.750, H=2.062
         PG=-1.305, KL_penalty=0.000, EntReg=-0.021, total=-1.328
  üìä Prompt averages: PG=-0.414, KL=0.000

üî¢ CORRECTED Loss Breakdown:
  Policy Gradient Loss:   -0.309
  KL Loss (with gradients):    0.000
  Total Loss:          =  -0.309

  KL penalty ratio: 0.0%

üîÑ Performing corrected gradient update...
  Total gradient norm: 17.36
  ‚úÖ Gradient update applied with proper KL regularization

==================== PHASE 9: POST-UPDATE PERFORMANCE ====================
  Prompt 1: avg reward = 0.179
